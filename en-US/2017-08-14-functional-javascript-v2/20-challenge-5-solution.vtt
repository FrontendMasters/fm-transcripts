WEBVTT

1
00:00:00.520 --> 00:00:03.600
&gt;&gt; Kyle Simpson: So exercise 5,
as I warned you ahead of time,

2
00:00:03.600 --> 00:00:05.720
it was not that many lines of code.

3
00:00:05.720 --> 00:00:07.300
Hopefully, it was pretty straightforward.

4
00:00:08.360 --> 00:00:10.090
We start out with this foo function, and

5
00:00:10.090 --> 00:00:15.330
what we wanna do is we want
it to return a function

6
00:00:15.330 --> 00:00:20.580
that remembers the first two arguments
past the foo, and adds them together.

7
00:00:20.580 --> 00:00:24.200
So let's name those two parameters,
x and y.

8
00:00:25.640 --> 00:00:29.200
And let's make and return a function.

9
00:00:29.200 --> 00:00:32.284
Here's our machine making machine
as we talked about before.

10
00:00:43.386 --> 00:00:45.542
&gt;&gt; Kyle Simpson: Now, this function needs

11
00:00:45.542 --> 00:00:49.370
to add x and y together and
return that result.

12
00:00:49.370 --> 00:00:51.400
So that's pretty straightforward.

13
00:00:51.400 --> 00:00:52.290
return x + y.

14
00:00:55.470 --> 00:00:59.330
Let's do some analysis that's consistent
with our current discussion that we've

15
00:00:59.330 --> 00:01:00.330
been having.

16
00:01:00.330 --> 00:01:02.470
Is the foo function itself pure?

17
00:01:10.256 --> 00:01:14.040
&gt;&gt; Speaker 2: High degree.
&gt;&gt; Kyle Simpson: High degree of confidence

18
00:01:14.040 --> 00:01:15.510
that foo itself is pure.

19
00:01:15.510 --> 00:01:17.910
What about the function that it returns?

20
00:01:17.910 --> 00:01:19.630
The one that we've named x down there

21
00:01:19.630 --> 00:01:20.191
on line 9.

22
00:01:25.837 --> 00:01:27.637
&gt;&gt; Speaker 2: Yes.
&gt;&gt; Kyle Simpson: High degree of

23
00:01:27.637 --> 00:01:29.134
confidence?
&gt;&gt; Speaker 2: Because there's no

24
00:01:29.134 --> 00:01:32.445
where that x and y can be redefined.
&gt;&gt; Kyle Simpson: Because there's

25
00:01:32.445 --> 00:01:36.742
nowhere that x and y can be reassigned,
so we know that every time we call x,

26
00:01:36.742 --> 00:01:39.696
we're always gonna get
that same value of 7 back.

27
00:01:39.696 --> 00:01:44.138
That's exactly the right mindset, that's
why I keep reinforcing it so that you'll

28
00:01:44.138 --> 00:01:48.410
develop that instinct, how can I look at
a function and know what its behavior is.

29
00:01:48.410 --> 00:01:53.870
So far, the best definition we've
got is given the same inputs,

30
00:01:53.870 --> 00:01:54.890
we get the same output.

31
00:01:57.030 --> 00:02:02.867
Now, we wanna talk about potentially
even a more sophisticated definition for

32
00:02:02.867 --> 00:02:04.268
function purity.

33
00:02:04.268 --> 00:02:09.690
But to get there, I need to tweak some
understanding of how we approach this.

34
00:02:09.690 --> 00:02:15.500
So I wanna ask you this question,
where does the work happen, the addition?

35
00:02:15.500 --> 00:02:18.026
Does it happen as a result of line 7?

36
00:02:18.026 --> 00:02:20.626
Or does it happen as
a result of line 9 and

37
00:02:20.626 --> 00:02:23.442
10?
&gt;&gt; Speaker 3: Can you ask that question

38
00:02:23.442 --> 00:02:26.660
again?
&gt;&gt; Kyle Simpson: The work that happens

39
00:02:26.660 --> 00:02:31.540
to do the addition of those two numbers,
does that work happen on line 7?

40
00:02:31.540 --> 00:02:34.927
Or does it happen on lines 9 and 10?
&gt;&gt; Speaker 2: 9 and 10.

41
00:02:34.927 --> 00:02:37.410
&gt;&gt; Kyle Simpson: Can we see that?

42
00:02:37.410 --> 00:02:40.512
That it does in fact,
it waits, if you will,

43
00:02:40.512 --> 00:02:43.952
to do that addition until
we run lines 9 and 10.

44
00:02:43.952 --> 00:02:48.105
In a sense, if line 9 and 10 never ran,

45
00:02:48.105 --> 00:02:54.529
we were kind of deferring the work
in case nobody ever needed it.

46
00:02:55.800 --> 00:02:59.092
Now, what if that works with
something more intensive than just

47
00:02:59.092 --> 00:03:00.210
mathematic addition?

48
00:03:00.210 --> 00:03:03.350
What if it was a list of a million values,
and

49
00:03:03.350 --> 00:03:07.730
we were calculating some variance across
it and it took a lot of work to calculate.

50
00:03:09.380 --> 00:03:14.000
And we weren't really sure whether or not
anybody was gonna even need the answer.

51
00:03:14.000 --> 00:03:20.050
So in that case, the deferral to put
it on line 3 would have made sense.

52
00:03:20.050 --> 00:03:24.630
Let's defer that work in case
nobody ever calls the x function.

53
00:03:24.630 --> 00:03:27.170
No reason to do unnecessary work, right?

54
00:03:29.180 --> 00:03:31.140
There's a term for that in programming.

55
00:03:31.140 --> 00:03:33.500
The term for
that is that's a lazy algorithm.

56
00:03:34.780 --> 00:03:38.510
Now, we don't mean lazy in
the pejorative insult sense.

57
00:03:38.510 --> 00:03:41.608
But we need it as compared
to an eager algorithm,

58
00:03:41.608 --> 00:03:44.580
which would have done the work
ahead of time, up front.

59
00:03:46.310 --> 00:03:48.930
So what if we said,
I wanna do the work up front,

60
00:03:48.930 --> 00:03:53.090
while we could have calculated that,
for example, on line one and a half?

61
00:03:53.090 --> 00:03:56.840
Now, I wanna talk about
the difference in end result.

62
00:03:56.840 --> 00:03:59.822
Because we are, in fact,
waiting until line 9.

63
00:03:59.822 --> 00:04:04.790
So in case line 9 never happened,
we didn't waste any work.

64
00:04:04.790 --> 00:04:09.281
But what do we know about line 10?
&gt;&gt; Speaker 2: It's gonna do the same

65
00:04:09.281 --> 00:04:11.030
thing-
&gt;&gt; Kyle Simpson: We're redoing the work.

66
00:04:12.750 --> 00:04:15.360
So if that's expensive work, and

67
00:04:15.360 --> 00:04:19.965
you expect that it's possible
somebody might ask for it twice.

68
00:04:19.965 --> 00:04:23.530
Well, now, you might start to say
maybe I ought to do it once upfront.

69
00:04:25.210 --> 00:04:28.570
So, what if we did the work up front?

70
00:04:28.570 --> 00:04:32.156
What if we said sum = x + y and

71
00:04:32.156 --> 00:04:37.243
then we simply return the sum variable?

72
00:04:37.243 --> 00:04:40.155
Now which line of code
does the work happen on?

73
00:04:42.508 --> 00:04:43.115
&gt;&gt; Speaker 2: 8.

74
00:04:43.115 --> 00:04:44.845
&gt;&gt; Kyle Simpson: It happens as a result of

75
00:04:44.845 --> 00:04:45.540
line 8.

76
00:04:45.540 --> 00:04:48.434
And do we still get
the same output on line

77
00:04:48.434 --> 00:04:52.910
10?
&gt;&gt; Kyle Simpson: Do we still get the same

78
00:04:52.910 --> 00:04:54.390
output on line 11?

79
00:04:54.390 --> 00:05:00.672
So is x still a pure function?
&gt;&gt; Speaker 2: Yes.

80
00:05:00.672 --> 00:05:01.857
&gt;&gt; Speaker 4: No.

81
00:05:01.857 --> 00:05:04.600
&gt;&gt; Kyle Simpson: Uh-oh, somebody says no.

82
00:05:04.600 --> 00:05:08.711
Why is it not a pure function?
&gt;&gt; Speaker 4: Cuz it's no longer pure,

83
00:05:08.711 --> 00:05:15.884
because you're changing the value
essentially within the function

84
00:05:15.884 --> 00:05:21.076
itself.
&gt;&gt; Kyle Simpson: So when does that change?

85
00:05:21.076 --> 00:05:25.790
You are correct that we assign the sum
variable, but when does that change occur?

86
00:05:25.790 --> 00:05:28.329
Does it occur before or
after we've created and

87
00:05:28.329 --> 00:05:29.669
returned the function?

88
00:05:32.006 --> 00:05:34.340
&gt;&gt; Speaker 2: Before.
&gt;&gt; Kyle Simpson: Before.

89
00:05:34.340 --> 00:05:38.870
So observationally, the x function,
by the time we get the x function,

90
00:05:38.870 --> 00:05:42.212
the answer to this question
is already set in stone and

91
00:05:42.212 --> 00:05:47.171
it's never gonna change again, right?
&gt;&gt; Kyle Simpson: So

92
00:05:47.171 --> 00:05:50.530
observationally, does it still meet
our current running definition for

93
00:05:50.530 --> 00:05:56.139
function theory?
&gt;&gt; Kyle Simpson: Remember,

94
00:05:56.139 --> 00:05:58.897
our definition is given the same inputs,
in this case, there are no inputs.

95
00:05:58.897 --> 00:06:02.822
Given the same inputs to x,
are we always gonna get the same output 7?

96
00:06:02.822 --> 00:06:04.724
&gt;&gt; Speaker 2: Yes.

97
00:06:07.876 --> 00:06:08.811
&gt;&gt; Kyle Simpson: We have a high degree of

98
00:06:08.811 --> 00:06:10.710
confidence in that, right?

99
00:06:10.710 --> 00:06:13.770
Unless somebody somehow mucks with
the JavaScript language runtime,

100
00:06:13.770 --> 00:06:16.724
itself, I'm pretty sure
that's what it's gonna do.

101
00:06:19.740 --> 00:06:23.030
But now the downside is,
if that's expensive work, and

102
00:06:23.030 --> 00:06:26.890
nobody ever calls line 10 and
11, we wasted that work.

103
00:06:29.130 --> 00:06:34.919
So there is a tension between this lazy
algorithm and this eager algorithm.

104
00:06:37.240 --> 00:06:39.950
And you might then wonder,
is there somewhere in the middle?

105
00:06:39.950 --> 00:06:44.540
Is there a compromise between the two
that gives us the best of both worlds?

106
00:06:46.370 --> 00:06:53.110
So what if we could say, I wanna wait to
do the work until somebody asks for it.

107
00:06:53.110 --> 00:06:56.130
So I want to be lazy about it.

108
00:06:56.130 --> 00:07:00.790
But once I've done the work once, I wanna
remember that and never do the work again.

109
00:07:04.220 --> 00:07:06.260
See how that would be
a compromise between the two?

110
00:07:07.360 --> 00:07:11.140
So what if I said var sum, and
I didn't do the work here.

111
00:07:12.290 --> 00:07:17.226
And inside of our x function,
I said if (sum

112
00:07:17.226 --> 00:07:22.220
=== undefined), cuz there's only one way

113
00:07:22.220 --> 00:07:27.220
that sum can ever be undefined is that's
this is the first time we will run.

114
00:07:27.220 --> 00:07:31.270
Then why don't we set
sum equal to that work?

115
00:07:34.300 --> 00:07:35.168
The next time we run,

116
00:07:35.168 --> 00:07:39.888
are we gonna redo that work?
&gt;&gt; Kyle Simpson: Nope.

117
00:07:44.506 --> 00:07:45.283
&gt;&gt; Kyle Simpson: See how this is kinda

118
00:07:45.283 --> 00:07:46.296
somewhere in the middle.

119
00:07:46.296 --> 00:07:50.622
It's still a lazy algorithm
that defers the work, but

120
00:07:50.622 --> 00:07:55.244
now we know that we only gotta
do the work once on line 13.

121
00:07:55.244 --> 00:07:57.439
And we're not gonna redo that work on line

122
00:07:57.439 --> 00:08:01.637
14.
&gt;&gt; Kyle Simpson: But

123
00:08:01.637 --> 00:08:05.682
now we have to ask ourselves,
is x still a pure a function?

124
00:08:16.759 --> 00:08:17.662
&gt;&gt; Speaker 2: I'm gonna stick my

125
00:08:17.662 --> 00:08:19.726
neck out and say yes.
&gt;&gt; Kyle Simpson: You're gonna say yes, or

126
00:08:19.726 --> 00:08:21.957
you're gonna say I have a high
degree of confidence, or

127
00:08:21.957 --> 00:08:24.960
a mid-range degree of confidence?
&gt;&gt; Speaker 2: A high degree of confidence,

128
00:08:24.960 --> 00:08:29.385
because it's still going to always return
the same output given the same input.

129
00:08:29.385 --> 00:08:30.105
&gt;&gt; Kyle Simpson: Okay, so

130
00:08:30.105 --> 00:08:33.355
given that running definition
that we keep going with,

131
00:08:34.680 --> 00:08:37.300
the blue side of the Rubik's cube,
if you will.

132
00:08:37.300 --> 00:08:39.285
If we look only at
what's happening with x,

133
00:08:39.285 --> 00:08:44.050
it still observationally is always
gonna produce the same result.

134
00:08:44.050 --> 00:08:48.230
And if we don't consider any of
the concerns, like for example,

135
00:08:48.230 --> 00:08:53.150
that the first run of x is gonna run
a lot slower than the second run.

136
00:08:53.150 --> 00:08:57.590
If we don't consider the time delay to
be something in the system that we care

137
00:08:57.590 --> 00:09:05.010
about, then lines 13 and lines 14
are observationally indistinguishable.

138
00:09:05.010 --> 00:09:05.560
Do you follow that?

139
00:09:07.660 --> 00:09:12.070
That is going to lead us,
that observation, us going to lead us to

140
00:09:12.070 --> 00:09:17.029
our final evolution for function purity,
our final definitional evolution.

141
00:09:18.230 --> 00:09:23.262
Where we look at the completeness
of the Rubik's cube, if you will.

142
00:09:23.262 --> 00:09:28.204
The official academic functional
programmer leads with this kind of

143
00:09:28.204 --> 00:09:30.680
definition for function purity.

144
00:09:31.850 --> 00:09:35.735
There's a term,
sounds fancier than it really is, and

145
00:09:35.735 --> 00:09:41.476
this term is referential transparency.
&gt;&gt; Kyle Simpson: Functional

146
00:09:41.476 --> 00:09:45.330
programmers will say that a pure
function is a function that

147
00:09:45.330 --> 00:09:48.070
has referential transparency.

148
00:09:48.070 --> 00:09:50.724
Great, thanks, we can all go home,
we're done, right?

149
00:09:50.724 --> 00:09:52.170
What does that mean?

150
00:09:52.170 --> 00:09:54.680
What is referential transparency?

151
00:09:55.910 --> 00:09:58.750
Its sounds more complex than it is.

152
00:09:58.750 --> 00:10:01.080
You can sound smart at
parties if you pull that out.

153
00:10:01.080 --> 00:10:07.230
Well, my referential transparency's
intact so, but here is what really is.

154
00:10:07.230 --> 00:10:13.120
We take the line 13, and
we know that is a function call that will

155
00:10:13.120 --> 00:10:18.700
return value 7, same thing with line 14.

156
00:10:18.700 --> 00:10:24.602
What referential transparency says is,
we could take that function call,

157
00:10:24.602 --> 00:10:30.792
and replace it with its result, and it
would have no other impact on the system.

158
00:10:30.792 --> 00:10:35.614
It would be unobservable to the rest of
the program if we took a function call and

159
00:10:35.614 --> 00:10:39.002
replaced it with its return.
&gt;&gt; Kyle Simpson: So

160
00:10:39.002 --> 00:10:43.348
wherever that x call is being used,
if we just took the x call and

161
00:10:43.348 --> 00:10:48.268
replaced it with the value 7 in this case,
the rest of the program would

162
00:10:48.268 --> 00:10:52.753
behave identically.
&gt;&gt; Kyle Simpson: That's what referential

163
00:10:52.753 --> 00:10:57.212
transparency says, that a function can
be replaced with its return value and

164
00:10:57.212 --> 00:11:01.899
not affect the rest of the system.
&gt;&gt; Kyle Simpson: Is

165
00:11:01.899 --> 00:11:04.020
that true of the x function?

166
00:11:05.570 --> 00:11:07.730
Does it have referential transparency?

167
00:11:09.440 --> 00:11:15.860
Can its result replace the call and
not affect the rest of the system?

168
00:11:15.860 --> 00:11:18.550
I would say I have a high
degree of confidence that this

169
00:11:18.550 --> 00:11:21.760
x function has referential transparency.

170
00:11:21.760 --> 00:11:26.030
And that is the official, most accurate
definition for function purity.

171
00:11:27.400 --> 00:11:30.830
And you can see how our
understanding of that concept

172
00:11:30.830 --> 00:11:35.540
has changed a bit as we've gotten a
different perspective on the Rubik's cube.

173
00:11:35.540 --> 00:11:39.060
We started out saying, well, it just can't
reference any variables outside of itself,

174
00:11:39.060 --> 00:11:42.520
and that turned out to not
really be very complete or

175
00:11:42.520 --> 00:11:45.480
accurate, cuz we're only
looking at it from one angle.

176
00:11:45.480 --> 00:11:47.354
And then we said well,
given the same inputs,

177
00:11:47.354 --> 00:11:50.340
I'm always gonna get the same output,
and that worked for a while.

178
00:11:50.340 --> 00:11:52.520
And now we say, well actually,

179
00:11:52.520 --> 00:11:57.390
you could replace the function with
its return and not affect the system.

180
00:11:57.390 --> 00:12:00.263
That's the full and
complete answer for function purity.

181
00:12:00.263 --> 00:12:02.153
Now, here's why this matters.

182
00:12:02.153 --> 00:12:06.440
You might think, well that suggests
then that I should replace it.

183
00:12:06.440 --> 00:12:10.066
If I don't need the call,
why not just compute it and replace it?

184
00:12:10.066 --> 00:12:13.330
We are not saying with
referential transparency,

185
00:12:13.330 --> 00:12:15.050
that you should actually do so.

186
00:12:16.480 --> 00:12:21.600
It is to suggest that you could,
and not affect the system.

187
00:12:21.600 --> 00:12:25.070
So is this just something
to make us feel clever?

188
00:12:27.030 --> 00:12:32.075
In Haskell and other languages that are
statically typed functional programming

189
00:12:32.075 --> 00:12:36.725
languages, they actually take
advantage of referential transparency.

190
00:12:36.725 --> 00:12:40.775
Because they can guarantee,
by virtue of how the language works,

191
00:12:40.775 --> 00:12:45.355
they can guarantee that a function does,
in fact, have referential transparency.

192
00:12:45.355 --> 00:12:50.773
And so what the compiler will do is it
will compute the value the first time,

193
00:12:50.773 --> 00:12:55.675
and then every other time after,
it won't run the function again,

194
00:12:55.675 --> 00:12:58.603
it'll just substituting in the value.

195
00:12:58.603 --> 00:13:00.480
It's a lot more performant that way,
right?

196
00:13:01.560 --> 00:13:03.810
So the compiler takes care of that for
you.

197
00:13:03.810 --> 00:13:07.210
You don't need to do any of that
stuff like what we're doing about

198
00:13:07.210 --> 00:13:10.190
tracking the variable and
holding on to it, or.

199
00:13:10.190 --> 00:13:11.720
It just automatically does that for you,

200
00:13:11.720 --> 00:13:15.150
because it knows about
referential transparency.

201
00:13:16.600 --> 00:13:20.663
JavaScript does not have
that same characteristic,

202
00:13:20.663 --> 00:13:25.548
we cannot guarantee a function
is entirely, completely pure.

203
00:13:25.548 --> 00:13:27.062
And so for JavaScript,

204
00:13:27.062 --> 00:13:32.370
we're not talking about the compiler doing
the replacing, so why does it matter?

205
00:13:33.610 --> 00:13:36.423
It matters to the reader of your code.

206
00:13:36.423 --> 00:13:39.673
And this is me,
not as an official functional programmer,

207
00:13:39.673 --> 00:13:44.260
but trying to help you understand
what do these concepts means to me?

208
00:13:44.260 --> 00:13:50.680
It is to suggest that the reader of
your code can compute that line once,

209
00:13:50.680 --> 00:13:54.060
and then any time they see that same line,
they don't need to recompute it.

210
00:13:55.230 --> 00:14:00.576
They have perfect confidence that when
they see x() somewhere else in your code,

211
00:14:00.576 --> 00:14:02.598
I already know that answer is 7,

212
00:14:02.598 --> 00:14:07.156
I don't need to recompute it.
&gt;&gt; Kyle Simpson: Could you see how that

213
00:14:07.156 --> 00:14:08.570
would help with the readability?

214
00:14:08.570 --> 00:14:11.610
If you had something repeated over and
over and over again, and

215
00:14:11.610 --> 00:14:15.200
somebody was gonna have to go recompute
it or refigure it out every time.

216
00:14:15.200 --> 00:14:19.860
What we've done by using
a pure function and

217
00:14:19.860 --> 00:14:24.190
telling that person, this function has
referential transparency, we've made it so

218
00:14:24.190 --> 00:14:26.380
they only need to do
the work at most once.

219
00:14:27.890 --> 00:14:31.828
Which is gonna make repeated
reading of that line of code a lot

220
00:14:31.828 --> 00:14:34.743
easier.
&gt;&gt; Kyle Simpson: That's why

221
00:14:34.743 --> 00:14:39.355
referential transparency, and
moreover, why function purity matters.

222
00:14:39.355 --> 00:14:44.876
It's the ultimate aid to the reader
to help them understand and

223
00:14:44.876 --> 00:14:46.762
trust a line of code.

224
00:14:51.449 --> 00:14:53.999
&gt;&gt; Kyle Simpson: Questions about using

225
00:14:53.999 --> 00:14:58.971
closure, using these hybrid techniques?

226
00:15:02.763 --> 00:15:04.358
&gt;&gt; Kyle Simpson: What we've done here,

227
00:15:04.358 --> 00:15:09.140
where I have this variable, and
I changed its value and recomputed it,

228
00:15:09.140 --> 00:15:14.160
but only on the outside we still
maintain referential transparency.

229
00:15:14.160 --> 00:15:20.910
That's a very ad hoc way of doing a thing
that has a name in functional programming.

230
00:15:22.150 --> 00:15:28.340
There's a term for this kind of
adaptive memory thing that's going on.

231
00:15:28.340 --> 00:15:33.578
And that what is called memoization,
M-E-M-O-I-Z-A-T-I-O-N.

232
00:15:33.578 --> 00:15:37.270
Now I don't know what that
word actually means, and

233
00:15:37.270 --> 00:15:40.420
I have no idea if what I'm about
to assert is where it comes from.

234
00:15:40.420 --> 00:15:43.570
But in my brain when I
see the word memoization,

235
00:15:43.570 --> 00:15:47.679
I feel like they just dropped r
from that and i think memorization.

236
00:15:48.800 --> 00:15:52.480
Just one letter missing,
because memoization

237
00:15:52.480 --> 00:15:57.680
is essentially memorizing
the work that's done.

238
00:15:57.680 --> 00:15:59.980
So this what memoization does.

239
00:15:59.980 --> 00:16:02.948
You can take a function, and memoize it.

240
00:16:02.948 --> 00:16:07.982
it's an adaptation, it's a machine
making machine, if you will.

241
00:16:07.982 --> 00:16:13.161
The new function,
will take every input that you give it and

242
00:16:13.161 --> 00:16:16.174
store that input with the output.

243
00:16:16.174 --> 00:16:19.296
It will keep track of that,
it will have a memory.

244
00:16:19.296 --> 00:16:23.609
And then the next time you call that
function with the exact same input, it

245
00:16:23.609 --> 00:16:28.570
will find that in its memory and just give
you the output back, and not recompute it.

246
00:16:30.030 --> 00:16:31.509
That's a memoized function.

247
00:16:32.890 --> 00:16:36.010
So this is sort of an adhoc
one off memoization.

248
00:16:37.390 --> 00:16:41.240
We didn't compute the work
until we ran it the first time.

249
00:16:41.240 --> 00:16:44.680
But then once we ran it,
we remembered that work, and

250
00:16:44.680 --> 00:16:48.129
now we're just gonna return the result
of that memory, over and over again.

251
00:16:49.420 --> 00:16:51.700
This is an adhoc form of memoization.

252
00:16:51.700 --> 00:16:55.566
There's a generalized memoize
utility that can be built and

253
00:16:55.566 --> 00:17:00.948
comes with most functional programming
libraries, where you can take a function,

254
00:17:00.948 --> 00:17:05.910
and if you expect that the results of
that function are expensive to compute.

255
00:17:05.910 --> 00:17:10.756
And you expected it's gonna be repeatedly
asking for it over and over, and over, and

256
00:17:10.756 --> 00:17:13.310
over, and over, and over again.

257
00:17:13.310 --> 00:17:15.800
You might wanna memoize the function.

258
00:17:15.800 --> 00:17:18.530
It's not really a behavioral difference,
it's a performance difference.

259
00:17:18.530 --> 00:17:21.330
It's basically for performance.

260
00:17:23.030 --> 00:17:28.050
Now thinking about what memoization does,
memoization

261
00:17:28.050 --> 00:17:32.960
is just like, it literally creates
a variable on the outside of

262
00:17:32.960 --> 00:17:38.130
the scope that we're talking about,
and that variable grows infinitely.

263
00:17:38.130 --> 00:17:41.720
Every time you give it a new input,
it's gonna add a new entry to that cache.

264
00:17:43.190 --> 00:17:46.927
That's really stretching
the bounds of what we would

265
00:17:46.927 --> 00:17:50.674
normally consider to be a pure function,
isn't it?

266
00:17:50.674 --> 00:17:54.670
Cuz now we have this thing that's growing
and the state that is changing over time.

267
00:17:54.670 --> 00:18:00.940
Here´s why a memoized function
still fits within our definition.

268
00:18:02.130 --> 00:18:05.860
Because, first and foremost,
observationally, the function that we

269
00:18:05.860 --> 00:18:10.960
are dealing with, the x function,
still has referential transparency.

270
00:18:12.910 --> 00:18:17.540
And moreover, that cache,
that growing state of things

271
00:18:17.540 --> 00:18:20.980
is not observable by any
other part of the system.

272
00:18:22.380 --> 00:18:25.197
If that cache was out
in a global variable,

273
00:18:25.197 --> 00:18:30.690
that would absolutely be side effects and
we would have violated function period.

274
00:18:30.690 --> 00:18:34.588
But with memoization,
where do you think they store the cache?

275
00:18:39.132 --> 00:18:40.094
&gt;&gt; Kyle Simpson: In the closure,

276
00:18:40.094 --> 00:18:43.280
just like we've stored
the cache in the closure.

277
00:18:43.280 --> 00:18:46.190
We've closed over the variable sum,

278
00:18:46.190 --> 00:18:51.504
it's the closure that holds onto that
variable and holds on to that state.

279
00:18:51.504 --> 00:18:56.360
A memoize utility is gonna do exactly
the same thing instead of a sum variable.

280
00:18:56.360 --> 00:19:01.130
It's gonna have a giant object cache that
it keeps track of, but it's close over it.

281
00:19:03.470 --> 00:19:08.980
So what I'm getting at is in
the usage of closure in functional

282
00:19:08.980 --> 00:19:14.430
programming that is consistent with
functional programming is to say,

283
00:19:14.430 --> 00:19:19.520
we use closure to
maintain state that can't

284
00:19:19.520 --> 00:19:24.571
be observed from the outside world.
&gt;&gt; Kyle Simpson: That's the whole

285
00:19:24.571 --> 00:19:29.240
purpose of closure is to take
a function and give it a memory.

286
00:19:29.240 --> 00:19:34.750
Now let me give you this example,
here's a function foo.

287
00:19:46.423 --> 00:19:48.415
&gt;&gt; Kyle Simpson: Is x a pure function,

288
00:19:48.415 --> 00:19:51.849
does x has referential transparency

289
00:19:51.849 --> 00:19:57.600
in this example?
&gt;&gt; Speaker 4: Yes.

290
00:19:57.600 --> 00:19:58.600
&gt;&gt; Kyle Simpson: Every time we call it,

291
00:19:58.600 --> 00:20:00.250
what are we gonna get?

292
00:20:00.250 --> 00:20:02.227
&gt;&gt; Speaker 4: Zero.
&gt;&gt; Kyle Simpson: Everybody agree we're

293
00:20:02.227 --> 00:20:07.883
gonna get zero?
&gt;&gt; Kyle Simpson: Is

294
00:20:07.883 --> 00:20:10.400
x still a pure function now?

295
00:20:13.430 --> 00:20:14.910
&gt;&gt; Speaker 2: No.
&gt;&gt; Speaker 4: Yes.

296
00:20:14.910 --> 00:20:16.280
&gt;&gt; Kyle Simpson: What's gonna happen when

297
00:20:16.280 --> 00:20:21.290
I call x the first time?
&gt;&gt; Kyle Simpson: What do I get?

298
00:20:21.290 --> 00:20:22.530
&gt;&gt; Speaker 2: One.

299
00:20:22.530 --> 00:20:23.806
&gt;&gt; Kyle Simpson: What's gonna happen when

300
00:20:23.806 --> 00:20:28.036
I call x the second time?
&gt;&gt; Speaker 2: Two.

301
00:20:28.036 --> 00:20:28.824
&gt;&gt; Kyle Simpson: That doesn't sound

302
00:20:28.824 --> 00:20:31.010
like function purity
referential transparency to me.

303
00:20:31.010 --> 00:20:36.346
That sounds like we've just with that one
simple change, we're still using closure,

304
00:20:36.346 --> 00:20:40.816
but just this one simple change now all
of a sudden we violated referential

305
00:20:40.816 --> 00:20:45.956
transparency.
&gt;&gt; Kyle Simpson: That's

306
00:20:45.956 --> 00:20:50.900
why referential transparency is
the definition that actually matters.

307
00:20:50.900 --> 00:20:53.670
Cuz in all our other definitions,
everything's fine.

308
00:20:55.050 --> 00:20:58.950
But we fail referential transparency,
so therefore we fail function purity.

309
00:21:07.040 --> 00:21:11.690
A consistent, a usage of closure
that is consistent with functional

310
00:21:11.690 --> 00:21:16.200
programming closes over a state that
does not observationally change.

311
00:21:17.530 --> 00:21:23.700
If the state in this case the ID variable
observationally changes, it's not

312
00:21:23.700 --> 00:21:28.070
functional programming consistent, it's
a violation of referential transparency.

313
00:21:29.600 --> 00:21:34.091
So as long as you can contain
those effects and be unobservable,

314
00:21:34.091 --> 00:21:37.596
you've maintained
referential transparency.

315
00:21:37.596 --> 00:21:42.420
And I go so far as to say, you can't do
functional programming without closure.

316
00:21:42.420 --> 00:21:48.290
So it's really important for
you to be able to juggle that tension.

317
00:21:48.290 --> 00:21:49.080
How do I use it?

318
00:21:49.080 --> 00:21:52.072
And how do I use it safely
versus how do I use it, and

319
00:21:52.072 --> 00:21:57.864
everything false apart.
&gt;&gt; Kyle Simpson: Any

320
00:21:57.864 --> 00:22:01.040
other questions about
this topic of closure?

321
00:22:02.380 --> 00:22:04.550
Does an ID always get redefine?

322
00:22:04.550 --> 00:22:07.320
No, ID is defined once on line 2.

323
00:22:07.320 --> 00:22:10.950
Basically, ID gets define
as of result of line 8.

324
00:22:10.950 --> 00:22:14.734
And if I call x over and over and
over again, there's only one ID.

325
00:22:14.734 --> 00:22:18.348
The first time, we're gonna return 0,
the second time,

326
00:22:18.348 --> 00:22:23.620
we're gonna return 1, then we're
gonna return 2, and so on, and so on.

327
00:22:23.620 --> 00:22:25.630
There's only one ID,
it doesn't get redefined,

328
00:22:25.630 --> 00:22:28.600
cuz we're not recalling foo every time.

329
00:22:28.600 --> 00:22:31.600
We're only calling the result of foo,
which is the x function.

