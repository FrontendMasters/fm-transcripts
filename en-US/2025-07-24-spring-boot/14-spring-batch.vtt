WEBVTT

1
00:00:00.160 --> 00:00:03.007
The very next thing I wanna talk
to you about is Spring Batch.

2
00:00:03.007 --> 00:00:08.273
Now, Spring Batch is a,
t's a framework to build and

3
00:00:08.273 --> 00:00:13.200
to solve ETL,
batch processing kinds of things.

4
00:00:13.200 --> 00:00:16.979
One of the things that people don't really
appreciate is that batch processing is

5
00:00:16.979 --> 00:00:20.800
still very much the bread and butter of
most financial services organizations.

6
00:00:20.800 --> 00:00:25.615
And look like I, do you remember that old
T shirt that said be very nice to me,

7
00:00:25.615 --> 00:00:29.710
I'll replace you with
a very small shell script.

8
00:00:29.710 --> 00:00:33.342
In my mind, it's always been very nice
to me or I'll replace your COBOL and

9
00:00:33.342 --> 00:00:36.470
Cakes installation with a very
small spring batch app, right?

10
00:00:36.470 --> 00:00:40.544
Spring Batch is just a very nice way to
solve these kinds of large amounts of

11
00:00:40.544 --> 00:00:42.750
sequential data processing tasks.

12
00:00:42.750 --> 00:00:45.612
And it's a framework that
builds on top of, sorry,

13
00:00:45.612 --> 00:00:50.750
it builds on top of Spring framework,
and that you can use from Spring Boot.

14
00:00:50.750 --> 00:00:52.030
And it is meant to deal with data.

15
00:00:52.030 --> 00:00:54.350
It's meant to load,
process and write data.

16
00:00:54.350 --> 00:00:55.870
It's ETL, that's the whole point, right?

17
00:00:55.870 --> 00:00:59.230
Extraction, transformation and loading,
processing, writing, that kind of thing.

18
00:00:59.230 --> 00:01:02.492
So what we're gonna do is we're
going to step back a little bit, and

19
00:01:02.492 --> 00:01:05.353
let's say that, I mean,
this code is all still valid, but

20
00:01:05.353 --> 00:01:08.270
let's keep it a little simpler here, okay?

21
00:01:08.270 --> 00:01:10.750
So I've commented out all that code.

22
00:01:10.750 --> 00:01:14.973
We're basically back to stock
standard spring boot application,

23
00:01:14.973 --> 00:01:19.947
going to get rid of the schema there, and
we're going to just comment that out.

24
00:01:19.947 --> 00:01:23.252
I'll go back to my build and

25
00:01:23.252 --> 00:01:28.040
keep it a little bit simpler, okay?

26
00:01:28.040 --> 00:01:29.453
Cmd + Shift + I.

27
00:01:29.453 --> 00:01:30.736
I'm not adding anything,

28
00:01:30.736 --> 00:01:34.880
I'm just removing things, so you know that
this is a separate, but it's orthogonal.

29
00:01:34.880 --> 00:01:37.523
You can use these things together,
but I just don't see the need to, so

30
00:01:37.523 --> 00:01:38.920
we'll keep it simple, okay?

31
00:01:38.920 --> 00:01:40.960
Okay, so
there's my Spring Boot application.

32
00:01:40.960 --> 00:01:44.240
It's now going to become a spring batch
application, it's a batch processing.

33
00:01:44.240 --> 00:01:47.880
And remember, batch processing is one
of those things where you're usually

34
00:01:47.880 --> 00:01:50.904
typically dealing with a lot of data and
there might be errors and

35
00:01:50.904 --> 00:01:53.980
there might be ingest involved and
there might be issues.

36
00:01:53.980 --> 00:01:57.527
And these kinds of things are by
definition things you find out only after

37
00:01:57.527 --> 00:01:59.340
hours and hours of running, right?

38
00:01:59.340 --> 00:02:02.460
So there are things you want to have
to be as bulletproof as possible.

39
00:02:03.820 --> 00:02:08.678
So Spring Batch ships with a number of
features that are meant to support you in

40
00:02:08.678 --> 00:02:11.980
the 1% case, or
the 0.01% case right there.

41
00:02:11.980 --> 00:02:15.280
It's easy and clean and
elegant in the 99% case.

42
00:02:15.280 --> 00:02:18.938
But it's that when, when things go wrong,
as an operator running this job,

43
00:02:18.938 --> 00:02:20.416
I want to be able to intervene.

44
00:02:20.416 --> 00:02:24.520
I want to be able to correct it or
to have fallbacks or whatever.

45
00:02:24.520 --> 00:02:25.406
And likewise,

46
00:02:25.406 --> 00:02:29.640
I also want to be able to do complex
sort of processing on this data, okay?

47
00:02:29.640 --> 00:02:32.840
So I've deleted these two
different tables here.

48
00:02:32.840 --> 00:02:34.480
I'm deleting all these tables.

49
00:02:34.480 --> 00:02:38.640
Goodbye to everything,
force refactoring, good.

50
00:02:38.640 --> 00:02:41.300
And I'm going to go back to
my application properties.

51
00:02:41.300 --> 00:02:46.340
I'm keeping my Spring data source
credentials and all that stuff.

52
00:02:46.340 --> 00:02:47.780
I'm going to keep the SQL init.

53
00:02:47.780 --> 00:02:49.300
I'm going to tell Spring Boot to run that.

54
00:02:49.300 --> 00:02:56.906
I'm going to create a new table here,
a new file rather called schema.sql,

55
00:02:56.906 --> 00:03:03.940
create table if not exists dog ID
serial primary key name text not null.

56
00:03:05.460 --> 00:03:10.483
Owner text is null, and

57
00:03:10.483 --> 00:03:17.520
then description text not null.

58
00:03:17.520 --> 00:03:22.480
Okay, pretty straightforward, good.

59
00:03:22.480 --> 00:03:25.360
So let's go ahead and
rest, and then, sorry,

60
00:03:25.360 --> 00:03:29.400
I'm going to tell Spring Batch
also to initialize its own schema.

61
00:03:29.400 --> 00:03:33.801
Okay, so this is the thing that's
interesting is that Spring Batch is meant

62
00:03:33.801 --> 00:03:37.200
to run over hours, minutes,
days, hours, whatever.

63
00:03:37.200 --> 00:03:39.680
It's meant to handle large
amounts of data, right?

64
00:03:39.680 --> 00:03:43.596
And to make it easy to read data from
different disparate data sources and

65
00:03:43.596 --> 00:03:46.240
write it out to different
disparate data syncs.

66
00:03:46.240 --> 00:03:53.600
And because of that, we keep track of
the state of a job in a SQL database.

67
00:03:53.600 --> 00:03:54.880
So, here we go.

68
00:03:56.400 --> 00:03:58.360
I've just restarted the application,
refreshed it.

69
00:03:58.360 --> 00:03:59.240
Now I go over here,

70
00:03:59.240 --> 00:04:03.590
you can see that there's now a bunch of
tables here related to Spring Batch, okay?

71
00:04:03.590 --> 00:04:08.287
So we've got Spring Batch, job execution,
Spring Batch, job instance, Spring Batch,

72
00:04:08.287 --> 00:04:09.510
step execution, etc.

73
00:04:09.510 --> 00:04:14.550
So these are all the state of
a given job and its execution.

74
00:04:14.550 --> 00:04:17.190
So the way it works is in spring batch,
you create a job and

75
00:04:17.190 --> 00:04:19.590
then you can run that job
as many times as you want.

76
00:04:19.590 --> 00:04:23.746
You define the job as just a regular bean
using Spring's programming model, but

77
00:04:23.746 --> 00:04:26.676
then you can run as many
instances of that job as you want,

78
00:04:26.676 --> 00:04:30.170
and those get stored here,
Batch, job, instance.

79
00:04:30.170 --> 00:04:34.170
And each job has 0 to n steps, right?

80
00:04:34.170 --> 00:04:37.914
So each step can do one bit of IO.

81
00:04:37.914 --> 00:04:40.568
Okay, so you might have one
step that does, you know,

82
00:04:40.568 --> 00:04:43.307
it's not uncommon to have
a batch job that has one step.

83
00:04:43.307 --> 00:04:45.690
But you can also have
multiple steps in a sequence.

84
00:04:47.050 --> 00:04:51.800
You can have conditional steps, you can
have conditional logic saying, okay, based

85
00:04:51.800 --> 00:04:56.437
on the outcome of this, if this exited
incorrectly, then don't bother with this.

86
00:04:56.437 --> 00:04:57.970
If it didn't, then do this, right?

87
00:04:57.970 --> 00:05:00.850
You can have conditional flows
in your spring batch job.

88
00:05:00.850 --> 00:05:02.370
So let's create a spring batch job.

89
00:05:02.370 --> 00:05:05.074
And what we're going to do is we're
going to load data from a CSV file and

90
00:05:05.074 --> 00:05:06.530
then write it out to the SQL database.

91
00:05:06.530 --> 00:05:09.730
Pretty cliche, but
it'll demonstrate the concepts, okay?

92
00:05:12.130 --> 00:05:15.810
So, I'm going to create
a spring batch job CAD job.

93
00:05:17.980 --> 00:05:20.060
In order for me to build this,
I need a job builder.

94
00:05:20.060 --> 00:05:21.340
Get it?

95
00:05:21.340 --> 00:05:25.180
So, new jobBuilder.

96
00:05:25.180 --> 00:05:31.033
Sorry, I need a job repository,
JobRepository.

97
00:05:31.033 --> 00:05:33.419
Good, job builder, passing in the name.

98
00:05:33.419 --> 00:05:36.780
So I'll call this job because,
again, amazing with names.

99
00:05:36.780 --> 00:05:38.410
So I'm gonna pass in that repository.

100
00:05:38.410 --> 00:05:42.629
And that repository is the thing that's
gonna talk to the SQL database for us.

101
00:05:42.629 --> 00:05:46.150
Jobs, by default, they have parameters.

102
00:05:46.150 --> 00:05:51.072
Those parameters are used to create a key
that if the parameters are the same,

103
00:05:51.072 --> 00:05:53.510
will stop the job from running again.

104
00:05:53.510 --> 00:05:54.199
That is to say,

105
00:05:54.199 --> 00:05:57.750
imagine you're doing a process that runs
every night at midnight or whatever.

106
00:05:57.750 --> 00:06:02.269
You're doing some sort of analytics,
you want to prepare a report, or worse,

107
00:06:02.269 --> 00:06:06.518
imagine you're doing some destructive
thing that actually changes state

108
00:06:06.518 --> 00:06:07.350
somewhere.

109
00:06:07.350 --> 00:06:10.932
You want to make sure that thing
doesn't run more than once, okay?

110
00:06:10.932 --> 00:06:13.630
Or it doesn't run more than
once per 24 hours or whatever.

111
00:06:13.630 --> 00:06:18.350
So you might have as the key for
one of these jobs, the date, the year,

112
00:06:18.350 --> 00:06:20.190
month and day, whatever.

113
00:06:20.190 --> 00:06:23.436
And so if that key, if that job has
already been run with that key,

114
00:06:23.436 --> 00:06:25.470
don't run it again.

115
00:06:25.470 --> 00:06:26.990
And that's a job parameter.

116
00:06:26.990 --> 00:06:31.270
And you can have job parameters
that contribute to the key or not.

117
00:06:31.270 --> 00:06:34.746
But the point is, by default there are no
job parameters, which means that if you

118
00:06:34.746 --> 00:06:38.224
run the same job again and there's no
keys, there's nothing to distinguish one

119
00:06:38.224 --> 00:06:41.690
run from another, it won't run,
which is actually kind of confusing.

120
00:06:41.690 --> 00:06:46.334
So, to make it easy to develop, I use
the runid incrementer that contributes

121
00:06:46.334 --> 00:06:49.130
a parameter which is
going to just increment.

122
00:06:49.130 --> 00:06:51.130
It'll be monotonically incrementing, okay?

123
00:06:52.490 --> 00:06:54.090
Now, first step.

124
00:06:54.090 --> 00:06:54.970
What's my first step?

125
00:06:54.970 --> 00:06:56.810
Well, I'm going to inject a step.

126
00:06:56.810 --> 00:07:01.205
This is dependency injection where
I have a spring bean method that is

127
00:07:01.205 --> 00:07:05.298
injecting a pointer to the job
repository that is configured for

128
00:07:05.298 --> 00:07:07.750
me automatically by Spring Batch.

129
00:07:07.750 --> 00:07:10.790
And I'm going to inject a pointer to
the other step that I want to use.

130
00:07:10.790 --> 00:07:12.310
I haven't defined the step,
but I will, okay?

131
00:07:14.070 --> 00:07:14.870
So that's this.

132
00:07:14.870 --> 00:07:16.710
What's the issue here?

133
00:07:19.031 --> 00:07:23.230
It's the next start.

134
00:07:23.230 --> 00:07:23.910
There you go, okay?

135
00:07:23.910 --> 00:07:25.470
So now what about the step?

136
00:07:25.470 --> 00:07:26.646
Well, I'm gonna only have one step.

137
00:07:26.646 --> 00:07:29.470
As I said,
you can have as many as you want, okay?

138
00:07:29.470 --> 00:07:35.230
And I'll pass in the job repository again,
new step builder, okay?

139
00:07:35.230 --> 00:07:38.590
Dot, and then you have a chunk.

140
00:07:38.590 --> 00:07:41.630
Now, remember,
this is sequential data access, right?

141
00:07:41.630 --> 00:07:44.745
So if I have a million rows or
a billion rows,

142
00:07:44.745 --> 00:07:49.710
then you don't wanna do something
naive like select all, okay?

143
00:07:49.710 --> 00:07:52.897
What you would rather do
instead is chunk through it.

144
00:07:52.897 --> 00:07:57.603
Read a million rows at a time, or
100,000 or 10,000 or whatever.

145
00:07:57.603 --> 00:08:00.124
Some number that you
can handle effectively.

146
00:08:00.124 --> 00:08:02.415
And that you can also afford to lose.

147
00:08:02.415 --> 00:08:08.353
The chunk boundary is the amount of
data that wraps a transaction, right?

148
00:08:08.353 --> 00:08:12.553
So when you read 1000 rows into memory,
you then process it.

149
00:08:12.553 --> 00:08:13.851
If something goes wrong,

150
00:08:13.851 --> 00:08:17.473
Spring Batch will roll back
the transaction along the last chunk size.

151
00:08:17.473 --> 00:08:20.523
So if your chunk size is 10,000,
then it'll roll back and

152
00:08:20.523 --> 00:08:23.233
mark as incomplete that
chunk of 10,000 rows.

153
00:08:23.233 --> 00:08:27.393
Even if, sorry,
9,999 of them process successfully,

154
00:08:27.393 --> 00:08:31.477
if it fails on the last one,
you'll lose all 10,000.

155
00:08:31.477 --> 00:08:33.003
It's the whole chunk size, right?

156
00:08:33.003 --> 00:08:36.898
So it's up to you to prescribe
what the chunk size should be, but

157
00:08:36.898 --> 00:08:40.144
it should be some number that
you can afford to lose and

158
00:08:40.144 --> 00:08:43.643
also that you can keep in
memory at the same time, okay?

159
00:08:43.643 --> 00:08:46.443
So start small and scale up until
you find a number that works.

160
00:08:46.443 --> 00:08:50.837
So if you've got a billion rows, the idea
here is that we're going to read 10

161
00:08:50.837 --> 00:08:55.431
records at a time from a source, do some
optional processing on it, and then write

162
00:08:55.431 --> 00:09:00.373
out to a sink all at once, at 10 at
a time in a single transaction, right?

163
00:09:00.373 --> 00:09:03.887
So you don't want to try and write a
single transaction with a billion rows, so

164
00:09:03.887 --> 00:09:06.373
don't make the chunk size a billion,
right?

165
00:09:06.373 --> 00:09:09.173
Make it something that's reasonable for
your output data source, okay?

166
00:09:11.013 --> 00:09:16.599
Okay, and because that's a question
of sort of your transactions,

167
00:09:16.599 --> 00:09:20.373
you have this platform
transaction manager.

168
00:09:20.373 --> 00:09:23.210
You have to give it a pointer to
a thing managed by Spring called

169
00:09:23.210 --> 00:09:24.803
the Transaction Manager, okay?

170
00:09:24.803 --> 00:09:29.443
This is an abstraction that we
have that you can use to talk to.

171
00:09:29.443 --> 00:09:30.803
There are implementations of this.

172
00:09:30.803 --> 00:09:32.283
This is a portable service abstraction.

173
00:09:32.283 --> 00:09:36.649
There are implementations of this
interface for SQL, for Neo4J, for MongoDB,

174
00:09:36.649 --> 00:09:40.129
for JPA, for JTA, for
distributed transactions, for Neo4J,

175
00:09:40.129 --> 00:09:43.493
for, I mean everything,
everything that has transactions.

176
00:09:43.493 --> 00:09:44.467
RabbitMQ, Kafka,

177
00:09:44.467 --> 00:09:48.273
there are implementations of this
that you can find everywhere, right?

178
00:09:48.273 --> 00:09:53.744
And behind the scenes, when you are using
Spring's Add Transactional support,

179
00:09:53.744 --> 00:09:58.433
let's say you have a service that
has a method that returns whatever.

180
00:09:58.433 --> 00:10:03.033
If you put Add Transactional on that,
Spring will automatically decorate that.

181
00:10:03.033 --> 00:10:03.953
It's like an attribute, right?

182
00:10:03.953 --> 00:10:06.793
It'll automatically decorate
that method in a transaction.

183
00:10:06.793 --> 00:10:11.233
It'll start a transaction beforehand and
commit it after the method is run, right?

184
00:10:11.233 --> 00:10:14.548
And that's because somebody's configured
a bean of transaction manager in

185
00:10:14.548 --> 00:10:17.123
the bean context,
in the application context.

186
00:10:17.123 --> 00:10:21.391
And that, which kind of transaction
manager you're using it's up to you, but

187
00:10:21.391 --> 00:10:24.843
by default, Spring Batch,
Spring data JDBC, all that stuff.

188
00:10:24.843 --> 00:10:29.163
I'll have a SQL data source transaction
manager in auto configured for me.

189
00:10:29.163 --> 00:10:34.212
Okay, so the chunk is the amount
of data I'm going to read and

190
00:10:34.212 --> 00:10:36.883
write in a single go.

191
00:10:36.883 --> 00:10:39.083
But what about the reading and
the writing and what kind of data?

192
00:10:39.083 --> 00:10:41.483
Well, again,
my goal is to read from a CSV file.

193
00:10:41.483 --> 00:10:45.693
So I've got a domain object called dog,
okay?

194
00:10:47.453 --> 00:10:49.625
And I'm going to read
that from a CSV file and

195
00:10:49.625 --> 00:10:52.173
I'm going to write that
out to a SQL database.

196
00:10:52.173 --> 00:10:57.061
So the chunk is going to read from a CSV
file into the domain type called dog, and

197
00:10:57.061 --> 00:10:59.893
I'm going to write the DOG
data to a CSV file.

198
00:10:59.893 --> 00:11:03.293
So the input type and
the output type are the same in this case.

199
00:11:03.293 --> 00:11:06.380
They don't have to be, though,
because between a reader and a writer,

200
00:11:06.380 --> 00:11:07.783
you can put a processor.

201
00:11:07.783 --> 00:11:09.863
The processor might transform the thing.

202
00:11:09.863 --> 00:11:13.143
You might affect a change.

203
00:11:13.143 --> 00:11:15.815
What if you're terrible and
it's like the night before Christmas and

204
00:11:15.815 --> 00:11:18.543
you want to like, raise the price of
all the toys or something, right?

205
00:11:18.543 --> 00:11:21.863
If you're a terrible person,
you can raise the price 10%.

206
00:11:21.863 --> 00:11:24.296
You read all the data from the catalog,

207
00:11:24.296 --> 00:11:28.743
multiply the price times 1.1 and
then write the thing out, right?

208
00:11:28.743 --> 00:11:29.783
You can do that in a batch.

209
00:11:29.783 --> 00:11:32.103
That's actually something you could
probably also do in a SQL database.

210
00:11:32.103 --> 00:11:33.643
But the point is you can do it in batch,
okay?

211
00:11:33.643 --> 00:11:37.693
So read, okay?

212
00:11:40.333 --> 00:11:46.173
ItemReader, ItemReader, okay?

213
00:11:46.173 --> 00:11:50.173
And the ItemReader is gonna be
an ItemReader of type Dog, so

214
00:11:50.173 --> 00:11:51.533
ItemReader, Dog.

215
00:11:53.133 --> 00:11:55.373
Okay, we'll come back to this in a second.

216
00:11:55.373 --> 00:11:57.213
This won't work to do.

217
00:11:57.213 --> 00:12:00.743
And then we need the itemWriter.

218
00:12:00.743 --> 00:12:08.343
So itemWriter of type dog, great, okay?

219
00:12:10.823 --> 00:12:13.863
And same thing down here, so itemWriter.

220
00:12:15.623 --> 00:12:22.103
Okay, so now we say reader is equal to
reader, and writer is writer, okay?

221
00:12:22.103 --> 00:12:25.063
The contract for these item readers and
item writers is very simple.

222
00:12:25.063 --> 00:12:26.303
You can create your own, no problem.

223
00:12:26.303 --> 00:12:27.953
So here's the itemReader contract, right?

224
00:12:29.233 --> 00:12:34.593
Your job is to return a T,
which is a generic type there when asked.

225
00:12:34.593 --> 00:12:39.859
So we have item readers, rather,
for flat files, for in memory, for

226
00:12:39.859 --> 00:12:45.393
JBDC, for JMS, for Kafka, for LDIF,
which is like image metadata.

227
00:12:45.393 --> 00:12:47.113
Sorry, that's directory metadata?

228
00:12:47.113 --> 00:12:48.193
Yeah, directory metadata.

229
00:12:48.193 --> 00:12:52.263
You've got MongoDB,
cursor reading, you've got paging,

230
00:12:52.263 --> 00:12:55.173
you've got redis, you've got whatever.

231
00:12:55.173 --> 00:12:59.331
I mean, stacks for XML documents,
you've got all sorts of stuff in here, but

232
00:12:59.331 --> 00:13:01.733
you can also easily create your own,
right?

233
00:13:01.733 --> 00:13:03.173
So item readers are just,
they read one thing at a time.

234
00:13:04.453 --> 00:13:08.213
The itemWriter writes a chunk, right?

235
00:13:08.213 --> 00:13:12.933
So your job is to write
one chunk's worth of data.

236
00:13:12.933 --> 00:13:18.453
Well, if the number that we specified
is 10, then this chunk size will be 10.

237
00:13:18.453 --> 00:13:22.393
This is actually a list
you can foreach over it.

238
00:13:22.393 --> 00:13:23.233
There's 10 items.

239
00:13:23.233 --> 00:13:25.833
So at most you'll be asked
to write 10 items, okay?

240
00:13:25.833 --> 00:13:28.953
So I'm gonna use the batteries included,
right?

241
00:13:28.953 --> 00:13:30.993
I'm going to use the ones
that come in Spring Batch.

242
00:13:30.993 --> 00:13:34.053
So I'll use the, rather not the new,

243
00:13:34.053 --> 00:13:38.273
the flat file item reader
to read a flat file, okay?

244
00:13:39.793 --> 00:13:40.873
And what flat file?

245
00:13:40.873 --> 00:13:45.937
Well, it's gonna be a CSV file that I have

246
00:13:45.937 --> 00:13:51.163
on my desktop here, dogs.csv resource.

247
00:13:52.843 --> 00:13:54.203
Sorry, not there, file.

248
00:13:56.043 --> 00:13:57.083
How do I get rid of that?

249
00:13:57.083 --> 00:13:57.963
Hide this.

250
00:13:57.963 --> 00:13:59.003
There we go.

251
00:13:59.003 --> 00:14:01.803
And put the resource here, good.

252
00:14:01.803 --> 00:14:05.643
So it'll be this flat file, okay?

253
00:14:05.643 --> 00:14:07.283
That flat file is a CSV file.

254
00:14:07.283 --> 00:14:11.245
So let's go take a look at that over here,

255
00:14:11.245 --> 00:14:18.283
Desktop/talk/dogs/dogs.csv, sorry,
talk.csv, there you go.

256
00:14:18.283 --> 00:14:21.347
You can see I've got a bunch of dogs
there that I need to load into memory or

257
00:14:21.347 --> 00:14:22.443
load into the program.

258
00:14:22.443 --> 00:14:24.203
And it's got a header row, right?

259
00:14:24.203 --> 00:14:29.039
So I'm going to take the header and
I want to tell this thing that it's, so

260
00:14:29.039 --> 00:14:34.043
let's see, delimited names,
here's the row, the column names rather.

261
00:14:35.803 --> 00:14:42.433
And I'm going to tell it to skip
the first line, lines to skip is one.

262
00:14:42.433 --> 00:14:47.393
I'm going to tell it to map
the CSV data to a Java object.

263
00:14:47.393 --> 00:14:49.411
Kind of like that row mapper
that you saw earlier,

264
00:14:49.411 --> 00:14:52.033
except here it's gonna be
a fieldSetMapper, right?

265
00:14:52.033 --> 00:14:57.110
So my job is to take the data from the csv

266
00:14:57.110 --> 00:15:01.555
and fieldSet.readInt(id),

267
00:15:01.555 --> 00:15:07.430
fieldSet.readString("name"), and

268
00:15:07.430 --> 00:15:12.203
then, okay, so name, what is it?

269
00:15:12.203 --> 00:15:17.523
Owner and then description, okay?

270
00:15:17.523 --> 00:15:21.563
And of course this can be a lambda,
much nicer.

271
00:15:22.763 --> 00:15:26.363
Okay, oops, new dog.

272
00:15:29.893 --> 00:15:32.333
Okay, goody.

273
00:15:32.333 --> 00:15:35.413
So there's that right, flat file item
reader that's going to read the data.

274
00:15:35.413 --> 00:15:39.093
If everything's gone to plan, then I
should see it all printed out here, right?

275
00:15:43.733 --> 00:15:47.893
Okay, let's just see if that works so
far, nope.

276
00:15:49.679 --> 00:15:50.613
I need to give it a name.

277
00:15:50.613 --> 00:15:52.910
Remember, it's going to store
this stuff in a database.

278
00:15:52.910 --> 00:15:56.819
So for things where you need durable
state, you have to provide a name.

279
00:15:56.819 --> 00:16:01.831
So okay, looks like it worked.

280
00:16:01.831 --> 00:16:05.827
So there's the,
you can see I've got two different chunks.

281
00:16:05.827 --> 00:16:07.307
There's only 18 records, I think.

282
00:16:07.307 --> 00:16:12.128
So, here's the first chunk and
here's the slightly smaller second chunk

283
00:16:12.128 --> 00:16:16.490
because there's not quite 20 records,
it's one chunk of 10,

284
00:16:16.490 --> 00:16:20.947
which is the max, and then 8,
which is the balance, the modulus.

285
00:16:20.947 --> 00:16:24.802
Okay, so
it's clearly able to read the data, right?

286
00:16:24.802 --> 00:16:26.607
By the time we get to the itemWriter,

287
00:16:26.607 --> 00:16:29.637
I clearly have a collection
of dogs from the CSV data.

288
00:16:29.637 --> 00:16:31.397
But now I want to write
this to a SQL database.

289
00:16:31.397 --> 00:16:33.717
Remember, I've got this
nice SQL table over here.

290
00:16:35.397 --> 00:16:36.797
No dogs, empty, okay?

291
00:16:36.797 --> 00:16:39.237
So our job now is to write this out.

292
00:16:39.237 --> 00:16:43.011
And I'm not gonna instead of
having my own item writer,

293
00:16:43.011 --> 00:16:48.815
I'll just use the JDBC batch itemWriter
builder for data of type dog.build, okay?

294
00:16:48.815 --> 00:16:53.645
In order for this to work, obviously I'm
going to need to inject the data source,

295
00:16:53.645 --> 00:16:55.327
so data source..

296
00:16:55.327 --> 00:17:02.207
And I'll need to provide the SQL
command or incantation or whatever.

297
00:17:02.207 --> 00:17:05.727
So insert into dogs, id,
name, description, owner.

298
00:17:05.727 --> 00:17:06.447
Do I have that?

299
00:17:07.966 --> 00:17:08.687
Yeah, I do.

300
00:17:08.687 --> 00:17:14.807
Okay, values, and then I don't know why
it keeps insisting on putting that in,

301
00:17:14.807 --> 00:17:17.247
but, 1, 2, 3, 4, okay?

302
00:17:21.177 --> 00:17:22.017
Okay, so there's that.

303
00:17:22.017 --> 00:17:24.737
And then I'm going to assert the updates.

304
00:17:24.737 --> 00:17:28.457
Sure, I'm gonna do a prepared setter,
so new ItemPreparedSetter.

305
00:17:29.817 --> 00:17:33.177
And the idea here is I've got an item,
I've got a single row.

306
00:17:33.177 --> 00:17:36.123
My job is to set the prepared
statement parameters to update,

307
00:17:36.123 --> 00:17:37.377
to write into that thing.

308
00:17:37.377 --> 00:17:39.907
And it's going to accumulate
10 of these at a time and

309
00:17:39.907 --> 00:17:42.657
then commit the transaction
with all 10 at the same time.

310
00:17:42.657 --> 00:17:45.245
It won't do one at a time, right?

311
00:17:45.245 --> 00:17:51.687
So item, sorry, prepared statement,
set, int the first one is the ID fine.

312
00:17:51.687 --> 00:17:55.207
Set string, sorry, string.

313
00:17:57.207 --> 00:17:59.999
Okay, item.name,

314
00:17:59.999 --> 00:18:05.447
item.description, item.owner.

315
00:18:05.447 --> 00:18:10.537
And that'll be 1, 2, 3, 4, goody.

316
00:18:10.537 --> 00:18:12.977
Okay, inject that.

317
00:18:12.977 --> 00:18:14.457
I think that'll work, let's try it.

318
00:18:14.457 --> 00:18:17.833
I don't know,
only one way to find out, and

319
00:18:17.833 --> 00:18:21.217
not quite table dogs does not exist, sure.

320
00:18:24.737 --> 00:18:30.097
Okay, so now there we go.

321
00:18:30.097 --> 00:18:31.537
There's our 18 records.

322
00:18:31.537 --> 00:18:34.657
So it's actually, you know,
we've successfully ETL that data.

323
00:18:35.787 --> 00:18:42.107
And while this is a little overkill for
just that one little row.

324
00:18:43.147 --> 00:18:44.187
What just happened there?

325
00:18:45.867 --> 00:18:48.507
It's a little overkill for
just that one little row.

326
00:18:48.507 --> 00:18:51.941
What we've got there is we've
got the job it has one step and

327
00:18:51.941 --> 00:18:56.107
the step has a reader and
a writer, and that's it, right?

328
00:18:56.107 --> 00:18:59.187
Where things get really interesting is
when you start building up multiple steps.

329
00:18:59.187 --> 00:19:01.923
So you can have another step and
another, and

330
00:19:01.923 --> 00:19:04.587
these things can actually be conditional.

331
00:19:04.587 --> 00:19:08.387
You can have steps that only execute
if some previous statement was true.

332
00:19:08.387 --> 00:19:12.696
The other thing that's interesting
is that the steps themselves,

333
00:19:12.696 --> 00:19:16.787
there's things you can do in terms of,
let's see, is it here?

334
00:19:22.363 --> 00:19:25.547
I forget how, but
you can do things in the same JVM.

335
00:19:25.547 --> 00:19:28.127
You can have these steps
execute in concurrence.

336
00:19:28.127 --> 00:19:34.167
You can also use, they call it
partitioned steps and remote chunking.

337
00:19:34.167 --> 00:19:38.355
And basically there's two different
schemes there where Spring Batch can in

338
00:19:38.355 --> 00:19:43.007
turn send the work to another node to
process the work elsewhere concurrently.

339
00:19:43.007 --> 00:19:45.167
So basically there's two
different ways to do this.

340
00:19:45.167 --> 00:19:49.327
One is I have,
let's say I have RabbitMQ or Kafka, right?

341
00:19:49.327 --> 00:19:54.144
I send from the leader node,
I can send via these

342
00:19:54.144 --> 00:19:59.577
either remote chunking or
partitioned processing.

343
00:19:59.577 --> 00:20:05.060
I can send the rows to read in
a source data set to another node and

344
00:20:05.060 --> 00:20:10.977
it'll be responsible for seeking and
then processing those rows.

345
00:20:10.977 --> 00:20:16.177
And it'll respond back to the leader
node saying, hey, I'm done, okay?

346
00:20:16.177 --> 00:20:17.857
That's one kind of processing.

347
00:20:17.857 --> 00:20:22.769
The other is you can send not just
the range, as we just discussed,

348
00:20:22.769 --> 00:20:27.217
you can send the actual data
in Kafka to another node.

349
00:20:27.217 --> 00:20:28.271
You can say, okay,

350
00:20:28.271 --> 00:20:32.239
I've read these 10 records into RAM on
the source node on this other node,

351
00:20:32.239 --> 00:20:36.417
please process them and then send me
back the confirmation that you're done.

352
00:20:36.417 --> 00:20:43.377
The effect is that you have the ability
to distribute the work across a cluster.

353
00:20:43.377 --> 00:20:46.659
You can have 100 different nodes
processing work in concurrence, right?

354
00:20:46.659 --> 00:20:49.630
So even though I'm doing it in
a sort of linear fashion here,

355
00:20:49.630 --> 00:20:53.437
spring batch scales up and out, and
that's one of the things I like about it.

