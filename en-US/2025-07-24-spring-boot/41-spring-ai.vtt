WEBVTT

1
00:00:00.160 --> 00:00:03.011
&gt;&gt; Josh Long: I wanted to save
the best for last, right?

2
00:00:03.011 --> 00:00:08.240
This is a Frontend Master's course,
it's awesome.

3
00:00:08.240 --> 00:00:12.880
And right now the front end
is changing a lot, right?

4
00:00:12.880 --> 00:00:17.283
It's a little hyperbolic today, but
I don't think we can argue with the idea

5
00:00:17.283 --> 00:00:20.465
that a lot of things are going
to be done through a model,

6
00:00:20.465 --> 00:00:23.400
an AI model of some sort
in the near future, right?

7
00:00:23.400 --> 00:00:27.209
Maybe not tomorrow, maybe not next year,
but for some use cases today and for

8
00:00:27.209 --> 00:00:31.130
a lot of use cases, maybe next year,
maybe 10 years from now, whatever.

9
00:00:31.130 --> 00:00:34.090
The model is becoming the new
user interface, right?

10
00:00:34.090 --> 00:00:37.562
The client, the chat client is
becoming the new user interface and

11
00:00:37.562 --> 00:00:41.292
we're seeing this, you know,
this sort of utility balloon, right?

12
00:00:41.292 --> 00:00:42.010
It's not great.

13
00:00:42.010 --> 00:00:45.148
AI as we know it today is not great for
everything, obviously, but

14
00:00:45.148 --> 00:00:48.250
there's no doubt it has
some good use cases.

15
00:00:48.250 --> 00:00:51.695
And so I think we in the Java community
are well positioned to take advantage of

16
00:00:51.695 --> 00:00:53.420
this moment in time.

17
00:00:53.420 --> 00:00:57.860
Right now most of what people talk about
when they talk about AI engineering is

18
00:00:57.860 --> 00:01:01.260
just integrating,
it's calling a REST API, right?

19
00:01:01.260 --> 00:01:02.220
This is not hard.

20
00:01:02.220 --> 00:01:06.300
Most of us are not going to
be building a model in AI.

21
00:01:06.300 --> 00:01:09.660
That's not what most people are doing
when they talk about AI engineering.

22
00:01:09.660 --> 00:01:13.122
They're not building their own models
in the same way that most people aren't

23
00:01:13.122 --> 00:01:15.260
building their own SQL databases either.

24
00:01:15.260 --> 00:01:19.786
They're using one that's been pre
built and stabilized and expanded and

25
00:01:19.786 --> 00:01:21.590
grown and all that.

26
00:01:21.590 --> 00:01:26.472
So if that's the name of the game and
if the integration protocol du jour is

27
00:01:26.472 --> 00:01:31.069
HTTP for most of these models,
then there's no reason to be afraid.

28
00:01:31.069 --> 00:01:33.350
This is a really easy problem to solve,
right?

29
00:01:33.350 --> 00:01:36.215
AI is here, it's easy to use,
it's easy to integrate,

30
00:01:36.215 --> 00:01:40.350
and arguably there is no better place to
integrate it than your Java code, right?

31
00:01:40.350 --> 00:01:44.511
Because where's most of the business
logic in most organizations at any size

32
00:01:44.511 --> 00:01:46.360
written in these days?

33
00:01:46.360 --> 00:01:47.320
I'll give you a hint.

34
00:01:47.320 --> 00:01:49.720
It's not Python, it's Java, right?

35
00:01:49.720 --> 00:01:51.120
Look at the enterprise, that's Java.

36
00:01:51.120 --> 00:01:53.480
The data scientists, sure,
they're using Python.

37
00:01:53.480 --> 00:01:55.034
That's not where your business logic is,

38
00:01:55.034 --> 00:01:57.880
that's not what's driving the scale
on your services and your systems.

39
00:01:57.880 --> 00:02:02.387
So what we wanna do is to make your
business logic and the data that

40
00:02:02.387 --> 00:02:07.800
business logic guards available to
the models to see if that can help.

41
00:02:07.800 --> 00:02:10.120
And so,
we created this project called Spring AI.

42
00:02:10.120 --> 00:02:13.393
So what we're gonna do is
we're gonna build, we're gonna

43
00:02:13.393 --> 00:02:18.200
revisit our old friend Prancer and answer
the question, how did we find him, right?

44
00:02:18.200 --> 00:02:20.200
How did we learn about Prancer?

45
00:02:20.200 --> 00:02:23.880
How did we, like, I want to actually go
through the motions of adopting that dog.

46
00:02:23.880 --> 00:02:25.800
We know how to do the adoption.

47
00:02:25.800 --> 00:02:30.680
We know actually how to update the
adoption table, right, with that field.

48
00:02:30.680 --> 00:02:34.160
But we only knew about Prancer
because I saw this thing on the news.

49
00:02:34.160 --> 00:02:38.714
This dog was so hilariously described in
that advert that somebody put it out there

50
00:02:38.714 --> 00:02:40.040
and it went on the news.

51
00:02:40.040 --> 00:02:41.440
That's how I found out about Prancer.

52
00:02:41.440 --> 00:02:47.006
That's not how one imagines, at least most
people discover the dogs of their dreams,

53
00:02:47.006 --> 00:02:49.380
or in my case, nightmares, right?

54
00:02:49.380 --> 00:02:51.350
They go to the shelter,
they have a conversation.

55
00:02:51.350 --> 00:02:52.190
There's a whole process.

56
00:02:52.190 --> 00:02:54.630
There's paperwork, there's formality,
all that kind of stuff.

57
00:02:54.630 --> 00:02:58.892
So what I want to do is to build an
assistant to help people adopt dogs from

58
00:02:58.892 --> 00:03:02.390
our fictitious dog adoption
agency called Pooch Palace.

59
00:03:02.390 --> 00:03:04.350
Okay, so
that's what we're going to do today.

60
00:03:04.350 --> 00:03:06.070
That's our purview, that's our mandate.

61
00:03:06.070 --> 00:03:08.030
So we're going to go here, desktop talk.

62
00:03:08.030 --> 00:03:08.530
Okay.

63
00:03:09.550 --> 00:03:11.630
Init database.

64
00:03:11.630 --> 00:03:15.150
Okay, I'm goinna make sure I
have the dog database there.

65
00:03:15.150 --> 00:03:16.573
And we're going to go back here and

66
00:03:16.573 --> 00:03:18.793
we're going to build a brand new project,
GraalVM.

67
00:03:18.793 --> 00:03:19.950
We're gonna use the web support.

68
00:03:19.950 --> 00:03:22.990
We're going to use OpenAI.

69
00:03:22.990 --> 00:03:24.590
Now I'm using OpenAI.

70
00:03:24.590 --> 00:03:25.590
You can use whatever you want.

71
00:03:25.590 --> 00:03:26.310
You can use Ollama.

72
00:03:26.310 --> 00:03:27.870
Ollama runs locally.

73
00:03:27.870 --> 00:03:30.630
It's like Docker for models.

74
00:03:30.630 --> 00:03:31.150
Okay.

75
00:03:31.150 --> 00:03:32.110
It's a container.

76
00:03:32.110 --> 00:03:34.980
Actually, Facebook started this all,
right?

77
00:03:34.980 --> 00:03:38.980
OpenAI is a proprietary hosted
software as a service AI model.

78
00:03:38.980 --> 00:03:42.870
But Facebook released a open source
AI model called Llama that was so

79
00:03:42.870 --> 00:03:46.554
popular that it kind of,
because of that, they standardized,

80
00:03:46.554 --> 00:03:50.239
they sort of extracted out the C
bindings required to run it, and

81
00:03:50.239 --> 00:03:53.860
that became a container
program called Ollama, right?

82
00:03:53.860 --> 00:03:56.060
And you can use Ollama to
then run any other model.

83
00:03:56.060 --> 00:04:00.180
There's thousands of models that
you can run via Ollama, okay?

84
00:04:00.180 --> 00:04:03.250
And I think Docker sort of
feeling a little left out.

85
00:04:03.250 --> 00:04:07.770
They just recently announced a feature
here called Docker Model Run.

86
00:04:07.770 --> 00:04:09.690
So Docker Model Runner.

87
00:04:09.690 --> 00:04:11.643
And we just announced this.

88
00:04:11.643 --> 00:04:15.882
We just announced support for
this in Spring AI M7 in,

89
00:04:15.882 --> 00:04:18.290
I don't know, what is it now?

90
00:04:18.290 --> 00:04:21.210
14 days ago, 13 days ago,
not even two weeks ago.

91
00:04:21.210 --> 00:04:25.470
Okay, so this just came out a few weeks
ago in Docker and we have support for

92
00:04:25.470 --> 00:04:26.930
it already in Spring AI.

93
00:04:26.930 --> 00:04:29.620
So you can use Docker to
run models now as well.

94
00:04:29.620 --> 00:04:32.260
That's why I didn't ask you
to install Ollama, right?

95
00:04:32.260 --> 00:04:34.220
You can use Docker if you want, right.

96
00:04:34.220 --> 00:04:35.620
There's infinite variety there.

97
00:04:35.620 --> 00:04:38.866
And this is something that's kind of
different from the other that I use

98
00:04:38.866 --> 00:04:41.353
the metaphor that it's sort
of like a data source, but

99
00:04:41.353 --> 00:04:44.871
it's actually a little different in
that it's not unreasonable to have more

100
00:04:44.871 --> 00:04:46.740
than one model in the same application.

101
00:04:46.740 --> 00:04:48.689
Whereas if somebody had more than one,

102
00:04:48.689 --> 00:04:52.780
if you have like two postgres in your
application, that might be a smell, right?

103
00:04:52.780 --> 00:04:55.521
If you saw a program that had five
different databases, I might wonder

104
00:04:55.521 --> 00:04:58.460
if that's maybe a little overwhelmed,
a little overstuffed, you know.

105
00:04:58.460 --> 00:05:01.260
But in the model scenario, it's not
uncommon to have more than one model.

106
00:05:01.260 --> 00:05:02.810
Okay.
And you can think of,

107
00:05:02.810 --> 00:05:06.380
just imagine the efficiencies,
imagine the size differences.

108
00:05:06.380 --> 00:05:08.620
Some models are very sophisticated,
they can do anything.

109
00:05:08.620 --> 00:05:12.780
Some like Ollama, I'm telling you, with
Ollama you could do darn near anything.

110
00:05:12.780 --> 00:05:14.300
It is quite good, right?

111
00:05:14.300 --> 00:05:16.028
Google, they have one called Gemma,

112
00:05:16.028 --> 00:05:18.700
which is like a lightweight
one that you can run locally.

113
00:05:18.700 --> 00:05:22.910
These are small models that are really,
really fast and really, really good.

114
00:05:22.910 --> 00:05:23.535
And they can do,

115
00:05:23.535 --> 00:05:27.030
they can write poetry in five different
languages in the same request.

116
00:05:27.030 --> 00:05:30.502
They can tell you about history,
they can do all sorts of things, right,

117
00:05:30.502 --> 00:05:33.030
Just like you'd ask from ChatGPT or
OpenAI.

118
00:05:33.030 --> 00:05:35.030
Some models, however, are much more terse.

119
00:05:35.030 --> 00:05:39.030
They're lightning fast,
don't hesitate, not at all.

120
00:05:39.030 --> 00:05:43.613
But also they are, maybe they only speak
a subset of English or one language and

121
00:05:43.613 --> 00:05:46.750
they don't have a full grasp of history or
whatever.

122
00:05:46.750 --> 00:05:47.365
You know, they,

123
00:05:47.365 --> 00:05:49.790
they can't write code because they
don't know anything about code.

124
00:05:49.790 --> 00:05:51.990
They're optimized for certain use cases.

125
00:05:51.990 --> 00:05:56.711
So it's not unreasonable to have an
application that uses maybe a fast model

126
00:05:56.711 --> 00:06:01.579
to do quick hot path analysis and then
forward the request onwards down to a more

127
00:06:01.579 --> 00:06:05.910
full featured, full fledged model
as well for final analysis.

128
00:06:05.910 --> 00:06:07.230
So I'm not gonna do that.

129
00:06:07.230 --> 00:06:10.030
But my point is you have a lot of
options when you use Spring AI.

130
00:06:10.030 --> 00:06:12.870
We support literally dozens
of different models.

131
00:06:12.870 --> 00:06:16.970
And by the way,
some models we don't even OpenAI.

132
00:06:16.970 --> 00:06:17.930
Kind of like with S3.

133
00:06:17.930 --> 00:06:22.359
You know how there's like 500 different
services that are S3 compatible, so

134
00:06:22.359 --> 00:06:25.970
it's not Amazon that hosts it,
but they speak the same protocol.

135
00:06:25.970 --> 00:06:29.250
The same has kind of become the case for
OpenAI.

136
00:06:29.250 --> 00:06:32.269
There are dozens of different
models from all over the world in

137
00:06:32.269 --> 00:06:35.650
different countries and
all that stuff that are OpenAI compatible.

138
00:06:35.650 --> 00:06:39.826
That is to say, if you speak
the OpenAI wire protocol, the REST API,

139
00:06:39.826 --> 00:06:43.900
then you can talk to their model
even though it's not OpenAI.

140
00:06:43.900 --> 00:06:46.220
Okay, so we're going to use OpenAI.

141
00:06:46.220 --> 00:06:47.580
I've got that in the class path here.

142
00:06:47.580 --> 00:06:49.102
We're gonna bring in Postgres, right?

143
00:06:49.102 --> 00:06:50.100
We got a Postgres driver.

144
00:06:50.100 --> 00:06:51.580
We're going to bring in.

145
00:06:51.580 --> 00:06:55.220
Not Postgres, we're going to bring
in PG Vector, actually, okay?

146
00:06:55.220 --> 00:06:59.420
PG Vector is Postgres,
plus some extra stuff, okay?

147
00:07:01.500 --> 00:07:04.860
And I think that's pretty good.

148
00:07:07.190 --> 00:07:08.510
That seem right?
That seems like it's okay.

149
00:07:08.510 --> 00:07:10.790
Okay, good.
So we got PGvector.

150
00:07:10.790 --> 00:07:11.484
What else?

151
00:07:11.484 --> 00:07:13.670
We want the MCP client support, okay?

152
00:07:13.670 --> 00:07:15.670
We'll talk about what that
is in a second as well.

153
00:07:15.670 --> 00:07:18.470
And I think I'm pretty happy there.

154
00:07:18.470 --> 00:07:21.110
We're gonna call this adoptions.

155
00:07:21.110 --> 00:07:24.310
Okay, hit Enter, go here.

156
00:07:24.310 --> 00:07:27.190
There's my database full of doggos.

157
00:07:27.190 --> 00:07:29.430
UAO adoptions, zip.

158
00:07:31.600 --> 00:07:35.000
Now we're going to build a application
that talks to that SQL database.

159
00:07:35.000 --> 00:07:38.400
So same as always, same setup here.

160
00:07:38.400 --> 00:07:44.192
Secret username, my user URL,

161
00:07:44.192 --> 00:07:50.414
jdbc, postgres, QL localhost,

162
00:07:50.414 --> 00:07:57.930
my database adoptions application.

163
00:07:57.930 --> 00:07:59.970
And we're going to build
a simple controller here.

164
00:07:59.970 --> 00:08:03.248
Just an assistant to help
people adopt their dogs,

165
00:08:03.248 --> 00:08:06.090
to answer questions about dogs, okay?

166
00:08:06.090 --> 00:08:09.810
For this fictitious dog adoption
agency called Pooch Palace.

167
00:08:09.810 --> 00:08:13.410
So we can imagine having a username maybe.

168
00:08:13.410 --> 00:08:16.523
Maybe if I had set up OAUTH or
some sort of security context,

169
00:08:16.523 --> 00:08:20.650
I could just infer that from the current
authenticated user, the principal.

170
00:08:20.650 --> 00:08:24.730
But I did not because I'm bad as a person,
okay?

171
00:08:24.730 --> 00:08:26.570
And inquire.

172
00:08:26.570 --> 00:08:29.210
Right.
And then this will be a request parameter.

173
00:08:30.890 --> 00:08:32.970
Param, there you go, or param.

174
00:08:34.490 --> 00:08:35.450
Good.

175
00:08:35.450 --> 00:08:42.890
And I'm going to now inject
private final chat client.

176
00:08:42.890 --> 00:08:45.051
Okay.
The chat client is my interface,

177
00:08:45.051 --> 00:08:48.560
just like the REST client,
the JDBC client, the web client.

178
00:08:48.560 --> 00:08:49.840
You've seen a bunch of clients so far.

179
00:08:49.840 --> 00:08:53.280
The chat client is my
interface to talk to a model.

180
00:08:53.280 --> 00:08:53.960
What model?

181
00:08:53.960 --> 00:08:55.680
Well, like I said,
I'm going to use OpenAI.

182
00:08:55.680 --> 00:08:57.200
How did I connect to OpenAI?

183
00:08:57.200 --> 00:08:59.120
I have an API key, Right?

184
00:08:59.120 --> 00:09:00.320
I've specified that.

185
00:09:00.320 --> 00:09:01.640
And there's a lot of other prop.

186
00:09:01.640 --> 00:09:04.360
Like if you're Talking to actually OpenAI,
that's usually enough.

187
00:09:04.360 --> 00:09:05.680
You specify the API key.

188
00:09:05.680 --> 00:09:08.731
I already did this though,
before I got on stage, my friends,

189
00:09:08.731 --> 00:09:12.880
I exported an environment variable and
then that's running in my shell, right?

190
00:09:12.880 --> 00:09:15.311
So just trust me that it's done and
forgive me for

191
00:09:15.311 --> 00:09:17.390
not leaking my API credential, okay?

192
00:09:17.390 --> 00:09:20.270
You just need to remember to
do that when you log in, okay?

193
00:09:20.270 --> 00:09:22.550
And for most, if you're using
actual OpenAI, that's enough.

194
00:09:22.550 --> 00:09:24.882
But a lot of times you might
be using something else.

195
00:09:24.882 --> 00:09:28.590
So notice that we have base URL for
these different OpenAI modules, right?

196
00:09:29.710 --> 00:09:31.310
Which brings us to another good point.

197
00:09:31.310 --> 00:09:34.306
What does a model provide, right?

198
00:09:34.306 --> 00:09:36.384
I'm assuming we're talking about chat,
right?

199
00:09:36.384 --> 00:09:38.750
So there is the chat client, right?

200
00:09:38.750 --> 00:09:45.480
The chat client is a easy
way to talk to a chat model.

201
00:09:45.480 --> 00:09:47.240
Okay, here's your chat model.

202
00:09:47.240 --> 00:09:52.120
But there's also transcription, right?

203
00:09:52.120 --> 00:09:52.920
Models.

204
00:09:52.920 --> 00:09:57.000
There's also embedding models, right?

205
00:09:57.000 --> 00:09:58.000
There's all sorts of stuff.

206
00:09:58.000 --> 00:10:00.040
There's all sorts of different
models that you can use.

207
00:10:00.040 --> 00:10:02.520
And so we're going to use the chat model.

208
00:10:02.520 --> 00:10:06.200
And not all models provide all
those different interfaces, right?

209
00:10:06.200 --> 00:10:09.850
Like, embedding is orthogonal to chats.

210
00:10:09.850 --> 00:10:12.250
You don't need to do both to
be an effective model, right?

211
00:10:12.250 --> 00:10:15.490
You can do just chat and you can
delegate embedding to something else.

212
00:10:15.490 --> 00:10:19.187
So one of the things you have to remember
when you use ollama is you might also use

213
00:10:19.187 --> 00:10:21.610
different ollama models for
different purposes.

214
00:10:21.610 --> 00:10:22.890
One for chat, one for embeddings.

215
00:10:22.890 --> 00:10:24.250
We'll talk about what
embeddings are in a bit.

216
00:10:24.250 --> 00:10:27.490
But so
OpenAI just happens to have everything.

217
00:10:27.490 --> 00:10:30.370
It has the ability to do all those things.

218
00:10:30.370 --> 00:10:33.412
When you auto configure an OpenAI starter,
when you use that starter,

219
00:10:33.412 --> 00:10:37.111
it auto configures the embedding model,
the transcription model, the image model,

220
00:10:37.111 --> 00:10:38.730
the chat model, et cetera, okay?

221
00:10:38.730 --> 00:10:39.970
So I'm going to build a chat client.

222
00:10:39.970 --> 00:10:44.337
This chat client builder already has a
pointer to the one already auto configured

223
00:10:44.337 --> 00:10:45.050
chat model.

224
00:10:45.050 --> 00:10:46.330
So I'm gonna use that chat client.

225
00:10:46.330 --> 00:10:48.930
By the way,
this chat client is really good code.

226
00:10:48.930 --> 00:10:51.074
I think the people that wrote
this code are the best,

227
00:10:51.074 --> 00:10:52.450
the best in the business, okay?

228
00:10:52.450 --> 00:10:56.730
Surely they are, and I think you
will benefit from using it as well.

229
00:10:56.730 --> 00:11:00.330
Now, moving on,
we're going to make a request here, okay?

230
00:11:00.330 --> 00:11:05.040
So I'm going to say,
return this AI prompt.

231
00:11:05.040 --> 00:11:07.280
When you make a prompt,
you're asking again.

232
00:11:08.720 --> 00:11:09.920
Just please remember this.

233
00:11:09.920 --> 00:11:14.612
Everything I'm showing you here
is just a fluid type safe DSL

234
00:11:14.612 --> 00:11:17.840
on the almighty string concat operator.

235
00:11:17.840 --> 00:11:21.600
All interoperability, all integration.

236
00:11:21.600 --> 00:11:25.440
That's just sending strings to
an AI model's HTTP endpoint.

237
00:11:26.480 --> 00:11:27.440
This is not special.

238
00:11:27.440 --> 00:11:30.141
Don't, don't get it twisted.

239
00:11:30.141 --> 00:11:31.139
This is a pretty straightforward stuff.

240
00:11:31.139 --> 00:11:32.300
So what are you gonna send?

241
00:11:32.300 --> 00:11:33.904
Well, in the body of that request,

242
00:11:33.904 --> 00:11:36.660
you can send a user prompt
that's just A regular question.

243
00:11:36.660 --> 00:11:37.940
And I happen to have a question.

244
00:11:37.940 --> 00:11:40.500
Great.
I can also send a system prompt.

245
00:11:40.500 --> 00:11:42.260
We're going to do that in a bit,
but not right now.

246
00:11:42.260 --> 00:11:46.831
I'm going to ask for
the content that comes back as a string.

247
00:11:46.831 --> 00:11:48.180
There's the string right there.

248
00:11:48.180 --> 00:11:53.780
Let's go ahead and restart this now.

249
00:11:54.910 --> 00:12:01.310
HTTP:8080 in jlong inquire question==.

250
00:12:01.310 --> 00:12:05.750
And I'm going to do a post
question equals my name.

251
00:12:05.750 --> 00:12:08.975
This is not actually a question,
I guess, but whatever it is, Josh,

252
00:12:08.975 --> 00:12:10.190
let's see if that works.

253
00:12:11.790 --> 00:12:12.310
Okay.

254
00:12:12.310 --> 00:12:13.190
Nice to meet you, Josh.

255
00:12:13.190 --> 00:12:14.190
How can I assist you today?

256
00:12:14.190 --> 00:12:16.590
Faboo, it knows me, right?

257
00:12:16.590 --> 00:12:18.430
We're best buddies now already.

258
00:12:18.430 --> 00:12:19.070
Let's just.

259
00:12:19.070 --> 00:12:21.710
Let's just put that to the test though,
right?

260
00:12:21.710 --> 00:12:24.990
What's my name?

261
00:12:28.312 --> 00:12:31.550
In shambles, it's horrible.

262
00:12:31.550 --> 00:12:32.910
I have that effect on people, though.

263
00:12:32.910 --> 00:12:33.430
It's not.

264
00:12:33.430 --> 00:12:34.310
It's not the first time.

265
00:12:34.310 --> 00:12:35.230
It just still hurts.

266
00:12:35.230 --> 00:12:36.910
It's already forgotten me, okay?

267
00:12:36.910 --> 00:12:39.550
I just talked to it a second ago and
it's already forgotten who I am.

268
00:12:39.550 --> 00:12:41.070
It says, I'm sorry, but
I don't know your name.

269
00:12:41.070 --> 00:12:42.550
If you'd like to share it, feel free.

270
00:12:42.550 --> 00:12:43.270
Curse you.

271
00:12:43.270 --> 00:12:44.270
OpenAI.

272
00:12:44.270 --> 00:12:45.030
It doesn't know.

273
00:12:45.030 --> 00:12:47.670
And this is very different from
the experience that a lot of people

274
00:12:47.670 --> 00:12:49.640
are accustomed to when they go to ChatGPT.

275
00:12:49.640 --> 00:12:50.215
Right?
And

276
00:12:50.215 --> 00:12:54.040
the browser ChatGPT is a chat
client that talks to an.

277
00:12:54.040 --> 00:12:55.120
An API model.

278
00:12:55.120 --> 00:12:58.920
A model that has an API,
that chat client keeps memory.

279
00:12:58.920 --> 00:13:00.280
It has session, right?

280
00:13:00.280 --> 00:13:01.320
It has state.

281
00:13:01.320 --> 00:13:02.080
It's durable.

282
00:13:02.080 --> 00:13:03.600
But the API itself doesn't.

283
00:13:03.600 --> 00:13:05.880
It's like Dory the Goldfish
from Finding Nemo, right?

