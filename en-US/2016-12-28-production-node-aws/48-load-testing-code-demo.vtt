WEBVTT

1
00:00:00.000 --> 00:00:03.918
[MUSIC]

2
00:00:03.918 --> 00:00:07.179
&gt;&gt; Kevin Whinnery: The first bit I'm
gonna show you is this tool, Locust.

3
00:00:07.179 --> 00:00:11.521
Which we're gonna use to beat up on that

4
00:00:11.521 --> 00:00:16.392
Elastic Beanstalk instance that we created

5
00:00:16.392 --> 00:00:21.150
yesterday with our TodoMVC application.

6
00:00:21.150 --> 00:00:25.988
So the first bit that you're gonna
want to note about using Locust is if

7
00:00:25.988 --> 00:00:30.826
you're on a if you're on a Mac or
if you're on an operating system that

8
00:00:30.826 --> 00:00:36.012
limits the number of open files that
any process can have at any given time.

9
00:00:36.012 --> 00:00:40.540
You are going to need to raise
that limit to use to use Locust.

10
00:00:40.540 --> 00:00:43.610
Because it does open a file
on the file system for

11
00:00:43.610 --> 00:00:48.640
every simulated user that it
throws against your system.

12
00:00:48.640 --> 00:00:54.413
On a Mac,
the command to do that is ulimit -n 1000.

13
00:00:54.413 --> 00:00:58.197
And that,
in the context of this terminal session,

14
00:00:58.197 --> 00:01:03.194
will raise the possible open files to you,
in this case, 1000.

15
00:01:03.194 --> 00:01:04.241
You can raise it up a little
bit more from there.

16
00:01:04.241 --> 00:01:09.003
But it has to be more than
however many concurrent users

17
00:01:09.003 --> 00:01:12.323
you want to test in your application.

18
00:01:12.323 --> 00:01:16.118
If you haven't installed Locust,

19
00:01:16.118 --> 00:01:21.360
the easiest way to do it
is to pip install locust.

20
00:01:21.360 --> 00:01:25.760
I think it might be actually locustio,
one of those two.

21
00:01:25.760 --> 00:01:29.260
And it'll be installed using
the pip package manager.

22
00:01:31.380 --> 00:01:35.250
And once you have it installed on your
system, you'll have this command called

23
00:01:35.250 --> 00:01:41.350
locust, to which you will pass a host
that you want to test against.

24
00:01:41.350 --> 00:01:48.800
And it'll run a simulated load against
the server that you pass in this command.

25
00:01:48.800 --> 00:01:53.963
But before you do that, you have to
create something called a locust file.

26
00:01:53.963 --> 00:01:58.715
And I have a very simple one
here that's going to generate

27
00:01:58.715 --> 00:02:01.000
a whole bunch of requests.

28
00:02:01.000 --> 00:02:07.850
And push a whole bunch of fake
data into our todo's database.

29
00:02:07.850 --> 00:02:11.495
So at the core of this is a user behavior,

30
00:02:11.495 --> 00:02:16.240
something, a Python class
called a UserBehavior.

31
00:02:16.240 --> 00:02:24.253
And that class has a set of tasks that
it can execute against your website.

32
00:02:24.253 --> 00:02:29.218
So with each of these Python functions,

33
00:02:29.218 --> 00:02:36.089
they are passed an instance
of the of the Locust object.

34
00:02:36.089 --> 00:02:40.245
And each Locust object has a HTTP client,

35
00:02:40.245 --> 00:02:44.281
which you can use to
send a POST request or

36
00:02:44.281 --> 00:02:48.860
a GET request to the host your choosing.

37
00:02:48.860 --> 00:02:54.230
And you can also pass in parameters, like
in this case, I'm generating some fake

38
00:02:54.230 --> 00:02:59.220
text to put into the TODO items.

39
00:02:59.220 --> 00:03:04.640
It's another package called faker
which generates some fake data,

40
00:03:04.640 --> 00:03:09.230
some lorem ipsum text it'll
insert into a TODO list item.

41
00:03:09.230 --> 00:03:12.959
Then will set the completed for
each of these items to false.

42
00:03:12.959 --> 00:03:16.980
And then the other action we
have is a read against our API.

43
00:03:16.980 --> 00:03:24.070
Which will, by default, grab the last
1,000 TODO items from the database.

44
00:03:24.070 --> 00:03:26.911
In our behavior, we define that for

45
00:03:26.911 --> 00:03:31.373
every 1 create operation,
we wanna execute 5 reads.

46
00:03:31.373 --> 00:03:37.301
And you can, in your Python code,
kind of tweak this to be as close to

47
00:03:37.301 --> 00:03:43.245
an actual usage scenario for
a user of your site as you possibly can.

48
00:03:43.245 --> 00:03:44.653
&gt;&gt; Speaker 2: Question?
&gt;&gt; Kevin Whinnery: Yeah?

49
00:03:44.653 --> 00:03:48.467
&gt;&gt; Speaker 2: Do they,
&gt;&gt; Speaker 2: Have any safeguards in

50
00:03:48.467 --> 00:03:51.560
the tool to prevent you from
doing nefarious things with this?

51
00:03:51.560 --> 00:03:53.885
It seems like you could-
&gt;&gt; Kevin Whinnery: Like denial of service?

52
00:03:53.885 --> 00:03:54.669
[LAUGH]
&gt;&gt; Speaker 2: Right.

53
00:03:54.669 --> 00:03:57.576
[LAUGH]
&gt;&gt; Kevin Whinnery: Not in the tool, no.

54
00:03:57.576 --> 00:04:04.274
I mean, I think most applications
probably will blacklist an IP eventually,

55
00:04:04.274 --> 00:04:08.750
like if it seems to be
involved in some evil-doing.

56
00:04:08.750 --> 00:04:14.316
But generally, yeah,
I guess it's not a Locust problem.

57
00:04:14.316 --> 00:04:17.136
It's more of our problem
as application developers.

58
00:04:17.136 --> 00:04:21.900
But that's its job,
its job is to simulate high

59
00:04:21.900 --> 00:04:26.080
amounts of load against an application.

60
00:04:27.930 --> 00:04:32.138
So,
&gt;&gt; Kevin Whinnery: So here we have,

61
00:04:32.138 --> 00:04:35.107
in the WebsiteUser class,

62
00:04:35.107 --> 00:04:40.279
we set our task to be some
kind of UserBehavior.

63
00:04:40.279 --> 00:04:45.480
And then we specify a min and
a max wait time.

64
00:04:45.480 --> 00:04:49.836
Which will, in milliseconds,
determine a lower bound and

65
00:04:49.836 --> 00:04:55.596
an upper bound between requests from
this simulated user in the application.

66
00:04:55.596 --> 00:04:57.921
And there's a ton more that you can do.

67
00:04:57.921 --> 00:04:59.691
You can send login requests.

68
00:04:59.691 --> 00:05:04.241
You can,
&gt;&gt; Kevin Whinnery: Excuse me,

69
00:05:04.241 --> 00:05:06.830
[INAUDIBLE] shut these down here.

70
00:05:06.830 --> 00:05:10.870
You can send login requests so you can
get authentication cookies back and

71
00:05:10.870 --> 00:05:13.420
execute authenticated requests.

72
00:05:13.420 --> 00:05:18.780
There's lots of different ways you
can configure this simulated session.

73
00:05:19.830 --> 00:05:24.957
So once we have our Locust file generated,
we're going to use

74
00:05:24.957 --> 00:05:30.578
the locust command to target this
this todomvc-plusplus instance

75
00:05:30.578 --> 00:05:36.230
running on elasticbeanstalk
in the us-west-2 region.

76
00:05:36.230 --> 00:05:41.240
So when we start that up, that's going
to start a local web server that

77
00:05:41.240 --> 00:05:46.110
can open up here in my browser,
&gt;&gt; Kevin Whinnery: On

78
00:05:46.110 --> 00:05:49.879
port 8089,
&gt;&gt; Kevin Whinnery: And

79
00:05:49.879 --> 00:05:51.590
it's gonna ask us for two things.

80
00:05:51.590 --> 00:05:54.980
It's gonna be the number
of users to simulate.

81
00:05:54.980 --> 00:05:58.600
And the hatch rate,
which is how often users

82
00:05:58.600 --> 00:06:04.010
are added to the list of users that
are gonna be spamming our application.

83
00:06:04.010 --> 00:06:07.870
So for starters, maybe we'll start
with something relatively small.

84
00:06:07.870 --> 00:06:12.740
For 100 users hatching
at a rate of 10 /second.

85
00:06:12.740 --> 00:06:19.884
We're going to launch that load at
our website and see how we're doing.

86
00:06:19.884 --> 00:06:25.882
So as we are executing the test, and
we can execute it for as long as we like,

87
00:06:25.882 --> 00:06:31.613
we see the number of requests going
into our GET and our POST endpoints.

88
00:06:31.613 --> 00:06:33.924
It starts to mount slowly over time.

89
00:06:33.924 --> 00:06:38.873
And with 100 users on prior tests,
I think it tops out,

90
00:06:38.873 --> 00:06:45.050
with these settings,
it'll be around 20, 25 requests/second.

91
00:06:45.050 --> 00:06:49.053
And we can see the min and
the max time for

92
00:06:49.053 --> 00:06:53.919
a response, and
that time is in milliseconds.

93
00:06:53.919 --> 00:06:58.594
And then we can look at the average and
the median response times from the server,

94
00:06:58.594 --> 00:06:59.880
as well.

95
00:06:59.880 --> 00:07:02.098
So for a load like this one,

96
00:07:02.098 --> 00:07:07.703
our server is usually able to respond
in a reasonable amount of time.

97
00:07:07.703 --> 00:07:11.647
The POSTed TODOS, where you create
a new one, is quite a bit faster,

98
00:07:11.647 --> 00:07:14.843
returning in an average of
about 127 milliseconds.

99
00:07:14.843 --> 00:07:18.143
But that read because I have
been spamming this quite a bit,

100
00:07:18.143 --> 00:07:20.719
there are quite a few
records in the database.

101
00:07:20.719 --> 00:07:27.000
It's a little bit slower,
at about 400 milliseconds on average.

102
00:07:27.000 --> 00:07:32.422
And it's creeping up a little
bit already as we're starting

103
00:07:32.422 --> 00:07:37.642
to tax our Elastic Beanstalk
instances a little bit more.

104
00:07:37.642 --> 00:07:42.172
But our instances do stay up.

105
00:07:42.172 --> 00:07:50.020
We haven't seen any failures or
500s yet, so that is a good thing.

106
00:07:50.020 --> 00:07:52.590
Now to see what's kind of happening
on the other end of this.

107
00:07:52.590 --> 00:07:57.420
Let's go into the Elastic Beanstalk
administrative interface.

108
00:07:57.420 --> 00:08:03.020
If we look at the Health section,
here, we can take a look

109
00:08:03.020 --> 00:08:07.590
at what's actually going on with the EC2
instances that are serving this traffic.

110
00:08:07.590 --> 00:08:10.470
So right now,
with our scaling rules as they are,

111
00:08:10.470 --> 00:08:13.870
we only have a single instance which
is serving all of this traffic.

112
00:08:14.870 --> 00:08:22.514
And its CPU utilization is
hovering at right around 50%.

113
00:08:22.514 --> 00:08:27.402
So at the moment,
this single instance is not having too

114
00:08:27.402 --> 00:08:30.840
much trouble keeping up with the load.

115
00:08:30.840 --> 00:08:35.350
Although, our average response
times do keep creeping up.

116
00:08:35.350 --> 00:08:40.118
So that's probably something
that we could investigate.

117
00:08:40.118 --> 00:08:42.251
And if we turn up the heat a little bit,

118
00:08:42.251 --> 00:08:46.300
then we can start to see where our
application is going to break down.

119
00:08:46.300 --> 00:08:50.258
So I'll stop this test,
&gt;&gt; Kevin Whinnery: And

120
00:08:50.258 --> 00:08:51.950
I'll create a new one.

121
00:08:51.950 --> 00:08:57.081
And this time,
we'll simulate 900 concurrent users,

122
00:08:57.081 --> 00:09:01.300
maybe hatching at a rate
of about 25 /second.

123
00:09:02.820 --> 00:09:07.243
Which is going to really
start taxing our poor little

124
00:09:07.243 --> 00:09:12.580
t2.micro instance running on
Elastic Beanstalk right now.

125
00:09:13.640 --> 00:09:18.474
So we can already see the average
response times going way

126
00:09:18.474 --> 00:09:23.308
up as our single instance
struggles to keep up with what is

127
00:09:23.308 --> 00:09:27.758
a pretty heavy load for
its current configuration.

128
00:09:27.758 --> 00:09:33.640
And if we go back over here to our
Health check, we can see, yeah.

129
00:09:33.640 --> 00:09:36.230
It's already starting to
blow up a little bit.

130
00:09:36.230 --> 00:09:39.878
Our CPU usage has crept up to about 92%.

131
00:09:39.878 --> 00:09:44.970
So it's starting to be
taxed a little more.

132
00:09:44.970 --> 00:09:48.245
But what we should see, in a minute or so,

133
00:09:48.245 --> 00:09:52.278
is Elastic Beanstalk
reacting to this situation.

134
00:09:52.278 --> 00:09:56.995
And our auto scaling group is
actually gonna provision another

135
00:09:56.995 --> 00:10:01.540
instance of our application to
start serving this traffic.

136
00:10:01.540 --> 00:10:05.846
It does take a minute, but
we can configure those preferences.

137
00:10:05.846 --> 00:10:09.130
I've actually been messing
around with this a little bit.

138
00:10:09.130 --> 00:10:14.204
And this is something that you
can do as your load testing your

139
00:10:14.204 --> 00:10:18.982
application to see like what
kind of scaling criteria you

140
00:10:18.982 --> 00:10:23.779
can bring to bear to ensure
a consistent response times.

141
00:10:23.779 --> 00:10:26.542
So if I look at the scaling configuration,

142
00:10:26.542 --> 00:10:31.259
I can take a look at my auto scaling,
&gt;&gt; Kevin Whinnery: My auto

143
00:10:31.259 --> 00:10:31.900
scaling options.

144
00:10:31.900 --> 00:10:34.640
I fiddled with these a little
bit from the default.

145
00:10:34.640 --> 00:10:40.120
The default was having a minimum of 1
instance available and a maximum of 4.

146
00:10:40.120 --> 00:10:47.307
I tuned up the maximum instances to 6, and
I also tuned down the scaling cooldown.

147
00:10:47.307 --> 00:10:50.060
60 seconds is gonna be far too short.

148
00:10:50.060 --> 00:10:53.610
But I wanted to make sure we saw some
more instances getting provisioned

149
00:10:53.610 --> 00:10:55.550
during the test here.

150
00:10:55.550 --> 00:10:57.580
So this is basically the delay time.

151
00:10:57.580 --> 00:11:02.492
So after some condition is
reached where it's decided that

152
00:11:02.492 --> 00:11:06.126
another instance needs to be provisioned,

153
00:11:06.126 --> 00:11:10.351
this is the cooldown in
seconds that Amazon will for

154
00:11:10.351 --> 00:11:16.559
sure wait until it provisions another
instance to start serving traffic.

155
00:11:16.559 --> 00:11:21.633
In the Scaling Triggers,
these you'll want to tune based

156
00:11:21.633 --> 00:11:26.740
on what you're able to discover
about your application.

157
00:11:26.740 --> 00:11:31.704
The default settings look at
the amount of data that you're pushing

158
00:11:31.704 --> 00:11:33.370
out over the network.

159
00:11:33.370 --> 00:11:37.570
But there's other bits here, as well.

160
00:11:37.570 --> 00:11:41.162
The one that we might take
a look at using, instead, for

161
00:11:41.162 --> 00:11:44.076
this application is maybe CPU utilization.

162
00:11:44.076 --> 00:11:50.010
Where we could take a look at the upper
threshold which would be maybe like 90.

163
00:11:50.010 --> 00:11:54.089
We can change the unit of
measurement to percent.

164
00:11:54.089 --> 00:11:58.289
So if the CPU threshold reaches 90 or 80%,

165
00:11:58.289 --> 00:12:05.118
that's when we'd actually start looking
at spinning up another instance.

166
00:12:05.118 --> 00:12:08.394
And then we can also set
the lower threshold for

167
00:12:08.394 --> 00:12:11.420
when it's time to rotate an instance out.

168
00:12:13.340 --> 00:12:16.770
So I'll cancel that for now,
head back to the dashboard.

169
00:12:16.770 --> 00:12:21.245
And what we should see,
&gt;&gt; Kevin Whinnery: Our environment is

170
00:12:21.245 --> 00:12:25.126
okay, but
as more instances are being available,

171
00:12:25.126 --> 00:12:31.650
Locust is going to continue to suck up
the ability to pepper them with requests.

172
00:12:31.650 --> 00:12:36.408
And we can see their average
response time is now up to

173
00:12:36.408 --> 00:12:41.399
about 17 seconds on average,
which is not awesome.

174
00:12:41.399 --> 00:12:44.119
But to help spread the load a little bit,

175
00:12:44.119 --> 00:12:49.643
we do have another instance that was just
brought brought on line two minutes ago.

176
00:12:49.643 --> 00:12:55.050
And it looks like it hasn't started
serving very much traffic, yet.

177
00:12:55.050 --> 00:13:01.430
Must have been provisioned recently, but
now it is up to 92% utilization, as well.

178
00:13:01.430 --> 00:13:03.690
So we're gonna probably,

179
00:13:03.690 --> 00:13:08.840
at this level of load, max out each of
these tiny little servers pretty quick.

180
00:13:08.840 --> 00:13:11.806
But the good news is
we're not falling over.

181
00:13:11.806 --> 00:13:14.080
We have, well,
it looks like we have had some failures.

182
00:13:16.070 --> 00:13:18.230
Quite at quite a few,
actually, at this level.

183
00:13:18.230 --> 00:13:21.600
At the lower levels I was testing,
I wasn't actually seeing any failures.

184
00:13:21.600 --> 00:13:23.577
But that's what you want
to start to check and

185
00:13:23.577 --> 00:13:27.530
see like at what level your application
is gonna start to break down.

186
00:13:27.530 --> 00:13:32.069
And what we can look at,
when we have the failures over here,

187
00:13:32.069 --> 00:13:38.306
is the error that we're getting from the
HTTP client which is running the request.

188
00:13:38.306 --> 00:13:45.919
Here we can see, we have a 504 error
against our Elastic Beanstalk instance.

189
00:13:47.180 --> 00:13:50.223
Which is probably related
to just not having the,

190
00:13:50.223 --> 00:13:54.788
time-outs are happening because we've
just lack the capacity to service

191
00:13:54.788 --> 00:13:58.639
the number of requests that
are coming up against the service.

192
00:14:00.906 --> 00:14:03.238
&gt;&gt; Kevin Whinnery: So
based on our criteria,

193
00:14:03.238 --> 00:14:08.870
we'll probably continue to
provision instances up until 6.

194
00:14:08.870 --> 00:14:13.884
But the primary thing I wanted
to show here was the ability to

195
00:14:13.884 --> 00:14:18.698
use this tool to at least
understand how much traffic your

196
00:14:18.698 --> 00:14:24.031
website can take before it is
completely brought to its knees.

197
00:14:27.241 --> 00:14:29.710
&gt;&gt; Kevin Whinnery: There are a few random
statistics out there that you can find.

198
00:14:29.710 --> 00:14:35.570
Like OpenStreetMap serves about
10 to 20 requests/second.

199
00:14:35.570 --> 00:14:39.085
There are a lot of sites which serve
actually quite a bit more across many

200
00:14:39.085 --> 00:14:40.269
thousands of servers.

201
00:14:40.269 --> 00:14:45.225
But based on your site's traffic,
you can ensure that,

202
00:14:45.225 --> 00:14:48.735
at certain levels of concurrent users,

203
00:14:48.735 --> 00:14:52.878
you're gonna remain
available at that level.

204
00:14:54.881 --> 00:14:56.810
&gt;&gt; Kevin Whinnery: So that is Locust.

205
00:14:56.810 --> 00:15:02.703
I will let my poor Elastic Beanstalk
instance relax a little bit.

206
00:15:02.703 --> 00:15:08.424
But after a test like this, we see that
with the size of the instances we have,

207
00:15:08.424 --> 00:15:14.247
we definitely become CPU-bound really
fast at those high levels of traffic.

208
00:15:17.650 --> 00:15:19.111
&gt;&gt; Kevin Whinnery: So that is Locust.

209
00:15:19.111 --> 00:15:23.987
It's, again, my favorite tool that I found
for doing this kind of load testing stuff.

210
00:15:23.987 --> 00:15:27.220
Definitely recommend
that you check it out.

