WEBVTT

1
00:00:00.000 --> 00:00:03.969
This trick will not give
you the exact solution.

2
00:00:03.969 --> 00:00:10.684
So it's not gonna get you to
the analytical solution of those minimums.

3
00:00:10.684 --> 00:00:16.987
But its iterative method which gonna get
you pretty close to the optimal solution.

4
00:00:16.987 --> 00:00:19.490
And that's what we'll try
to implement right now.

5
00:00:19.490 --> 00:00:23.280
So with learning rates,

6
00:00:23.280 --> 00:00:26.140
we need to specify how many steps
we'll want to do, all right?

7
00:00:26.140 --> 00:00:30.820
So steps, let's say 200 steps for now.

8
00:00:30.820 --> 00:00:35.490
And remember,
I was getting to the minimum, and

9
00:00:35.490 --> 00:00:40.140
that's how many kinda steps I want
to do to get to the solution.

10
00:00:40.140 --> 00:00:45.568
And now,
I just need to implement for loop.

11
00:00:46.994 --> 00:00:53.285
Actually, let's use plural steps for
step in range steps.

12
00:00:57.084 --> 00:00:59.312
In our steps, what I will do,

13
00:00:59.312 --> 00:01:03.869
I will simply calculate
the current loss function, right?

14
00:01:03.869 --> 00:01:10.380
And then, we'll decide where I need to go.

15
00:01:10.380 --> 00:01:14.830
Yes, and I need derivatives, basically,
to figure out what is the direction.

16
00:01:14.830 --> 00:01:15.660
Should I go up?

17
00:01:15.660 --> 00:01:17.100
Should I increase my b?

18
00:01:17.100 --> 00:01:18.841
Should I decrease it, right?

19
00:01:18.841 --> 00:01:25.473
And the trick of TensorFlow is that all of
those derivatives can be calculated for

20
00:01:25.473 --> 00:01:30.365
you automatically using
something called GradientTape.

21
00:01:31.450 --> 00:01:34.654
So let's actually implement it here.

22
00:01:34.654 --> 00:01:39.844
So with TensorFlow GradientTape As tape,

23
00:01:39.844 --> 00:01:46.569
for instance, what we can do,
I executed the code accidentally.

24
00:01:46.569 --> 00:01:52.427
So it basically creates this
environment where TensorFlow

25
00:01:52.427 --> 00:01:57.464
now is keeping track of
arithmetical operations,

26
00:01:57.464 --> 00:02:01.840
which happening in this block of code.

27
00:02:01.840 --> 00:02:06.095
So what we can do,
we can then type something like this.

28
00:02:06.095 --> 00:02:11.831
We have TensorFlow GradientTape,
just do the predictions.

29
00:02:13.579 --> 00:02:19.048
And predictions, yeah,
we'll be using our predict function on X.

30
00:02:22.117 --> 00:02:24.080
And calculate the loss function.

31
00:02:24.080 --> 00:02:29.940
So loss function,
it's our mean squared error,

32
00:02:29.940 --> 00:02:37.039
and we comparing our
predictions with true Y values.

33
00:02:37.039 --> 00:02:44.770
All right, so
that's what we will be doing in our steps.

34
00:02:44.770 --> 00:02:51.330
We will just simply look at
the calculating the predictions,

35
00:02:51.330 --> 00:02:55.520
right, for our original data points axis.

36
00:02:55.520 --> 00:02:57.580
And then, calculating the loss function.

37
00:02:57.580 --> 00:03:00.220
But what is interesting
next is that we can

38
00:03:00.220 --> 00:03:04.700
automatically calculate
the derivatives from this tape.

39
00:03:04.700 --> 00:03:08.873
And to do this, if we just type, for

40
00:03:08.873 --> 00:03:13.895
instance, gradients, tape gradients.

41
00:03:13.895 --> 00:03:20.749
And we need to specify function,
loss function.

42
00:03:20.749 --> 00:03:27.820
And then,
list of variables which we plan to modify.

43
00:03:27.820 --> 00:03:32.324
So in our case it will be w and b.

44
00:03:32.324 --> 00:03:34.097
So w and b.

45
00:03:34.097 --> 00:03:40.838
So we need to first tell the TensorFlow

46
00:03:40.838 --> 00:03:45.570
that we have those w and b.

47
00:03:45.570 --> 00:03:48.632
So we can say w equal
to TensorFlow constant.

48
00:03:52.050 --> 00:03:56.785
And initialize it with something,
for instance, 0.0, right?

49
00:03:56.785 --> 00:04:01.456
And then, b will be also our constant
initialized with, I don't know,

50
00:04:01.456 --> 00:04:02.463
minus 1, 0.

51
00:04:04.770 --> 00:04:09.140
Doesn't really matter, because we will
be modifying those values, right?

52
00:04:09.140 --> 00:04:14.226
So now, TensorFlow knows that
we have those w's and b's,

53
00:04:14.226 --> 00:04:19.640
and in our GradientTape,
it will keep track of those.

54
00:04:19.640 --> 00:04:26.480
And also,
what I need to do in my function, predict.

55
00:04:26.480 --> 00:04:32.200
Yeah, I need to or specify w,
for instance, and b.

56
00:04:33.669 --> 00:04:39.154
So I'm switching from just
renaming the variables.

57
00:04:39.154 --> 00:04:43.704
Now, we have those w's and
b's initialized with those can

58
00:04:43.704 --> 00:04:48.442
kinda random values, and
we will keep track of this command.

59
00:04:48.442 --> 00:04:51.986
So we're doing the prediction for
our points x and

60
00:04:54.522 --> 00:05:01.570
We can even specify w and
b here, all right?

61
00:05:01.570 --> 00:05:04.620
And then, calculating the loss function.

62
00:05:04.620 --> 00:05:11.386
And we want to return gradients, so
kinda those partial derivatives,

63
00:05:11.386 --> 00:05:17.039
values, of the loss function
to those variables, w and b.

64
00:05:17.039 --> 00:05:21.216
And now, since we know the gradient,
basically,

65
00:05:21.216 --> 00:05:26.170
you can think about it like
the direction where we should go to

66
00:05:26.170 --> 00:05:30.080
modify our w's and
b's to get to this minimum.

67
00:05:30.080 --> 00:05:35.088
And we can simply modify w's by running

68
00:05:35.088 --> 00:05:41.319
also another TensorFlow
command assign_sub.

69
00:05:41.319 --> 00:05:45.654
Assign_sub will just take
current value of w and

70
00:05:45.654 --> 00:05:50.210
subtract the argument
provided to this function.

71
00:05:50.210 --> 00:05:58.090
And the argument in this
case will be gradients 0.

72
00:05:58.090 --> 00:06:04.280
So gradient will be the array of two
values, gradient for w and gradient for b.

73
00:06:04.280 --> 00:06:11.180
So I'm just simply referring to the first
index of this array, gradients.

74
00:06:11.180 --> 00:06:15.840
And for b, I need to do the same thing.

75
00:06:17.020 --> 00:06:22.348
But just use the next
element of gradients.

76
00:06:22.348 --> 00:06:26.687
So remember, I said that we also
need learning rates, right?

77
00:06:26.687 --> 00:06:28.560
Not to overshoot.

78
00:06:28.560 --> 00:06:34.057
So I simply can multiply our gradients

79
00:06:34.057 --> 00:06:39.060
by learning rate right here.

80
00:06:39.060 --> 00:06:43.627
And that should help us to simply,
not minimize, but

81
00:06:43.627 --> 00:06:50.548
just take only part of those calculated
gradients, and slowly get to the point.

82
00:06:50.548 --> 00:06:53.015
So there's a trick here.

83
00:06:53.015 --> 00:06:56.192
If you take a learning rate too small,

84
00:06:56.192 --> 00:07:00.870
you will be barely changing your w's and
b's, right?

85
00:07:00.870 --> 00:07:06.120
And instead of 200 steps,
you might gonna need two millions steps.

86
00:07:06.120 --> 00:07:09.710
At the same time,
if you're learning rate is too large,

87
00:07:09.710 --> 00:07:12.540
you might actually jump out
of your minimum completely.

88
00:07:12.540 --> 00:07:16.040
So learning rate in this case,

89
00:07:16.040 --> 00:07:20.780
and determinant I'm trying to introduce,
is hyperparameter.

90
00:07:20.780 --> 00:07:23.300
So in this case,
learning rate is hyperparameter.

91
00:07:23.300 --> 00:07:27.420
It somehow controls how
the training is happening.

92
00:07:27.420 --> 00:07:30.187
And in machine learning and deep learning,

93
00:07:30.187 --> 00:07:34.760
there are usually a lot of those
learning rate hyperparameters.

94
00:07:34.760 --> 00:07:40.220
So it basically will control
how the training is happening.

95
00:07:41.232 --> 00:07:46.560
All right, so with all of that,
I think we're ready to run our command.

96
00:07:46.560 --> 00:07:51.090
The only thing I want to probably
just print out our current status.

97
00:07:51.090 --> 00:07:56.416
So let's say if step divided by,

98
00:07:56.416 --> 00:08:01.310
let's say, 20 equals 2.

99
00:08:01.310 --> 00:08:08.598
I mean, if the reminder of
dividing step by 20 equals to 0,

100
00:08:08.598 --> 00:08:13.795
then I will do print, I don't know, step.

101
00:08:20.112 --> 00:08:22.663
Yeah, format.

102
00:08:22.663 --> 00:08:27.420
And we can do step right here.

103
00:08:27.420 --> 00:08:30.431
All right, let's try to execute.

104
00:08:32.279 --> 00:08:35.743
Yes, it's not an assignment,
it's comparison.

105
00:08:37.982 --> 00:08:42.920
All right,
gradient object has no attribute gradient.

106
00:08:45.330 --> 00:08:50.229
Because it is gradient, not gradients.

107
00:08:54.839 --> 00:08:59.369
Assign variable.

108
00:08:59.369 --> 00:09:00.370
Okay okay.

109
00:09:00.370 --> 00:09:01.746
Another mistake.

110
00:09:01.746 --> 00:09:08.710
Constant will not be modifiable, so
I want to use actually variable instead.

111
00:09:08.710 --> 00:09:13.220
All right, so steps up to 200, perfect.

112
00:09:13.220 --> 00:09:15.215
So we actually did something, right?

113
00:09:15.215 --> 00:09:20.230
So in our code,
we were just modifying those w's and b's.

114
00:09:20.230 --> 00:09:22.860
How well we did, I have no idea.

115
00:09:22.860 --> 00:09:23.720
Let's check.

116
00:09:23.720 --> 00:09:28.079
So we actually have those w as

117
00:09:28.079 --> 00:09:32.809
close to 1.4, let's say.

118
00:09:32.809 --> 00:09:35.718
And what about b?

119
00:09:35.718 --> 00:09:37.740
b is 0.47.

120
00:09:37.740 --> 00:09:45.756
Remember, I specified my original w as
something close to 0.1, b equal to 0.5.

121
00:09:45.756 --> 00:09:47.520
So we kinda there.

122
00:09:48.750 --> 00:09:52.444
Probably to even visualize it,
we can use those graphs.

123
00:09:56.975 --> 00:10:02.090
But I will need to, yeah, do something.

124
00:10:02.090 --> 00:10:03.959
Let's call them tru.

125
00:10:07.013 --> 00:10:13.260
And specify the true values, the one
we used to initiate our data, right?

126
00:10:13.260 --> 00:10:18.665
And I

127
00:10:18.665 --> 00:10:24.150
should change it here as well.

128
00:10:27.780 --> 00:10:32.450
And that's gonna be our blue line,

129
00:10:32.450 --> 00:10:35.725
or actually green line.

130
00:10:35.725 --> 00:10:39.156
And that's our guess line.

131
00:10:39.156 --> 00:10:45.365
Just red color, right?

132
00:10:45.365 --> 00:10:49.702
So if we plot this right now, you can see

133
00:10:49.702 --> 00:10:54.970
previously it was basically down low,
right?

134
00:10:54.970 --> 00:11:01.558
Because our, well, actually we started
with the w equal to 0, b equal to minus 1.

135
00:11:01.558 --> 00:11:07.379
But minus 1, it means that it was actually
sitting below the 0 line at minus 1.

136
00:11:07.379 --> 00:11:10.596
And it's basically shifted up, and

137
00:11:10.596 --> 00:11:15.490
also tilted, to get closer to
the true values of b and w.

138
00:11:15.490 --> 00:11:19.766
And the thing is, we can continue
this training process even more.

139
00:11:19.766 --> 00:11:25.797
So for instance, if I, yeah, I should
probably comment out those w's and

140
00:11:25.797 --> 00:11:30.412
b's just to use previously
already calculated values.

141
00:11:30.412 --> 00:11:37.610
But now, if I just rerun this block again,
and you can see, it's running pretty fast.

142
00:11:37.610 --> 00:11:40.070
Previously, w was 139.

143
00:11:40.070 --> 00:11:44.920
If I re-execute it,
it's now getting even closer, right?

144
00:11:44.920 --> 00:11:47.843
And the same for b, it's getting closer.

145
00:11:47.843 --> 00:11:52.385
And that was the plot
after first 200 steps.

146
00:11:52.385 --> 00:11:54.826
Now, it's really close.

147
00:11:54.826 --> 00:11:58.240
But maybe we at this point in
the situation where we kinda

148
00:11:58.240 --> 00:12:00.577
jumping around the minimum, right?

149
00:12:00.577 --> 00:12:03.023
And not getting to the exact.

150
00:12:03.023 --> 00:12:07.313
So there's still a little bit of
a difference between red line and

151
00:12:07.313 --> 00:12:08.695
green line.

152
00:12:08.695 --> 00:12:12.955
If I change my learning
rate to something smaller,

153
00:12:12.955 --> 00:12:15.925
that will allows me to get even closer.

154
00:12:15.925 --> 00:12:19.945
But basically, this example of linear
regression, the reason why I introduced

155
00:12:19.945 --> 00:12:27.535
it, I wanted to introduce a couple
of concepts, like lost function.

156
00:12:27.535 --> 00:12:31.340
Loss function is simply
the measurement of the error.

157
00:12:31.340 --> 00:12:36.710
And back to our example with
neural network, it should be

158
00:12:36.710 --> 00:12:41.460
the measurement of if you did the
prediction correctly or not, for instance.

159
00:12:41.460 --> 00:12:46.840
If you're looking at the hot dog or
saying it's not hot dog,

160
00:12:46.840 --> 00:12:51.180
our loss function should return
pretty big value, right?

161
00:12:51.180 --> 00:12:55.030
We are very, very mistaken.

162
00:12:55.030 --> 00:12:59.730
And then,
you can just calculate derivatives

163
00:12:59.730 --> 00:13:04.590
of this loss function to all the weights
we have in our neural network.

164
00:13:04.590 --> 00:13:06.750
And we will know how those weights and

165
00:13:06.750 --> 00:13:12.295
biases should be modified to
actually get you the correct answer.

166
00:13:12.295 --> 00:13:17.115
So it's kind of the same algorithm,
but on significantly larger scale, and

167
00:13:17.115 --> 00:13:21.728
when you're operating with thousands
of different weights and biases.

168
00:13:21.728 --> 00:13:24.685
But the common principle
is exactly the same.

169
00:13:24.685 --> 00:13:30.590
You're just providing the input data,
what you expecting right?

170
00:13:30.590 --> 00:13:35.394
And you keeping track of what is
happening and calculating how weights and

171
00:13:35.394 --> 00:13:39.657
biases should be tweaked to actually
get to the correct results,

172
00:13:39.657 --> 00:13:42.924
something similar will be
provided as the input.

173
00:13:44.171 --> 00:13:46.515
Yeah, as simple as that.

174
00:13:46.515 --> 00:13:49.620
[LAUGH] maybe not so simple.

175
00:13:49.620 --> 00:13:53.155
Okay, next, we gonna be playing
actually with pictures, and

176
00:13:53.155 --> 00:13:56.300
that probably was the heaviest math part,
once again.

177
00:13:56.300 --> 00:14:01.790
So now, it's gonna be all
about just simply writing code

178
00:14:01.790 --> 00:14:05.130
which will take the image as the input and
print something funny at the end.

