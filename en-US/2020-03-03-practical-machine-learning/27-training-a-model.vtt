WEBVTT

1
00:00:00.120 --> 00:00:05.160
Basically our solver is defined,
our model is defined previously.

2
00:00:05.160 --> 00:00:08.710
Now, we actually need to
do the training itself.

3
00:00:08.710 --> 00:00:13.670
And the way how you do it is just saying,
model.fit,

4
00:00:13.670 --> 00:00:19.830
and then just basically providing
what input data you have.

5
00:00:19.830 --> 00:00:24.388
So in our case, it will be just x_train.

6
00:00:27.533 --> 00:00:30.390
You also need to provide all the labels,
right?

7
00:00:30.390 --> 00:00:38.470
So our y_train is the array of all of
those going zero, ones that up to nines.

8
00:00:38.470 --> 00:00:40.629
And what else do we need?

9
00:00:42.120 --> 00:00:45.670
You need to specify the amount of epochs.

10
00:00:47.060 --> 00:00:48.830
So what is epoch?

11
00:00:50.000 --> 00:00:57.368
Epoch is the event where you show to
your model all the input dataset once.

12
00:00:57.368 --> 00:01:02.162
So when we say, one epoch happened,
it means that we went through all

13
00:01:02.162 --> 00:01:08.110
the images of my, in this case,
60,000 handwritten digits, right?

14
00:01:08.110 --> 00:01:11.985
But what we can do,
we can just show them once.

15
00:01:11.985 --> 00:01:18.920
Then remember, with learning rate, we just
gonna slightly getting to the result.

16
00:01:18.920 --> 00:01:22.470
So, I can simply repeat
this process several times.

17
00:01:22.470 --> 00:01:26.428
For instance,
I can say epoch should be equal to 20.

18
00:01:26.428 --> 00:01:34.037
And what will happened also, when you will
be going through those training datasets,

19
00:01:34.037 --> 00:01:39.590
your data can be also reshuffled
to prevent any biases.

20
00:01:39.590 --> 00:01:42.690
So, okay, let's see model.fit.

21
00:01:42.690 --> 00:01:48.926
We've got our images, we got our labels,
we gonna do training for 20 epochs.

22
00:01:48.926 --> 00:01:51.060
Anything else we need to specify?

23
00:01:51.060 --> 00:01:54.090
Well, for now,
let's just keep it the way it is.

24
00:01:54.090 --> 00:01:57.720
Let's execute and
see how the training is happening.

25
00:01:57.720 --> 00:02:00.237
So, our first epoch, finished.

26
00:02:00.237 --> 00:02:05.690
So we went through all the 60,000 images,
right, and it took only four seconds.

27
00:02:05.690 --> 00:02:10.695
And our loss function which is
sparse categorical cross entropy is

28
00:02:10.695 --> 00:02:14.745
at the end of this epoch was 0.2.

29
00:02:14.745 --> 00:02:18.975
And as we can see,
it's only decreasing with each epoch.

30
00:02:18.975 --> 00:02:22.135
But I also like to look at the accuracies.

31
00:02:22.135 --> 00:02:26.165
So even after the first epoch,
we get to 93% accuracy,

32
00:02:26.165 --> 00:02:30.555
97 on the second epoch, so
that's pretty cool, right?

33
00:02:30.555 --> 00:02:36.850
So with a 99% accuracy,
at least on the training dataset,

34
00:02:36.850 --> 00:02:41.350
we recognizing those numbers
from the training dataset.

35
00:02:41.350 --> 00:02:47.305
But it's not exactly fair
when you training your model

36
00:02:47.305 --> 00:02:51.585
on training datasets, and then asking,
what's the accuracy of the prediction?

37
00:02:51.585 --> 00:02:53.475
Of course, it will get to 100%, right?

38
00:02:53.475 --> 00:02:57.295
Because your model is trained
to recognize those particular

39
00:02:57.295 --> 00:03:00.015
images you using as you train dataset.

40
00:03:00.015 --> 00:03:00.985
So let's be fair.

41
00:03:02.145 --> 00:03:04.465
We have test dataset.

42
00:03:04.465 --> 00:03:07.985
That's the images which our
model haven't seen yet.

43
00:03:07.985 --> 00:03:13.673
And we can actually use those to
affiliates the true accuracy of our model,

44
00:03:13.673 --> 00:03:14.220
right?

45
00:03:14.220 --> 00:03:19.600
By the way, we can probably reduce
the amount of epochs to ten or

46
00:03:19.600 --> 00:03:20.570
something like that, right?

47
00:03:20.570 --> 00:03:23.240
Because you can see that our model been

48
00:03:25.000 --> 00:03:29.020
around 99% accuracy for
a pretty long time now.

49
00:03:29.020 --> 00:03:33.132
So what we can do, we will simply, yeah,

50
00:03:33.132 --> 00:03:39.958
the easiest way to restart everything,
let's redefine the model.

51
00:03:44.029 --> 00:03:47.938
Recompile our solver and
now in the model.fit,

52
00:03:47.938 --> 00:03:51.590
that's where the training is happening.

53
00:03:51.590 --> 00:03:56.568
And by the way,
another really nice function in Jupiter,

54
00:03:56.568 --> 00:04:02.967
if you just type two percents and
put time, it will print out the wall time,

55
00:04:02.967 --> 00:04:06.842
how long this particular
cell was executed.

56
00:04:06.842 --> 00:04:11.520
And all right, I said that we will
reduce the amount of epochs to ten.

57
00:04:11.520 --> 00:04:15.527
And to provide these test dataset,

58
00:04:15.527 --> 00:04:21.720
we can say, Where was it?

59
00:04:21.720 --> 00:04:23.840
Validation data, yes.

60
00:04:23.840 --> 00:04:30.320
So if I type validation data and
then I need to specify x_test, right,

61
00:04:30.320 --> 00:04:36.850
because that was our images, the one
our model haven't seen yet, and y_test.

62
00:04:36.850 --> 00:04:41.180
So in our, below.

63
00:04:41.180 --> 00:04:47.362
Let's just quickly take a look
at what we have in x_test.shape.

64
00:04:50.414 --> 00:04:56.720
So in x_test, we have another 10,000
images with the same resolution.

65
00:04:56.720 --> 00:05:01.507
So basically what will happen,
our model will be used x_train and

66
00:05:01.507 --> 00:05:03.909
y_train to be trained, right?

67
00:05:03.909 --> 00:05:09.967
But then after each epoch, it will
simply go through all the x_tests and

68
00:05:09.967 --> 00:05:12.648
y_tests to do the estimation,

69
00:05:12.648 --> 00:05:18.231
how accurately now we can recognize
those x_tests and y_tests.

70
00:05:18.231 --> 00:05:22.971
All right, so let's actually run the code.

71
00:05:22.971 --> 00:05:26.273
So right now we only printing
the loss function and accuracy for

72
00:05:26.273 --> 00:05:29.447
the training dataset, but
as soon as we done with the epoch,

73
00:05:29.447 --> 00:05:34.180
we doing the calculation of accuracy for
validation dataset and loss function.

74
00:05:34.180 --> 00:05:39.230
So true accuracy of our model is
actually higher than the training,

75
00:05:39.230 --> 00:05:41.544
which is pretty interesting.

76
00:05:41.544 --> 00:05:43.363
But that's happening sometimes, right?

77
00:05:43.363 --> 00:05:50.486
It's just the numbers in our validation
dataset is so obvious for instance, right?

78
00:05:50.486 --> 00:05:52.491
You can get slightly higher accuracy.

79
00:05:52.491 --> 00:05:58.643
But you can see that we somewhat
normalizing close to 97,

80
00:05:58.643 --> 00:06:02.220
I'd say almost 98% accuracy.

81
00:06:02.220 --> 00:06:10.850
So only 2% of our handwritten digits are
misrecognized, which is pretty awesome.

82
00:06:10.850 --> 00:06:16.178
But this data set,
it is kinda unique dataset.

83
00:06:16.178 --> 00:06:21.490
And this dataset considered to be
hello world for computer vision,

84
00:06:21.490 --> 00:06:25.530
because it's really simple and
it's really difficult to overfeed.

85
00:06:25.530 --> 00:06:28.942
So there is no kinda different objects
overlapping with each other, right?

86
00:06:28.942 --> 00:06:32.730
There is no images where zero and
five are sitting right next to each other.

87
00:06:32.730 --> 00:06:34.660
It's individual digits, right?

88
00:06:34.660 --> 00:06:37.370
So it's really clean, really easy.

89
00:06:38.580 --> 00:06:44.270
But still, it gives you an idea of
what we should do to train our models.

90
00:06:44.270 --> 00:06:47.150
All right, so with all of these,
what else can we do?

91
00:06:48.670 --> 00:06:51.210
We can now actually access history.

92
00:06:51.210 --> 00:06:56.195
So if you go to model.history,

93
00:06:56.195 --> 00:07:01.285
and for instance, history,
once again, what it will print

94
00:07:01.285 --> 00:07:06.625
out is how our loss function was changing
and how our accuracy was changing.

95
00:07:06.625 --> 00:07:09.866
That's for the training dataset, right,

96
00:07:09.866 --> 00:07:13.202
and how the validation
was changing as well.

97
00:07:13.202 --> 00:07:16.690
And that might be quite helpful
if you want to print it out.

98
00:07:16.690 --> 00:07:20.290
So for instance, what we can do,
we can say, do I have plt?

99
00:07:20.290 --> 00:07:21.720
Yes, I do.

100
00:07:21.720 --> 00:07:27.692
I can say please plot, We can see,

101
00:07:27.692 --> 00:07:31.040
let's h be our model.

102
00:07:31.040 --> 00:07:34.670
So h, if I run this,
is just the dictionary.

103
00:07:34.670 --> 00:07:38.947
So I can say, h Accuracy.

104
00:07:41.498 --> 00:07:44.784
So if I do this,

105
00:07:47.575 --> 00:07:52.319
It will print out to me how the accuracy
was changing with a time, right?

106
00:07:52.319 --> 00:07:55.564
And I can do the same thing for instance.

107
00:07:57.651 --> 00:08:03.096
There was validation, val_accuracy.

108
00:08:03.096 --> 00:08:05.770
And now, two plots.

109
00:08:05.770 --> 00:08:08.700
So that is really good, right?

110
00:08:08.700 --> 00:08:12.305
So basically, we gets to more or

111
00:08:12.305 --> 00:08:17.781
less stable accuracy of 97.5% even after,

112
00:08:17.781 --> 00:08:23.880
let's say, third epoch,
which is pretty good.

113
00:08:23.880 --> 00:08:29.110
And once again, our model is
fully connected in our network.

114
00:08:29.110 --> 00:08:31.090
So, let's just play along with it.

115
00:08:31.090 --> 00:08:34.070
What if I reduce the amount of neurons?

116
00:08:35.720 --> 00:08:37.220
How many do we want?

117
00:08:37.220 --> 00:08:38.020
Let's say 16.

118
00:08:39.620 --> 00:08:40.830
I don't know, right?

119
00:08:40.830 --> 00:08:46.491
So it basically means that,
all of these 700 plus pixels,

120
00:08:46.491 --> 00:08:49.710
we'll be talking to only 16 neurons.

121
00:08:49.710 --> 00:08:53.930
That's only one hidden layer we have, and
then the less layers still are 10 digits.

122
00:08:53.930 --> 00:09:01.110
So if we just rerun those, we see that's
the number produced here, perfect.

123
00:09:01.110 --> 00:09:06.830
We can still use the same optimizers,
right, and just do training.

124
00:09:06.830 --> 00:09:11.429
First of all, you can notice instantly
that the training is a little bit faster

125
00:09:11.429 --> 00:09:16.045
just because we need to, we're playing
with lots of data basically, right?

126
00:09:16.045 --> 00:09:21.520
And let's see,
the accuracy kind of dropped, right?

127
00:09:21.520 --> 00:09:26.012
So 92%,
remember that even after the first epoch,

128
00:09:26.012 --> 00:09:29.625
we got to almost like 97% instantly, and

129
00:09:29.625 --> 00:09:34.328
now it takes a little bit of
the time to get to even 95%.

130
00:09:36.917 --> 00:09:42.960
And yeah,
we can probably add additional layers.

131
00:09:42.960 --> 00:09:47.172
So although, we can have less,

132
00:09:51.176 --> 00:09:53.780
Less neurons overall.

133
00:09:53.780 --> 00:09:57.940
But just because we have
multiple layers like this,

134
00:09:59.730 --> 00:10:05.130
you will see in a second that it might
give us actually pretty good solution.

