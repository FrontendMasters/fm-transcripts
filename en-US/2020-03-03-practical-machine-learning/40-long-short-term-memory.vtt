WEBVTT

1
00:00:00.180 --> 00:00:05.450
Here, what we're are doing, we once again,
what we will do, we will use LSTM.

2
00:00:05.450 --> 00:00:09.700
LSTM stands for, Long Short Term Memory,

3
00:00:09.700 --> 00:00:14.170
it's one of the modes in the class
of recurrent neural networks.

4
00:00:14.170 --> 00:00:19.860
And in this example we'll just
grab texts by different authors.

5
00:00:21.350 --> 00:00:26.760
There'll be Butler, we can use
Shakespeare for instance, right?

6
00:00:26.760 --> 00:00:29.310
Yeah, I think I added
Shakespeare at the end.

7
00:00:29.310 --> 00:00:36.650
I wanted to actually have fun with this
example and download lyrics by Eminem,

8
00:00:36.650 --> 00:00:42.500
Back Street Boys and
who else, Britney spears.

9
00:00:42.500 --> 00:00:48.260
And just compare if we can distinguish
based on lyrics between those singers,

10
00:00:48.260 --> 00:00:53.440
but unfortunately,
website with lyrics it was down yesterday.

11
00:00:53.440 --> 00:00:58.812
So were not able to prepare this demo,
but we will rely on just authors.

12
00:00:58.812 --> 00:01:02.672
There is a lot of Shakespeare,

13
00:01:02.672 --> 00:01:06.541
Butler, Derby, and Couper.

14
00:01:06.541 --> 00:01:09.050
So, what are we doing here

15
00:01:09.050 --> 00:01:12.775
it's kinda another thing
that's more advanced example.

16
00:01:12.775 --> 00:01:17.995
Because we will have a particular
buffer and we'll specified batch size,

17
00:01:17.995 --> 00:01:20.985
and all of those can
be hyper parameter and

18
00:01:20.985 --> 00:01:25.030
will definitely affect the speed and
accuracy of our model.

19
00:01:25.030 --> 00:01:30.890
So we can just maybe later play
around with to get better results,

20
00:01:30.890 --> 00:01:36.330
but we're loading and
shuffling our data with the buffers.

21
00:01:36.330 --> 00:01:40.301
And you can see that the text we have,
for instance,

22
00:01:40.301 --> 00:01:44.453
that's not,
blah blah blah anger and yawn and

23
00:01:44.453 --> 00:01:48.800
at least those both texts
belong to the same author.

24
00:01:48.800 --> 00:01:53.382
But for instance,
this one is a different author and

25
00:01:53.382 --> 00:01:59.690
index 2, I think that's my,
so 0, 1, 2 it's Batler.

26
00:01:59.690 --> 00:02:05.350
But you got the idea, is basically phrases
which belong to different authors.

27
00:02:05.350 --> 00:02:09.810
And what we were trying to do,
we were trying to train our neural network

28
00:02:11.100 --> 00:02:16.530
to recognize which author
wrote a particular phrase.

29
00:02:18.080 --> 00:02:22.460
So tokenization takes
a little bit of time.

30
00:02:22.460 --> 00:02:26.870
And what tokenizer is doing is
just grabbing this raw text,

31
00:02:27.870 --> 00:02:33.040
finding the kind of putting it
in the database of words and

32
00:02:33.040 --> 00:02:36.770
then assigning index
to all of those words.

33
00:02:36.770 --> 00:02:42.180
And you can see that we have 23,000 unique

34
00:02:42.180 --> 00:02:46.794
words in our vocabulary,
which is quite impressive.

35
00:02:46.794 --> 00:02:51.555
All right, and
now our phrases, for instance,

36
00:02:51.555 --> 00:02:55.235
that was,
the example I grabbed was the first text.

37
00:02:55.235 --> 00:02:58.695
So the original text was just this phrase,

38
00:02:58.695 --> 00:03:02.985
it was reduced to the least of
those corresponding indexes.

39
00:03:04.367 --> 00:03:09.155
All right, and so
what we can do just prepare the data sets.

40
00:03:10.840 --> 00:03:14.660
Once again, can just use the padding,

41
00:03:14.660 --> 00:03:19.110
because we have phrases with
a different length, right?

42
00:03:19.110 --> 00:03:23.000
So what I can do,
I can just simply say that I will be

43
00:03:23.000 --> 00:03:28.240
using 16 words.

44
00:03:28.240 --> 00:03:34.200
And if for instance we have less
then we will populate first whatever

45
00:03:34.200 --> 00:03:38.850
kind of cells with my indices and
the rest will be equal to zero.

46
00:03:38.850 --> 00:03:41.440
Meaning that those words were not present.

47
00:03:41.440 --> 00:03:46.900
And if the text line I'm trying to
feed there have more than 16 words,

48
00:03:46.900 --> 00:03:54.530
I will simply ignore the rest, I will only
consider the first 16 words in the phrase.

49
00:03:54.530 --> 00:03:58.730
And the neural network is defined like
this, we're still using sequential.

50
00:03:59.810 --> 00:04:05.890
First we're use an embedding
with 64 degrees of freedom and

51
00:04:05.890 --> 00:04:08.460
bidirectional LSDM.

52
00:04:09.950 --> 00:04:15.330
Basically will allow us to memorize
the position of the words and

53
00:04:15.330 --> 00:04:18.480
then we can have, for instance,
couple of dense layers.

54
00:04:18.480 --> 00:04:22.669
So I'm just gonna be having 64 and for
instance in last layer we can have,

55
00:04:22.669 --> 00:04:25.570
I don't know, 16 for instance, right?

56
00:04:25.570 --> 00:04:32.270
So let's just run this model definition,
a compilation we'll use Adam optimizer and

57
00:04:32.270 --> 00:04:36.470
sparse categorical crossentropy
will be our loss function.

58
00:04:36.470 --> 00:04:43.660
And it will take about four minutes to
run, but I can probably just execute this

59
00:04:43.660 --> 00:04:48.620
and talk a little bit more about
things you can do with text.

60
00:04:48.620 --> 00:04:51.961
So, Yeah,

61
00:04:51.961 --> 00:04:57.758
it's taking some time probably just
to load all the data into the memory.

62
00:04:57.758 --> 00:05:04.059
Yeah, and it will be running only for
three epochs, but

63
00:05:04.059 --> 00:05:09.839
each epoch probably will
take about a minute or so.

64
00:05:09.839 --> 00:05:15.676
All right, I'll just wait for

65
00:05:15.676 --> 00:05:18.850
it to finish, and
I think that's gonna be the last notebook.

66
00:05:18.850 --> 00:05:24.594
So as soon as we get to this example
we can play around a little bit and

67
00:05:24.594 --> 00:05:28.398
that's gonna be it, gonna be done, yeah.

68
00:05:36.966 --> 00:05:42.950
Right, so training is continuing on and
you can see that it's our accuracy.

69
00:05:42.950 --> 00:05:49.087
Accuracy on the training data
set is already at about 77%.

70
00:05:49.087 --> 00:05:52.234
And for the,
[LAUGH] validation accuracy for

71
00:05:52.234 --> 00:05:55.898
some reason it didn't calculate it,
it's here.

72
00:05:58.680 --> 00:06:01.660
I'll say maybe if we'll
successfully calculate it on,

73
00:06:01.660 --> 00:06:03.530
it's actually doing it right now.

74
00:06:05.240 --> 00:06:10.646
Interesting, interesting, so
for the validation data set,

75
00:06:10.646 --> 00:06:15.241
the accuracy,
it's calculated was 87% not bad.

76
00:06:18.762 --> 00:06:20.881
&gt;&gt; What would you consider bad.?

77
00:06:20.881 --> 00:06:24.510
You've been saying not bad and
what seems like random numbers to me?

78
00:06:26.510 --> 00:06:30.232
Is it just because-
&gt;&gt; The thing is when I played with

79
00:06:30.232 --> 00:06:37.253
the text analytics, quite often I
was getting something close to 60%.

80
00:06:37.253 --> 00:06:42.700
And having almost 90, for me it's not bad.

81
00:06:42.700 --> 00:06:46.370
It's just yeah,
that's actually pretty interesting point.

82
00:06:48.550 --> 00:06:51.115
I consider machine learning and

83
00:06:51.115 --> 00:06:56.440
deep learning not to be a science
at this point but more like an art.

84
00:06:56.440 --> 00:07:00.966
And quite often you almost like need
to build an intuition what works and

85
00:07:00.966 --> 00:07:04.220
what not working for
a particular type of problem.

86
00:07:05.600 --> 00:07:08.100
Unfortunately, me telling
you that you need

87
00:07:08.100 --> 00:07:12.470
this particular number of neurons in your
layer and you want to use full, well,

88
00:07:12.470 --> 00:07:16.710
maybe you can say that fully connected
convolutional neural networks can be

89
00:07:16.710 --> 00:07:18.846
used for
these particular types of problems, right?

90
00:07:18.846 --> 00:07:20.942
And recurrent neural networks can used for
text,

91
00:07:20.942 --> 00:07:23.980
is just because we want to remember
the position on the words.

92
00:07:23.980 --> 00:07:26.400
But at the same time,
all other parameters can,

93
00:07:26.400 --> 00:07:30.370
how many neurons you want to
have in the intermediate layer.

94
00:07:30.370 --> 00:07:34.300
All of that almost kinda
comes with the intuition and

95
00:07:34.300 --> 00:07:39.550
learning which tricks works also heavily
depends on your training data set.

96
00:07:41.110 --> 00:07:45.715
Because really good phrase,
I like it a lot, you are what you eat.

97
00:07:45.715 --> 00:07:49.410
It exactly refers to neural networks.

98
00:07:49.410 --> 00:07:55.900
The thing is if your data set is polluted,
if it's not preprocessed properly,

99
00:07:55.900 --> 00:07:59.610
most probably your neural network
will learn something ridiculous.

100
00:07:59.610 --> 00:08:01.977
For instance,
if you have commas, two commas,

101
00:08:01.977 --> 00:08:04.720
then you comment probably
gonna be negative.

102
00:08:04.720 --> 00:08:05.950
It's not true, right?

103
00:08:05.950 --> 00:08:11.310
But somehow neural network might
figure out this dependency

104
00:08:11.310 --> 00:08:15.850
just because you had more examples which
were negative, which had two commas in it.

105
00:08:16.940 --> 00:08:20.330
It's ridiculous example, but
it actually what's happening in real life.

106
00:08:20.330 --> 00:08:26.070
So you just need to play around
with different techniques,

107
00:08:26.070 --> 00:08:29.870
even different activation functions and
almost like a build

108
00:08:31.580 --> 00:08:35.230
intuition, what to expect
depending on the model you having.

109
00:08:35.230 --> 00:08:40.180
And once again, as soon as even the same
type of problem but different data is

110
00:08:40.180 --> 00:08:45.500
applied, most probably you will have to
kinda reiterate once again and play around

111
00:08:45.500 --> 00:08:49.310
with different models and different setups
and play around with different parameters.

112
00:08:49.310 --> 00:08:53.960
So on top of choosing different
topologies, you have different

113
00:08:53.960 --> 00:08:58.205
activation functions getting inside of
those topologies and different structures.

114
00:08:58.205 --> 00:09:03.235
But also you have different optimizers,
different lost functions, and

115
00:09:03.235 --> 00:09:05.955
different hyper parameters
which control all of those.

116
00:09:05.955 --> 00:09:09.725
So you have so many degrees of freedom and
unfortunately,

117
00:09:09.725 --> 00:09:16.410
all of that is still heavily depend on
the data you're using for training.

118
00:09:16.410 --> 00:09:19.800
So, that's why I'm saying it's not
a direct science at this point.

119
00:09:19.800 --> 00:09:23.160
It's more like around building
the intuition and knowing what worked for

120
00:09:23.160 --> 00:09:26.400
you previously and
probably you should start from this point.

121
00:09:26.400 --> 00:09:30.130
But still to get better accuracy for

122
00:09:30.130 --> 00:09:34.210
your model most probably you'll have
to tweak a little bit your hyper

123
00:09:34.210 --> 00:09:38.530
parameters to get to this sweet spot where
the model will be performing the most.

124
00:09:38.530 --> 00:09:43.530
And especially with the text, a lot
will depend on preprocessing your text.

125
00:09:43.530 --> 00:09:49.220
How you build, how you transform
your raw text into numbers.

126
00:09:50.660 --> 00:09:55.160
All right, we pretty much done,
yeah, it took almost three minutes.

127
00:09:55.160 --> 00:09:59.180
So our evolution, we're doing it again.

128
00:09:59.180 --> 00:10:05.250
So we get to pretty close to 80% and
we can even try our examples.

129
00:10:05.250 --> 00:10:09.670
So, yeah, that's the phrase

130
00:10:09.670 --> 00:10:13.970
which converted to a particular index
words and we can do the prediction.

131
00:10:13.970 --> 00:10:15.960
And with pretty high accuracy,

132
00:10:15.960 --> 00:10:21.300
my model is saying that that's
the phrase was written by Shakespeare.

133
00:10:21.300 --> 00:10:25.210
Right because,
do you want to try something else.

134
00:10:25.210 --> 00:10:28.059
Give me some very well
known Shakespeare phrase

135
00:10:30.320 --> 00:10:31.870
&gt;&gt; To be or not to be?

136
00:10:31.870 --> 00:10:32.722
&gt;&gt; Good, okay.

137
00:10:32.722 --> 00:10:38.134
&gt;&gt; [LAUGH]
&gt;&gt; I have no idea, will it work or not?

138
00:10:40.207 --> 00:10:45.607
Yeah, it thinks we have 70% that
it is Shakespeare and yeah,

139
00:10:45.607 --> 00:10:51.010
about 16% that it was,
what was the author number two?

140
00:10:51.010 --> 00:10:53.279
Author number two was Derby.

