WEBVTT

1
00:00:00.000 --> 00:00:06.780
How we transition from machine learning
to deep learning is this next step.

2
00:00:06.780 --> 00:00:11.540
So, let's talk about deep learning next,
what is deep learning and

3
00:00:11.540 --> 00:00:14.470
where this deep or deep is coming from?

4
00:00:15.930 --> 00:00:23.100
I'll have to tell you a little bit about
the neural networks and neurons first.

5
00:00:23.100 --> 00:00:29.587
So let me introduce those concepts and
then we will return back to deep learning.

6
00:00:29.587 --> 00:00:34.810
So in 60s, 70s, well, and
actually in 50s as well,

7
00:00:34.810 --> 00:00:39.198
people come up with idea
that we can probably try to

8
00:00:39.198 --> 00:00:43.175
reproduce what's happening in our brains.

9
00:00:43.175 --> 00:00:45.170
What's happening with the neurons.

10
00:00:45.170 --> 00:00:51.090
And the way how neurons operates, if
they have a lot of input signals, right?

11
00:00:51.090 --> 00:00:53.020
Some data coming into the neuron.

12
00:00:53.020 --> 00:00:57.210
Neuron somehow can figure out what
is important and what is not.

13
00:00:57.210 --> 00:01:01.989
It can be activated or it can just
simply ignore the input signals, right?

14
00:01:01.989 --> 00:01:08.675
And you have the whole chains of those
neurons connected to each other, right?

15
00:01:08.675 --> 00:01:13.822
And at the ends, we're somehow processing
all of that input data which is

16
00:01:13.822 --> 00:01:19.310
coming to us in form of visual signal,
for instance, right, or sound.

17
00:01:19.310 --> 00:01:23.480
And all of those, something happening
with those chains activations or

18
00:01:23.480 --> 00:01:24.652
de-activations.

19
00:01:24.652 --> 00:01:28.678
And we're doing something
with this information.

20
00:01:28.678 --> 00:01:33.590
And people figured out,
we can probably try to replicate

21
00:01:33.590 --> 00:01:38.090
this process in the machine, or
through the algorithm, right?

22
00:01:38.090 --> 00:01:43.984
And let's say our algorithm, our model,
and I'm gonna call it neuron.

23
00:01:46.426 --> 00:01:50.210
First of all, what it's doing,
what's kinda the input information?

24
00:01:50.210 --> 00:01:55.413
Well, it can be just
some channels of data.

25
00:01:55.413 --> 00:02:02.585
So let's say, we can call it X1,
X2, X3 and X4.

26
00:02:02.585 --> 00:02:07.115
In computers, this data will be in
form of provided numbers, right?

27
00:02:07.115 --> 00:02:08.945
It can be integers or floats.

28
00:02:08.945 --> 00:02:10.621
Let's say just our floating point numbers.

29
00:02:10.621 --> 00:02:16.530
And, for
example I will assign 0.1 to X1 and

30
00:02:16.530 --> 00:02:20.679
then -4.7 to X2, 0 to X3.

31
00:02:20.679 --> 00:02:26.220
I don't know, 500 to X4,
something like that.

32
00:02:26.220 --> 00:02:29.040
So all of that is our input data, right?

33
00:02:29.040 --> 00:02:36.030
Then we need to do something in the neuron
to basically aggregate all of the data and

34
00:02:36.030 --> 00:02:40.950
maybe ignore some of the channels,
maybe amplify some of the channels.

35
00:02:40.950 --> 00:02:43.750
How can we do it mathematically?

36
00:02:43.750 --> 00:02:48.360
Well, we can assign weights
to all our channels.

37
00:02:48.360 --> 00:02:52.982
So W1, W2, W3 and
W4 is gonna be our weights,

38
00:02:52.982 --> 00:02:59.650
which we simply associate with
the corresponding input channel.

39
00:02:59.650 --> 00:03:06.839
And for instance, if we want to completely
ignore the signal coming from one channel,

40
00:03:06.839 --> 00:03:13.156
what we can do, we can simply multiply
X1 by W1, but W1 can be equal to 0.

41
00:03:13.156 --> 00:03:16.979
So if our weight's equal to 0,
it will nullify,

42
00:03:16.979 --> 00:03:20.984
it will completely dismiss
any data sitting in x1,

43
00:03:20.984 --> 00:03:24.822
because anything multiplied
by 0 is equal to 0.

44
00:03:24.822 --> 00:03:30.809
That's one of the ways to simply
ignore any input channels.

45
00:03:30.809 --> 00:03:35.451
If, for instance, W2, other weight,

46
00:03:35.451 --> 00:03:39.695
is pretty big, let's say, 500,

47
00:03:39.695 --> 00:03:44.471
multiplied by X2, it will simply amplify

48
00:03:44.471 --> 00:03:50.160
the signal coming in our X2 channel,
right?

49
00:03:50.160 --> 00:03:55.428
And the same thing, for instance,
if I want to deamplify the signal,

50
00:03:55.428 --> 00:03:58.660
for instance, in our X4, I have 500.

51
00:03:58.660 --> 00:04:03.134
I can multiply it by weight,
which is really,

52
00:04:03.134 --> 00:04:06.500
really small, like 0.001.

53
00:04:06.500 --> 00:04:09.160
And in this case,
it will give me just only half.

54
00:04:10.310 --> 00:04:17.490
So that's the way how we can simply
modify the signal amplitude, right?

55
00:04:17.490 --> 00:04:22.160
But then, all that information coming
from all different sources, and

56
00:04:22.160 --> 00:04:29.880
I need somehow to shrink it to one
information piece, how can I do this?

57
00:04:29.880 --> 00:04:34.245
Well, I can simply just add
all of those numbers together.

58
00:04:34.245 --> 00:04:36.910
I forgot X3 multiplied by W3.

59
00:04:38.910 --> 00:04:43.740
Right, so in this case I simply saying

60
00:04:43.740 --> 00:04:48.340
which channels I wants to ignore,
which channels I want to amplify.

61
00:04:49.830 --> 00:04:51.690
And adding the signal together, right?

62
00:04:51.690 --> 00:04:57.000
And that's becoming one signal,
which I can then process.

63
00:04:57.000 --> 00:05:01.198
So mathematically, all of that,

64
00:05:01.198 --> 00:05:06.697
all of what I just draw,
can be easily written

65
00:05:06.697 --> 00:05:11.497
as summation symbol Xi multiply by Wi.

66
00:05:11.497 --> 00:05:15.729
That's our input signals multiplied
by all of those weights, right?

67
00:05:15.729 --> 00:05:18.936
And right now we're not talking about
how weights should be selected.

68
00:05:18.936 --> 00:05:20.756
I'm just saying they are there.

69
00:05:20.756 --> 00:05:23.319
They describe what our neuron is doing.

70
00:05:24.671 --> 00:05:30.753
And also, just for
reasons which will be clearer later,

71
00:05:30.753 --> 00:05:35.315
we're adding some additional constant,

72
00:05:35.315 --> 00:05:39.420
bias to all of this distribution.

73
00:05:39.420 --> 00:05:45.408
It will allow us, for instance,
if we have constant signal going,

74
00:05:45.408 --> 00:05:48.677
we can then normalize it or shift it.

75
00:05:48.677 --> 00:05:52.710
So basically,
I'm adding bias to our neuron.

76
00:05:52.710 --> 00:05:55.740
And all of that is one data.

77
00:05:55.740 --> 00:06:01.880
So now in the neuron,
we have just this one data channel.

78
00:06:01.880 --> 00:06:04.630
Which is represented by
this simple formula.

79
00:06:04.630 --> 00:06:07.338
And I hope it's not too mathy, right?

80
00:06:07.338 --> 00:06:09.610
Those concepts are pretty basic.

81
00:06:11.170 --> 00:06:16.910
We simply just somehow modified
the input signals, right?

82
00:06:16.910 --> 00:06:21.020
And now we want to do something with it.

83
00:06:21.020 --> 00:06:22.420
So what can we do?

84
00:06:22.420 --> 00:06:27.660
In the neurons in our brain,
simply some neurons are activated,

85
00:06:27.660 --> 00:06:33.700
and in this case, they can just
send the signal to the next neuron.

86
00:06:33.700 --> 00:06:39.160
Or sometimes the signal is just simply
ignored, or a neuron is do nothing.

87
00:06:39.160 --> 00:06:41.574
So mathematically speaking,

88
00:06:41.574 --> 00:06:46.690
we can say that if the neuron is
activated, it can just output 1.

89
00:06:46.690 --> 00:06:52.105
And if it's not activated,
it can just output 0.

90
00:06:52.105 --> 00:06:53.437
That's it, right?

91
00:06:53.437 --> 00:06:59.754
How can we do this in our data, we can
have pretty much with all this summation,

92
00:06:59.754 --> 00:07:05.430
we can have any numbers from minus
infinity to plus infinity, right?

93
00:07:05.430 --> 00:07:13.123
And we can probably just do something with
those by modifying weights and biases.

94
00:07:13.123 --> 00:07:15.160
But it's still pretty large distribution.

95
00:07:15.160 --> 00:07:20.507
So to kind of do this
transformation from minus

96
00:07:20.507 --> 00:07:25.442
infinity to infinity
into a space of 0s and

97
00:07:25.442 --> 00:07:30.668
1, we can use sigmoid function, sigmoid.

98
00:07:30.668 --> 00:07:34.742
It's easier to explain what sigmoid
is doing by just drawing it.

99
00:07:34.742 --> 00:07:40.049
So let's say our axis, or,
no, we used axis before, so

100
00:07:40.049 --> 00:07:45.146
let's just call it data,
or input, input is better.

101
00:07:47.442 --> 00:07:53.845
So in our input axis, we have everything
from minus infinity to infinity.

102
00:07:53.845 --> 00:07:58.512
And on our output, we want to have

103
00:07:58.512 --> 00:08:03.859
something between 0 and 1, right?

104
00:08:03.859 --> 00:08:08.793
Or sometimes minus -1 and 1, depending.

105
00:08:08.793 --> 00:08:15.141
These rules, like don't have zero once,
it's what we define, right?

106
00:08:15.141 --> 00:08:19.967
And basically,
sigmoid is the function which

107
00:08:19.967 --> 00:08:24.916
is equal to pretty much 0 or
something really,

108
00:08:24.916 --> 00:08:30.624
really small at infinity and
crosses the 0 at 0.5.

109
00:08:30.624 --> 00:08:35.947
And then once, again, can go to
pretty close to 1 at the infinity.

110
00:08:35.947 --> 00:08:37.634
So that's what sigmoid function is.

111
00:08:37.634 --> 00:08:41.728
And no matter what you
provided as the input,

112
00:08:41.728 --> 00:08:47.242
it will shrink the distribution
of the outputs from 0 to 1.

