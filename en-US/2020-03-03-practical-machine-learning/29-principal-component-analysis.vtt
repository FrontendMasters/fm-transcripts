WEBVTT

1
00:00:00.020 --> 00:00:05.190
Let's go back to our notebook
called MNIST PCA Compression.

2
00:00:05.190 --> 00:00:08.180
PCA stands for
Principal Component Analysis.

3
00:00:08.180 --> 00:00:10.093
And what this technique,
it's a mathematical technique.

4
00:00:10.093 --> 00:00:14.888
What it's actually doing is,
let's say you have

5
00:00:14.888 --> 00:00:19.465
multi-dimensional data, whatever that is.

6
00:00:19.465 --> 00:00:23.835
So for instance, with our images,
28 by 28 pixels, as I said,

7
00:00:23.835 --> 00:00:27.453
each pixel can be some sort of
this independent dimension.

8
00:00:27.453 --> 00:00:32.076
What PCA is doing is just basically
looking at all of the provided data,

9
00:00:32.076 --> 00:00:34.859
in our case, those 60,000 images.

10
00:00:34.859 --> 00:00:38.735
And just saying, okay, for
that particular pixel, for

11
00:00:38.735 --> 00:00:42.710
this particular degree of freedom,
we have huge change.

12
00:00:42.710 --> 00:00:50.110
For instance, it's changing from 0,
to all the way to 255, right?

13
00:00:50.110 --> 00:00:51.548
So we're covering all the spectrum.

14
00:00:51.548 --> 00:00:57.540
And the numbers are not only 0 and
255, but all in between.

15
00:00:57.540 --> 00:01:02.120
So it's kinda looking at how different

16
00:01:02.120 --> 00:01:07.210
our particular degree of freedom,
in our case, pixel for the whole data set.

17
00:01:07.210 --> 00:01:12.030
And what it's doing is basically
saying okay, for instance, top pixel,

18
00:01:12.030 --> 00:01:13.770
top right pixel, always white.

19
00:01:14.860 --> 00:01:17.899
So we don't care about this information,
and we can probably drop it.

20
00:01:17.899 --> 00:01:23.270
And Principal Component Analysis doing
exactly that, or at least can do that.

21
00:01:23.270 --> 00:01:28.100
It can just see how much useful
information contained per degree

22
00:01:28.100 --> 00:01:33.630
of freedom, or per pixel, and
can give you a suggestion,

23
00:01:33.630 --> 00:01:36.250
if you can get rid of this information or
not.

24
00:01:36.250 --> 00:01:39.850
So let me actually [LAUGH]
demonstrate what I mean by this.

25
00:01:39.850 --> 00:01:45.622
So I will use Principal Component Analysis
implementation from scikit-learn.

26
00:01:45.622 --> 00:01:51.207
scikit-learn is an amazing Python package,
which contains a lot of statistical,

27
00:01:51.207 --> 00:01:55.387
and let's call them classical
machine learning algorithms.

28
00:01:55.387 --> 00:01:57.440
PCA is one of them.

29
00:01:57.440 --> 00:02:00.134
So we will be fetching MNIST once again.

30
00:02:00.134 --> 00:02:06.140
But it's already throughout this part of
the scikit-learn data set, for instance.

31
00:02:06.140 --> 00:02:08.430
Or it's still running,

32
00:02:09.580 --> 00:02:14.152
it's probably trying to download
files to my local machine.

33
00:02:14.152 --> 00:02:19.198
All right, so
we're splitting our test into,

34
00:02:19.198 --> 00:02:21.856
yeah, data and target.

35
00:02:21.856 --> 00:02:26.003
And that's basically the ratio,
how it will control my demo.

36
00:02:26.003 --> 00:02:29.850
I will say how much
information I want to preserve.

37
00:02:29.850 --> 00:02:35.560
So for instance,
I'd say keep 95% of the information.

38
00:02:35.560 --> 00:02:38.539
So that's why percentage set to 0.95.

39
00:02:38.539 --> 00:02:41.970
I will get back to you,
it's in the next iteration.

40
00:02:41.970 --> 00:02:44.800
So and here, I'm just doing
Principal Component Analysis.

41
00:02:44.800 --> 00:02:49.862
I'm saying to which dimensions can

42
00:02:49.862 --> 00:02:58.474
our model be reduced to preserve
95% of the information?

43
00:02:58.474 --> 00:03:05.041
And it's saying out of 784, that's how
many pixels, original pixels we have.

44
00:03:05.041 --> 00:03:09.285
It's enough to have only 154 dimensions.

45
00:03:09.285 --> 00:03:17.274
We have 90% [LAUGH] accuracy to preserve
the information about this whole data set.

46
00:03:17.274 --> 00:03:21.717
So if I just plot how
our information depends

47
00:03:21.717 --> 00:03:25.960
on number of degrees of freedom, right?

48
00:03:25.960 --> 00:03:33.510
Or dimensions, you'll see that basically,
100% will be at 784, right?

49
00:03:33.510 --> 00:03:37.653
But if I want to lose some information,
for instance,

50
00:03:37.653 --> 00:03:42.259
if I want to preserve only 50%
of the useful information.

51
00:03:42.259 --> 00:03:44.462
It means that I can shrink, or

52
00:03:44.462 --> 00:03:51.070
compress my data set up to only something
like 10 useful degrees of freedom.

53
00:03:51.070 --> 00:03:55.075
It's very abstract,
what I'm explaining right now, but

54
00:03:55.075 --> 00:03:57.965
this, pretty easily, it can be visualized.

55
00:03:57.965 --> 00:04:02.248
So for instance, with a PCA,
I'm just doing recalculations.

56
00:04:02.248 --> 00:04:07.322
Once again, computing the exact
percentage of information,

57
00:04:07.322 --> 00:04:12.394
which will be propagated if I use
only that degrees of freedom,

58
00:04:12.394 --> 00:04:18.010
and doing recalculation,
kind of a recreation of the digit itself.

59
00:04:18.010 --> 00:04:21.006
And now I can plot those digits.

60
00:04:21.006 --> 00:04:23.600
So there are a couple of things.

61
00:04:23.600 --> 00:04:28.620
So that was the original
digits on the left, right?

62
00:04:28.620 --> 00:04:35.509
And I said I'm willing to lose
5% of useful information, right?

63
00:04:35.509 --> 00:04:40.338
And have only 95% propagated,
recreate again.

64
00:04:40.338 --> 00:04:44.700
As you can see, we can still recognize
those handwritten digits, right?

65
00:04:44.700 --> 00:04:50.870
1, 0, but there's some artifacts appeared,
and also the background became grey.

66
00:04:52.770 --> 00:04:58.740
It's because our PCA model decided
that's not useful information at all,

67
00:04:58.740 --> 00:05:02.490
and get rid of all the background
information for us.

68
00:05:02.490 --> 00:05:07.850
It only kept the most interesting one,
kinda the whole digits.

69
00:05:07.850 --> 00:05:12.230
So the reason why I'm
showing to you this is

70
00:05:12.230 --> 00:05:17.650
if we change our percentage to,
let's say, 5, so

71
00:05:17.650 --> 00:05:22.600
only 50% of the useful
information can be propagated.

72
00:05:22.600 --> 00:05:29.500
You will see that we only
can have 11 dimensions,

73
00:05:29.500 --> 00:05:34.479
or you can think about 11 neurons.

74
00:05:34.479 --> 00:05:38.240
They can still have all
the information about the digits,

75
00:05:38.240 --> 00:05:41.060
handwritten digits with plugging.

76
00:05:41.060 --> 00:05:47.766
So with all that change,
let's see how it will affect the images.

77
00:05:47.766 --> 00:05:50.502
All right, so
I'm just recalculating the PCAs.

78
00:05:50.502 --> 00:05:56.010
Yeah, 51% actually will be propagated.

79
00:05:56.010 --> 00:06:00.030
So you can see that image
is blurred significantly.

80
00:06:00.030 --> 00:06:04.770
And some of them actually not that
clearly recognizable anymore.

81
00:06:04.770 --> 00:06:09.070
And so for instance,
this was supposed to be two, but

82
00:06:09.070 --> 00:06:12.620
for me, it looks like almost nine, right?

83
00:06:12.620 --> 00:06:15.360
So Principal Component Analysis,

84
00:06:15.360 --> 00:06:20.670
the reason why, once again,
I showed it to you, is two things.

85
00:06:20.670 --> 00:06:25.950
First of all, think about all of this
data you're doing as the information,

86
00:06:25.950 --> 00:06:28.790
and that's your neural network.

87
00:06:28.790 --> 00:06:33.564
Just taking this information,
available information, and to some degree,

88
00:06:33.564 --> 00:06:35.601
compressing it, do something.

89
00:06:35.601 --> 00:06:39.303
The ends, we just have those 10 neurons,
which will be activated,

90
00:06:39.303 --> 00:06:41.220
depending on the label, right?

91
00:06:41.220 --> 00:06:48.008
But it also can give you an idea what
should be the smallest number of neurons

92
00:06:48.008 --> 00:06:53.305
to actually get useful
information propagated through?

93
00:06:53.305 --> 00:07:01.740
So once again,
let's play around with really scenarios.

94
00:07:01.740 --> 00:07:06.053
For instance, if I only want
1% of the useful information

95
00:07:06.053 --> 00:07:10.920
propagated, Through the model.

96
00:07:10.920 --> 00:07:16.380
With 1%, that means [LAUGH]
only 1 neuron will be active.

97
00:07:18.560 --> 00:07:21.110
With 1 neuron, let's see.

98
00:07:21.110 --> 00:07:23.560
Okay, it's calculating right now, cool.

99
00:07:25.820 --> 00:07:27.989
So we definitely can recognize 0,
that's perfect.

100
00:07:27.989 --> 00:07:32.720
[LAUGH] the rest is somewhat blurred,
right?

101
00:07:32.720 --> 00:07:40.885
But it's only kind of fun example to
demonstrate that you can play around.

102
00:07:40.885 --> 00:07:43.892
And there's definitely a connection
between information theory,

103
00:07:43.892 --> 00:07:45.505
machine learning, deep learning.

104
00:07:45.505 --> 00:07:49.760
And to better understand how
neural networks operates,

105
00:07:49.760 --> 00:07:53.080
you might dig into information theory.

106
00:07:53.080 --> 00:07:57.553
And Principal Component Analysis is
one of the way to actually figure

107
00:07:57.553 --> 00:08:01.575
out what's the most important
channels of the information?

108
00:08:01.575 --> 00:08:03.149
And if you modify them,

109
00:08:03.149 --> 00:08:08.360
how it will affect the information at
the other side of the model, so to speak.

