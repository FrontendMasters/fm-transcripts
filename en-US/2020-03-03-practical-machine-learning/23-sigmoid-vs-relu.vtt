WEBVTT

1
00:00:00.290 --> 00:00:03.880
Another interesting
activation function is ReLU.

2
00:00:03.880 --> 00:00:07.190
ReLU stands for Rectified Linear Units.

3
00:00:07.190 --> 00:00:10.982
And why it is so interesting?

4
00:00:10.982 --> 00:00:15.500
I think I want to spend just two
minutes to draw a couple of things for

5
00:00:15.500 --> 00:00:19.630
you to actually explain what ReLU
is doing and why it is so cool.

6
00:00:21.130 --> 00:00:24.180
So on our new diagram,

7
00:00:24.180 --> 00:00:29.271
we will be comparing sigmoid

8
00:00:29.271 --> 00:00:36.640
versus ReLU, Rectified Linear Units.

9
00:00:36.640 --> 00:00:42.280
So sigmoids, if you remember it
will just take all of those signals

10
00:00:45.010 --> 00:00:50.225
and simply transforming
it in something between 0

11
00:00:50.225 --> 00:00:54.690
and 1, that's what sigmoid is doing.

12
00:00:56.420 --> 00:00:58.830
ReLU is doing something different.

13
00:00:59.850 --> 00:01:05.420
So it also takes all this signals, but

14
00:01:05.420 --> 00:01:10.520
it will simply, actually, let's use green.

15
00:01:10.520 --> 00:01:16.210
It we'll set it to 0 when our
signals are equal to 0 and

16
00:01:16.210 --> 00:01:20.350
it will return the same
value when it's positive.

17
00:01:20.350 --> 00:01:25.012
So you can think about it that relu

18
00:01:25.012 --> 00:01:30.010
mathematically speaking, oops,

19
00:01:30.010 --> 00:01:35.515
sorry, rely, relu = max(x, 0).

20
00:01:36.670 --> 00:01:39.500
It's mathematically how
the ReLU is defined.

21
00:01:39.500 --> 00:01:43.747
And the reason why it is so
cool, why it is,

22
00:01:46.727 --> 00:01:51.668
So fast actually,
it's because when our number is negative,

23
00:01:51.668 --> 00:01:55.010
it just should return zero, right?

24
00:01:55.010 --> 00:01:58.870
And when it's positive,
it will return the x itself.

25
00:01:58.870 --> 00:02:03.350
So for this function to process on
the hardware, what we will need to do,

26
00:02:03.350 --> 00:02:06.170
we need to check only one beat.

27
00:02:06.170 --> 00:02:08.270
What is the sine of our number?

28
00:02:08.270 --> 00:02:11.800
And if the sine is negative,
we just simply return back 0.

29
00:02:11.800 --> 00:02:14.750
If it's positive,
we return the number itself.

30
00:02:14.750 --> 00:02:19.560
At the same time with a signoid,
you have exponent sitting aside,

31
00:02:19.560 --> 00:02:24.710
so there should be some lookup function
right, to find the proper value.

32
00:02:24.710 --> 00:02:30.020
So when you have exponents in your
functions, it complicates things.

33
00:02:30.020 --> 00:02:34.386
So calculating even one
value of sigmoid of some x,

34
00:02:34.386 --> 00:02:39.158
will probably require several
iterations on the CPU and

35
00:02:39.158 --> 00:02:43.134
GPU, but with ReLU,
you can do it in one tick.

36
00:02:43.134 --> 00:02:45.580
And you can paralyze a lot of things.

37
00:02:45.580 --> 00:02:49.236
So that's why ReLU is cool and
ReLU have been introduced by Google, guys.

38
00:02:49.236 --> 00:02:55.801
And the original idea why
sigmoid was introduced first,

39
00:02:55.801 --> 00:03:01.545
they were afraid that if
we have Redstone signal,

40
00:03:01.545 --> 00:03:05.420
and don't limit it to once.

41
00:03:05.420 --> 00:03:08.353
The sound might be way too big, right?

42
00:03:08.353 --> 00:03:15.007
For instance, in this case if we
using ReLu it's still the case, but

43
00:03:15.007 --> 00:03:20.310
we can overcome it by simply
switching to higher flow.

44
00:03:20.310 --> 00:03:24.660
Basically, we can instead
of using 32 bits,

45
00:03:24.660 --> 00:03:29.026
per number like in our 32 bit floats,
right?

46
00:03:29.026 --> 00:03:31.520
Or four bytes per floating point number.

47
00:03:31.520 --> 00:03:34.481
We can switch 64 bits for instance.

48
00:03:34.481 --> 00:03:38.270
So double precision will allow you
to extend the dynamic range of

49
00:03:38.270 --> 00:03:43.320
the calculations and therefore you can
process significantly larger numbers.

50
00:03:43.320 --> 00:03:47.330
And the thing is there are other
mechanisms which helps you to regulate and

51
00:03:47.330 --> 00:03:50.630
not go to very large values.

52
00:03:50.630 --> 00:03:53.870
Now, it's scope for the regularization
in machine learning, and

53
00:03:53.870 --> 00:03:59.270
basically allows you to,
all those weights to adjust

54
00:03:59.270 --> 00:04:04.460
depending on the amplitude of the signal
not get to those really large numbers.

55
00:04:04.460 --> 00:04:06.630
But you still get
the benefit of this really,

56
00:04:06.630 --> 00:04:09.310
really fast calculation of relativity.

