WEBVTT

1
00:00:00.570 --> 00:00:05.470
Right now, if I just run this codes and
rerun the summary, now you can see

2
00:00:05.470 --> 00:00:10.980
that's for instance after flatting our
two dimensional image into one dimension.

3
00:00:10.980 --> 00:00:14.630
So basically it became kind of one layer,
right?

4
00:00:14.630 --> 00:00:18.930
We then connectied to 256 neurons,

5
00:00:18.930 --> 00:00:25.090
each pixel intensity of each pixel will
be provided to all of our 256 neurons.

6
00:00:26.320 --> 00:00:30.420
So each neuron basically
will see the whole image,

7
00:00:31.470 --> 00:00:35.350
and then some of the neurons
will be activated and

8
00:00:35.350 --> 00:00:39.130
the last layer will contain
only ten neurons but

9
00:00:39.130 --> 00:00:44.450
they will be connected to all of our
256 neurons in the previous layer.

10
00:00:44.450 --> 00:00:45.944
So that's what's fully connected.

11
00:00:45.944 --> 00:00:49.537
&gt;&gt; Did our simple mention that?

12
00:00:49.537 --> 00:00:54.720
&gt;&gt; Yes, although I'm not gonna
be drawing all 256 of them.

13
00:00:54.720 --> 00:00:58.468
But sure, I can draw something simple.

14
00:00:58.468 --> 00:01:04.506
So let's switch to diagrams, and So

15
00:01:04.506 --> 00:01:09.652
as I said, we have three layers, flatten,

16
00:01:09.652 --> 00:01:15.506
then dense with 256, and
then dense with ten.

17
00:01:15.506 --> 00:01:20.422
So basically you can imagine
that two dimensional

18
00:01:20.422 --> 00:01:24.760
image Sorry, 28 by 28 pixels, right?

19
00:01:26.140 --> 00:01:30.130
That's our input data
provided to the neurons.

20
00:01:30.130 --> 00:01:34.440
But our neurons in, actually,
we can use even different colors.

21
00:01:34.440 --> 00:01:39.390
So D256, that's our second layer, right?

22
00:01:39.390 --> 00:01:44.348
So imagine we have kinda
200 neurons like this.

23
00:01:47.993 --> 00:01:53.396
So what flattened layer is
doing is just simply taking

24
00:01:53.396 --> 00:01:59.169
first 28 pixels, and
that's gonna be our first 28,

25
00:02:01.701 --> 00:02:08.080
Neurons, or
actually signals provided to the neurons.

26
00:02:08.080 --> 00:02:11.180
But you can think about it as
the intermediate step, right?

27
00:02:11.180 --> 00:02:18.290
And then the next 28 pixels will be
flattened or put right next to it.

28
00:02:18.290 --> 00:02:25.000
So basically that's your first 28,
next 28, and like this another 28 times.

29
00:02:26.170 --> 00:02:28.610
So the last layer will go here.

30
00:02:29.810 --> 00:02:35.310
And what's happening next just because
I said want to have dense layer,

31
00:02:35.310 --> 00:02:40.630
it means that all my signals so
the information from the first pixel,

32
00:02:40.630 --> 00:02:48.980
right, its intensity will be provided
as the channel one for all the neurons.

33
00:02:50.360 --> 00:02:54.080
And yeah, I can basically a lot
of lines like this, right?

34
00:02:54.080 --> 00:02:57.349
And second pixel will be also
connected to all the neurons.

35
00:02:57.349 --> 00:03:04.472
And all the Pixels will
be connected [LAUGH].

36
00:03:04.472 --> 00:03:08.264
So that's the dense layer,
right and then for

37
00:03:08.264 --> 00:03:14.000
the lowest layer where we have one,
two, three, four, ten pixels,

38
00:03:14.000 --> 00:03:18.690
or sorry, ten neurons will
have exactly the same thing.

39
00:03:18.690 --> 00:03:24.460
So first neuron from the previous layer
will be connected to all ten neurons,

40
00:03:24.460 --> 00:03:28.710
second will be connected to all ten
neurons, and so on and so forth.

41
00:03:28.710 --> 00:03:34.165
Basically, it means that
in our connections,

42
00:03:34.165 --> 00:03:42.256
we'll have to have ten multiplied
by 256 weights, right?

43
00:03:42.256 --> 00:03:47.889
So 2,560 Ws,

44
00:03:47.889 --> 00:03:52.330
and then there also should be biases,
right?

45
00:03:52.330 --> 00:03:57.790
So plus 10 biases and
that's how many numbers

46
00:03:59.510 --> 00:04:05.110
will be used here to preserve the state.

47
00:04:05.110 --> 00:04:10.780
So for instance,
whenever pixel is activated here, right?

48
00:04:10.780 --> 00:04:14.490
So with all of those
fully connected layers,

49
00:04:14.490 --> 00:04:19.330
we have all these signals about each

50
00:04:19.330 --> 00:04:24.130
pixel available to all the neurons
in the first layer, right?

51
00:04:24.130 --> 00:04:29.040
So that's our dense layer 1,

52
00:04:29.040 --> 00:04:35.492
and this will be our dense layer 2.

53
00:04:35.492 --> 00:04:40.030
So it basically means that, yes,

54
00:04:40.030 --> 00:04:44.270
we have to keep track of all
of those weights and biases.

55
00:04:44.270 --> 00:04:50.601
And for instance, how the training
will look like for this scenario,

56
00:04:50.601 --> 00:04:56.839
let's say we simply provide its
image of 0, handwritten 0 right?

57
00:04:56.839 --> 00:04:59.697
And we say in okay, you should look at 0,

58
00:04:59.697 --> 00:05:04.873
it means that this neuron should give
us the highest probability, right?

59
00:05:04.873 --> 00:05:09.532
Because it correspond to, 0,
3 so that's going to be neuron,

60
00:05:09.532 --> 00:05:13.275
which should be activated
when it's looking at 0.

61
00:05:13.275 --> 00:05:17.045
So it will simply, if the prediction
was done correctly, right?

62
00:05:17.045 --> 00:05:20.321
So for instance,
the intensity of this particular pixel, or

63
00:05:20.321 --> 00:05:22.090
let me draw with something else.

64
00:05:22.090 --> 00:05:26.892
We took this pixel, right,
which had some intensity,

65
00:05:26.892 --> 00:05:30.569
just because it was
the ink in this pixel and

66
00:05:30.569 --> 00:05:35.620
let's say this was provided for
all the neurons, right?

67
00:05:35.620 --> 00:05:42.228
But remember that we're activating
our weights randomly, right?

68
00:05:42.228 --> 00:05:46.790
So, when we did multiplication
of intensity of this particular

69
00:05:48.360 --> 00:05:51.730
pixel by corresponding weights,

70
00:05:51.730 --> 00:05:57.150
some neurons will not notice it because
weights was simply equal to 0, right?

71
00:05:57.150 --> 00:06:01.700
And some will actually get really strong
signal if the weight was pretty high and

72
00:06:01.700 --> 00:06:06.560
they will propagate a base
to our next layer, right?

73
00:06:06.560 --> 00:06:11.170
And for instance, there is a chance that
prediction will be done correctly, that

74
00:06:11.170 --> 00:06:17.410
we're looking at 0, and that will only
kinda Increase those weights even more.

75
00:06:17.410 --> 00:06:22.580
If we miss predicted, for instance,
although we were looking at 0,

76
00:06:22.580 --> 00:06:27.130
and some of the pathways were activated
but the prediction was made that we

77
00:06:27.130 --> 00:06:31.110
are looking looking at 2, jist because
of the randomness of the weights,

78
00:06:31.110 --> 00:06:36.620
we'll try to actually get rid of those
strong connections by minimizing

79
00:06:36.620 --> 00:06:42.510
the weights and reinforcing the weights
which should have gave us 0 instead.

80
00:06:42.510 --> 00:06:46.120
So that's what backpropagation is doing,
right?

81
00:06:46.120 --> 00:06:49.640
And basically if you do it's
if you train intense in our

82
00:06:51.510 --> 00:06:55.975
model, if we look at for
instance, what can we do?

83
00:06:55.975 --> 00:07:00.900
x_train, and let's look at shape,

84
00:07:02.660 --> 00:07:06.220
you can see that we actually
have 60,000 images.

85
00:07:06.220 --> 00:07:13.510
So you need to have pretty a large
dataset to train your model.

86
00:07:13.510 --> 00:07:14.140
And yeah,

87
00:07:14.140 --> 00:07:19.400
this 28 by 28 does the resolution
of our original handwritten digits.

88
00:07:19.400 --> 00:07:24.870
At the beginning, you're starting with
the random initialized model, right?

89
00:07:24.870 --> 00:07:29.980
But eventually, similar to how
we started with this line right,

90
00:07:29.980 --> 00:07:35.050
and iteratively got to the solution,
we kind of doing the same here.

91
00:07:35.050 --> 00:07:39.520
We starting with all of those
random weights and biases, but

92
00:07:39.520 --> 00:07:43.900
telling our model what
answer we should get,

93
00:07:43.900 --> 00:07:48.690
we simply modifying those weights and
biases to get to the right solution.

94
00:07:48.690 --> 00:07:52.740
But we need to do a lot of
iterations like that, and

95
00:07:52.740 --> 00:07:57.030
our data set should be large enough
with all different variations.

96
00:07:57.030 --> 00:07:59.940
So you can imagine, we have what,

97
00:07:59.940 --> 00:08:04.200
ten different classes and
60,000, yeah 60,000 images,

98
00:08:04.200 --> 00:08:09.130
it means that we have 6,000 images
per each digit, which is pretty huge.

