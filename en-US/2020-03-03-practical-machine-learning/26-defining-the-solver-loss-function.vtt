WEBVTT

1
00:00:00.400 --> 00:00:01.966
So we defined our model, right?

2
00:00:01.966 --> 00:00:04.730
We have this flatten to dense layers.

3
00:00:04.730 --> 00:00:10.890
What we need to do next is to tell,
can define the solver.

4
00:00:10.890 --> 00:00:15.724
So solver will contain
basically two things.

5
00:00:15.724 --> 00:00:20.619
We will need to specify how we
want get to the right solution and

6
00:00:20.619 --> 00:00:23.920
what should be our loss function.

7
00:00:23.920 --> 00:00:30.044
So if we say something
like this model.compile,

8
00:00:30.044 --> 00:00:35.530
and that's basically completion of
our solver for that particular model.

9
00:00:35.530 --> 00:00:39.250
And as the parameters,
we can specify optimizer.

10
00:00:40.460 --> 00:00:45.289
So we can use something like stochastic

11
00:00:45.289 --> 00:00:49.688
gradient descent, SGD, or adam.

12
00:00:49.688 --> 00:00:52.217
In the previous example
with linear regression,

13
00:00:52.217 --> 00:00:55.298
we used something similar to
stochastic gradient descent.

14
00:00:55.298 --> 00:00:58.172
We started with some random point and

15
00:00:58.172 --> 00:01:02.438
gradually through descending
in our gradient space,

16
00:01:02.438 --> 00:01:07.829
we get to the extremor of the function or
sorry, mathematic jargon.

17
00:01:07.829 --> 00:01:12.675
[LAUGH] Let's say the solution,
the minimum of the function, in this case,

18
00:01:12.675 --> 00:01:15.950
it was the minimum of the error function.

19
00:01:15.950 --> 00:01:20.370
In our case, adam is just another solver
which can get to this point a little bit

20
00:01:20.370 --> 00:01:26.620
quicker and we still need to
provide the loss function.

21
00:01:26.620 --> 00:01:30.643
So loss function, there are several
variants what you can use here.

22
00:01:30.643 --> 00:01:38.250
By the way, just out of curiosity,
if we type something like this, no.

23
00:01:39.460 --> 00:01:45.190
I think I need to specify a question mark
after the function to read about it.

24
00:01:45.190 --> 00:01:49.738
So if I type model.compile and
then puts question mark,

25
00:01:49.738 --> 00:01:53.163
yeah, I can read more about the function.

26
00:01:53.163 --> 00:01:59.394
So for instance, the default optimizer
is rmsprop, which stands for

27
00:01:59.394 --> 00:02:05.107
Root Mean Square Prop,
I don't remember what prop stands for.

28
00:02:05.107 --> 00:02:09.941
[LAUGH] But it's just another
way to get to this minimum.

29
00:02:09.941 --> 00:02:15.620
And for the loss function, yeah, there
are different already predefined losses.

30
00:02:15.620 --> 00:02:20.190
So we can actually go directly
to TensorFlow losses.

31
00:02:21.290 --> 00:02:29.330
The beauty of Keras is that it can
use TensorFlow functions easily.

32
00:02:29.330 --> 00:02:35.510
So you can actually mix TensorFlow and
Keras and vice versa quite easily.

33
00:02:35.510 --> 00:02:40.692
So what we can do,
we can simply type TensorFlow losses, and

34
00:02:40.692 --> 00:02:46.300
just look at available losses to us and
there are plenty, right?

35
00:02:46.300 --> 00:02:53.220
So for our problem, just because
we have ten neurons with Softmax,

36
00:02:53.220 --> 00:03:00.780
it's usually recommended to have
sparse categorical cross entropy.

37
00:03:00.780 --> 00:03:01.950
Let's see, do we?

38
00:03:01.950 --> 00:03:06.430
Yeah, cross entropy right here.

39
00:03:06.430 --> 00:03:07.860
What else do we need?

40
00:03:07.860 --> 00:03:12.060
So let's say optimizer,
loss function and metrics.

41
00:03:12.060 --> 00:03:15.430
Yeah, we need to specify how
we gonna measure our success.

42
00:03:16.480 --> 00:03:20.060
So let me just close this one, metrics.

43
00:03:21.250 --> 00:03:25.216
And once again I can use
TensorFlow predefined class, or

44
00:03:25.216 --> 00:03:28.116
you can say I will just measure accuracy.

45
00:03:31.352 --> 00:03:33.900
Oops, I think I misspelled something.

46
00:03:33.900 --> 00:03:36.720
Let's see, what did I misspell,
accuracy, okay.

47
00:03:36.720 --> 00:03:39.380
Probably it should be capitalized.

48
00:03:43.283 --> 00:03:48.334
No, I know what the problem is.

49
00:03:48.334 --> 00:03:53.700
The thing is, with the metrics,
I'm providing the whole list

50
00:03:53.700 --> 00:04:00.930
of things I'm measuring my performance
by and accuracy can be just one of them.

51
00:04:00.930 --> 00:04:04.190
So let's run this code now,
still a mistake.

52
00:04:05.790 --> 00:04:12.433
Expect it, and that was a mistake.

53
00:04:15.105 --> 00:04:16.910
Watch it return.

54
00:04:16.910 --> 00:04:22.080
Okay, one second, let me quickly
take a look at the code again.

55
00:04:22.080 --> 00:04:24.186
So okay, we have,

56
00:04:27.290 --> 00:04:32.980
Let's switch to pure Keras encoding.

57
00:04:32.980 --> 00:04:40.083
So with optimizer adam with the loss
function, we can say sparse,

58
00:04:45.779 --> 00:04:54.404
categorical_crossentropy.

59
00:04:54.404 --> 00:04:55.410
I have a question.
Yes.

60
00:04:55.410 --> 00:05:00.020
Is it possible that it could be ACC,
ACC for accuracy?

61
00:05:01.040 --> 00:05:05.276
I think it was with the previous
version of TensorFlow and

62
00:05:05.276 --> 00:05:11.112
they switched from ACC to accuracy kind
of full spelling, but we can check it.

63
00:05:11.112 --> 00:05:11.681
Yeah.

64
00:05:11.681 --> 00:05:12.703
Okay.

65
00:05:12.703 --> 00:05:13.460
Accuracy?

66
00:05:14.790 --> 00:05:16.419
Did I misspell it?

67
00:05:16.419 --> 00:05:17.645
No.

68
00:05:17.645 --> 00:05:21.634
Accuracy, okay, it worked.

69
00:05:21.634 --> 00:05:25.298
[LAUGH] I know what happened.

70
00:05:25.298 --> 00:05:27.910
I use TensorFlow but yeah,

71
00:05:27.910 --> 00:05:32.930
I should have basically

72
00:05:32.930 --> 00:05:37.510
specified to create the object of that
class and that provided the class itself.

73
00:05:37.510 --> 00:05:44.810
Basically, if you spell this with words,
right, that's the recommended method

74
00:05:44.810 --> 00:05:49.910
of referring to different optimizer and
losses in Keras.

75
00:05:49.910 --> 00:05:53.300
And I think for the beginners,
it's the optimal way.

76
00:05:53.300 --> 00:05:58.829
If you want to switch to
TensorFlow optimizers for

77
00:05:58.829 --> 00:06:02.810
instance, right, and then adam.

78
00:06:02.810 --> 00:06:07.510
For instance, if I execute it right now,
it will also create the mistake,

79
00:06:07.510 --> 00:06:12.730
because I need to create subclass and
now it's working.

80
00:06:12.730 --> 00:06:17.800
But it also allows you to, for instance,
specify additional parameters.

81
00:06:17.800 --> 00:06:19.700
So different optimizers,

82
00:06:19.700 --> 00:06:23.930
you can change those hyper
parameters like beta and epsilon.

83
00:06:23.930 --> 00:06:29.460
It kinda defines the behavior, how our
optimizer will be getting to the solution.

84
00:06:29.460 --> 00:06:31.970
But we can leave all
the default values right now.

85
00:06:31.970 --> 00:06:32.700
It doesn't really matter.

