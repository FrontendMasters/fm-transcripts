WEBVTT

1
00:00:00.150 --> 00:00:04.480
The next notebook which
will demonstrate another,

2
00:00:06.240 --> 00:00:08.510
let's say technique in
natural language processing.

3
00:00:08.510 --> 00:00:10.260
That's what NLP stands for.

4
00:00:10.260 --> 00:00:14.835
And by the way,
if you wanna play with a demo,

5
00:00:14.835 --> 00:00:18.020
[LAUGH] this one is pretty interesting.

6
00:00:18.020 --> 00:00:22.120
It basically recognize toxicity,

7
00:00:22.120 --> 00:00:28.460
meaning that is there an insult or
sexual explicit.

8
00:00:28.460 --> 00:00:31.351
I don't wanna type anything here but

9
00:00:31.351 --> 00:00:36.488
you can play with it on your own and
see it's actually pretty fun.

10
00:00:36.488 --> 00:00:42.852
[LAUGH] And the source code is available
on GitHub as well under tfjs models.

11
00:00:42.852 --> 00:00:48.740
All right, so get back to
different word representations.

12
00:00:48.740 --> 00:00:55.010
So the main problem or
I guess problem with text is that

13
00:00:55.010 --> 00:01:01.750
we need to somehow convert those letters
and words into numerical representation,

14
00:01:01.750 --> 00:01:06.350
which then can be used by
the narrow network for training.

15
00:01:06.350 --> 00:01:12.050
And one of the ways to actually
create the embeddings, remember,

16
00:01:12.050 --> 00:01:19.150
I was talking about dimensionality
reductions but I was doing it with images.

17
00:01:19.150 --> 00:01:21.490
And when we're saying that for
handwritten digits,

18
00:01:21.490 --> 00:01:26.430
we can probably reduce the dimensionality
to only I know 11 degrees of freedom and

19
00:01:26.430 --> 00:01:29.130
still it will preserve the most
important information.

20
00:01:29.130 --> 00:01:32.860
With the words,
you can kinda do the similar thing.

21
00:01:34.060 --> 00:01:39.640
So you can just specify that you have
a particular degrees of freedom,

22
00:01:39.640 --> 00:01:45.140
and each word will be just a point
in this multidimensional space.

23
00:01:45.140 --> 00:01:50.553
And the idea is that our neural
network somehow might even

24
00:01:50.553 --> 00:01:56.490
figure out where to put kinda
words with a similar meaning.

25
00:01:56.490 --> 00:02:00.100
If they are synonyms, they probably
gonna be really close to each other.

26
00:02:00.100 --> 00:02:05.320
And then relationship, remember I said for
instance, almost this kinda

27
00:02:05.320 --> 00:02:11.930
Euclidean distance between boy and
man can be similar to woman and girl.

28
00:02:11.930 --> 00:02:15.470
So embedding,
what embedding is this technique,

29
00:02:15.470 --> 00:02:18.815
it's basically just by analyzing text,

30
00:02:18.815 --> 00:02:25.040
we'll try to assign
a particular coordinates to all

31
00:02:25.040 --> 00:02:29.390
the words it will meet in
this multidimensional space.

32
00:02:29.390 --> 00:02:32.470
And the thing is it's
part of the keras itself.

33
00:02:32.470 --> 00:02:36.540
So in layers, you can use embedding and

34
00:02:36.540 --> 00:02:42.300
specify how many, well, how many
dimensions you want to have, right?

35
00:02:42.300 --> 00:02:45.720
And I think that's the how many
words we will be providing.

36
00:02:45.720 --> 00:02:47.690
But let me quickly just run this code.

37
00:02:47.690 --> 00:02:54.340
And we can use the imdb data set
once again to train our model.

38
00:02:55.840 --> 00:03:02.770
So the text in this case, we actually
have slightly different data set, right?

39
00:03:02.770 --> 00:03:07.380
There is words, this underscore
basically means that that's the space so

40
00:03:07.380 --> 00:03:09.050
we are looking at the end of the word.

41
00:03:09.050 --> 00:03:11.632
But it can be part of the words.

42
00:03:11.632 --> 00:03:17.550
Yeah, I don't have the, br for instance,
it doesn't really have the under words.

43
00:03:17.550 --> 00:03:18.952
It's just part of some other words.

44
00:03:18.952 --> 00:03:23.500
All right,
without going too deep into the details,

45
00:03:23.500 --> 00:03:29.120
we can say that I want 16
dimensions in my embedding and

46
00:03:29.120 --> 00:03:32.540
my keras will be Sequential, right?

47
00:03:32.540 --> 00:03:37.290
So the first layer will be this Embedding
so kinda space which will transform

48
00:03:37.290 --> 00:03:42.650
the vocabulary into these 16
dimensional representation.

49
00:03:42.650 --> 00:03:46.490
Then what I want to do,
I want just to kinda average,

50
00:03:46.490 --> 00:03:51.490
find the average among
all of those channels.

51
00:03:51.490 --> 00:03:55.870
And finally, just with one
Dense layer with the sigmoid,

52
00:03:56.880 --> 00:04:00.700
it will tell me if it's positive or
negative review.

53
00:04:00.700 --> 00:04:05.855
So the idea here I'm actually just
training this embedding layer, right,

54
00:04:05.855 --> 00:04:09.735
to somehow grasp the representation
of different words.

55
00:04:09.735 --> 00:04:12.445
So let's see how long it will
take me to train, one minute.

56
00:04:12.445 --> 00:04:17.476
So okay, I can probably just
run this code and wait for it.

57
00:04:17.476 --> 00:04:24.225
So you can see that loss function for
my training actually was decreasing for

58
00:04:24.225 --> 00:04:28.548
the training data set and
once again kinda starts

59
00:04:28.548 --> 00:04:33.840
fluctuating quite a bit even
after probably seven epochs.

60
00:04:33.840 --> 00:04:38.512
So once again,
it's indicator of somewhat overfeeding.

61
00:04:38.512 --> 00:04:44.320
So if we look at the accuracy,
it's not decreasing that drastically.

62
00:04:44.320 --> 00:04:47.280
So I can look like it's still
fluctuating quite a bit.

63
00:04:47.280 --> 00:04:51.840
So I guess I can even stop
after probably fourth

64
00:04:51.840 --> 00:04:55.970
epoch to have more kinda stable solution,
generalized solution.

65
00:04:57.070 --> 00:04:59.070
All right, is it done?

66
00:05:00.330 --> 00:05:08.420
Six, we about half through the training
Let's just wait for it to finish.

67
00:05:08.420 --> 00:05:11.624
It should take another 20 seconds.

68
00:05:11.624 --> 00:05:16.036
&gt;&gt; Is this sort of training
probably the same that Amazon uses?

69
00:05:16.036 --> 00:05:20.380
For example, on their ratings,
it's like when it says this is the most

70
00:05:20.380 --> 00:05:22.960
critical review or
this is the most positive review?

71
00:05:24.230 --> 00:05:29.770
&gt;&gt; They use slightly different algorithm.

72
00:05:29.770 --> 00:05:34.862
So this dataset particularly is just
taking the reviews provided and

73
00:05:34.862 --> 00:05:38.120
just saying if it's positive or negative.

74
00:05:38.120 --> 00:05:41.854
You're probably talking about
recommender system, right,

75
00:05:41.854 --> 00:05:45.438
recommendation system or-
&gt;&gt; Just on each product that says here's

76
00:05:45.438 --> 00:05:49.285
all the reviews but here's the top most
critical and here's the most positive one.

77
00:05:51.509 --> 00:05:55.772
&gt;&gt; Yeah, but I felt they don't use
machine learning algorithm for that.

78
00:05:55.772 --> 00:06:02.490
I think there's a system of weighted
voting, the comments up and down, right?

79
00:06:02.490 --> 00:06:04.790
And I think it's actually not
using machine learning there.

80
00:06:04.790 --> 00:06:09.170
It's just simply showing you
the most voted up positive review or

81
00:06:09.170 --> 00:06:10.040
something like that.

82
00:06:10.040 --> 00:06:11.680
But technically yes, you can apply

83
00:06:12.930 --> 00:06:17.520
deep learning to recognize
the positivity of the comment.

84
00:06:17.520 --> 00:06:21.300
But the thing is this system,
the one we're considering right now,

85
00:06:21.300 --> 00:06:23.690
they are somewhat pretty simple.

86
00:06:23.690 --> 00:06:29.163
And yeah,
I just want to demonstrate this quickly.

87
00:06:31.301 --> 00:06:36.490
So on Google, there is a thing
called Embedding Projector.

88
00:06:36.490 --> 00:06:40.746
What I did right now,
I just grabbed our embedding layer, right,

89
00:06:40.746 --> 00:06:43.407
our embedding layer was layer number 0.

90
00:06:43.407 --> 00:06:50.257
And I simply saved the vector, so
those kinda 16 dimensional data and

91
00:06:50.257 --> 00:06:55.090
also the words into those two files,
vecs.tsv.

92
00:06:55.090 --> 00:06:58.758
And I can use Embedding Projector
to visualize all of that.

93
00:07:01.258 --> 00:07:08.030
So right now, it's Word2Vec data set.

94
00:07:08.030 --> 00:07:09.430
And I can load my own.

95
00:07:09.430 --> 00:07:12.930
So for instance, on the first one,
it should be vectors.

96
00:07:12.930 --> 00:07:17.398
So I need to go to Documents, practical,

97
00:07:17.398 --> 00:07:21.739
and file's vec, should be uploaded and

98
00:07:21.739 --> 00:07:28.139
also [INAUDIBLE] the data from here,
the one we just graded.

99
00:07:28.139 --> 00:07:34.020
All right, and that's it,
right, Click outside to dismiss.

100
00:07:34.020 --> 00:07:36.380
Perfect, so
we just loaded our own data set.

101
00:07:36.380 --> 00:07:40.330
But it's not, so we had 16 dimensions.

102
00:07:40.330 --> 00:07:43.390
It kinda reduced to
the most important ones,

103
00:07:43.390 --> 00:07:47.790
only three dimensions because we cannot
visualize 16 dimensions on my screen.

104
00:07:47.790 --> 00:07:52.540
But the most interesting part if we
just put the words explicitly, so for

105
00:07:52.540 --> 00:07:57.330
instance, what is interesting here?

106
00:07:57.330 --> 00:08:01.070
I definitely can see that great,
beautiful,

107
00:08:01.070 --> 00:08:03.750
wonderful on these parts, right?

108
00:08:03.750 --> 00:08:07.880
And the opposite side there is unhappy,
poorly, terrible.

109
00:08:07.880 --> 00:08:11.380
So somehow our system
already can separate and

110
00:08:11.380 --> 00:08:15.000
assign negative values to those
words positive to those, right?

111
00:08:15.000 --> 00:08:19.910
And then basically it will influence
how we're doing the prediction.

112
00:08:19.910 --> 00:08:25.340
If you kinda turn this around,
maybe we can find some odd there,

113
00:08:25.340 --> 00:08:31.160
interesting dependencies,
well, grades, perfect.

114
00:08:31.160 --> 00:08:32.080
I don't see anything.

115
00:08:33.170 --> 00:08:38.447
Excellent, yeah,
it seems it basically tried to find

116
00:08:38.447 --> 00:08:43.841
the most important words which
helped us to distinguish

117
00:08:43.841 --> 00:08:49.366
if we're looking at the positive or
negative feedback.

118
00:08:49.366 --> 00:08:55.020
But the thing is I found some
very interesting dependencies.

119
00:08:55.020 --> 00:09:00.950
For instance, DVD and VHS, [LAUGH] were
on different sides of the spectrum.

120
00:09:00.950 --> 00:09:06.493
I couldn't explain why it was
happening like that, but it was.

121
00:09:06.493 --> 00:09:11.195
So somehow ,yeah, probably words,
DVDs were in more

122
00:09:11.195 --> 00:09:16.010
positive feedbacks and
VHS negatives, I don't know.

123
00:09:16.010 --> 00:09:19.240
But that's one of the way to
kinda to try to understand what

124
00:09:19.240 --> 00:09:23.532
the embedding is actually doing, right?

125
00:09:23.532 --> 00:09:25.750
And so it allows you embedding.

126
00:09:25.750 --> 00:09:27.330
When you're training like this,

127
00:09:27.330 --> 00:09:31.580
it will might even learn some
dependencies between the words.

128
00:09:31.580 --> 00:09:37.280
For instance,
you can type kinda well, word well,

129
00:09:37.280 --> 00:09:43.170
and see what's the words with the kinda
similar meaning or similar condition.

130
00:09:43.170 --> 00:09:45.029
I actually like Euclidean.

131
00:09:45.029 --> 00:09:49.590
Well, unique, well, job,
I'm not sure how to interpret this.

132
00:09:49.590 --> 00:09:52.681
Let's find something else like good.

133
00:09:55.614 --> 00:09:58.563
Convention, Powell-
&gt;&gt; Why are there some words with

134
00:09:58.563 --> 00:10:00.300
underscores after them?

135
00:10:00.300 --> 00:10:03.900
&gt;&gt; So
it's the way how the dataset was prepared.

136
00:10:03.900 --> 00:10:09.380
So interested with underscore
means that we are looking

137
00:10:09.380 --> 00:10:16.020
at the word interested but split,
it's only part of the word.

138
00:10:16.020 --> 00:10:21.960
So for instance, where it's splitful
will be represented by this word.

139
00:10:21.960 --> 00:10:23.630
Is there a word splitful?

140
00:10:23.630 --> 00:10:26.500
Maybe not but you've got the idea
is basically means that it

141
00:10:26.500 --> 00:10:29.105
was only part of the work as well as nte.

142
00:10:30.190 --> 00:10:34.975
It's not that a word but it was part of
the word and sometimes they're doing it.

143
00:10:34.975 --> 00:10:36.640
They're just building parts of the word.

144
00:10:36.640 --> 00:10:41.540
Because they're trying to find maybe just
some combination were part of the word

145
00:10:41.540 --> 00:10:47.300
still gonna preserve some meaning
about the whole maybe even sentence.

146
00:10:47.300 --> 00:10:50.795
So it's just the tricks that you're
doing with text to once again,

147
00:10:50.795 --> 00:10:54.950
kinda convert this just written
form into digital representation.

148
00:10:54.950 --> 00:10:59.696
And the embedding is just the additional
obstruction on whatever bag of words you

149
00:10:59.696 --> 00:11:02.851
created by kinda cutting and
chopping parts of them.

