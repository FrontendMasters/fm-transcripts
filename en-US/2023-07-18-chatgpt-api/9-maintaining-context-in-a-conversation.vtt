WEBVTT

1
00:00:00.090 --> 00:00:05.466
So you know that when you're chatting with
ChatGPT, it maintains the context, right?

2
00:00:05.466 --> 00:00:09.461
So I can ask another question, I said,

3
00:00:09.461 --> 00:00:13.831
do you know in which
city they are located?

4
00:00:13.831 --> 00:00:15.113
Let's wait.

5
00:00:15.113 --> 00:00:16.454
I will answer the wait.

6
00:00:16.454 --> 00:00:19.910
I'm sorry, I don't know which
city you're referring to,

7
00:00:19.910 --> 00:00:24.537
can you please provide more context or
specify which city you're asking about?

8
00:00:24.537 --> 00:00:26.839
So what happened here?

9
00:00:26.839 --> 00:00:32.451
Actually, on every request it's
losing the previous context.

10
00:00:32.451 --> 00:00:36.797
So it doesn't know that we were
talking about Frontend Masters.

11
00:00:36.797 --> 00:00:41.577
So let's answer one question at a time.

12
00:00:41.577 --> 00:00:43.052
We are waiting.

13
00:00:43.052 --> 00:00:47.595
You can see that we are receiving
the whole answer at once,

14
00:00:47.595 --> 00:00:52.615
not word by word or token by token,
as we were used to on ChatGPT.

15
00:00:52.615 --> 00:00:54.483
Can we emulate that thing?

16
00:00:54.483 --> 00:00:55.699
Yes, it's possible.

17
00:00:55.699 --> 00:00:58.539
It's a property called a stream.

18
00:00:58.539 --> 00:01:03.544
Let me show you the difference here
when we were using the terminal.

19
00:01:03.544 --> 00:01:08.404
So, there is another property, you can
apply more properties here, for example,

20
00:01:08.404 --> 00:01:10.531
here we can change the temperature.

21
00:01:10.531 --> 00:01:16.793
Remember that value between 0 and 1, so
I can say 0.5 and I can say a stream.

22
00:01:16.793 --> 00:01:19.636
And I can ask for
true by default is false.

23
00:01:19.636 --> 00:01:26.034
So if I try this stream,
Not in the terminal.

24
00:01:29.476 --> 00:01:32.374
Let's look at the difference.

25
00:01:32.374 --> 00:01:36.513
Instead of giving me one answer at once,

26
00:01:36.513 --> 00:01:42.003
it's sending me partial JSONs per word,
actually.

27
00:01:43.785 --> 00:01:48.924
And it's not,
now the text now is not JSON value, okay?

28
00:01:48.924 --> 00:01:52.143
So it's partial JSONs
meaning JSONs over HTTP.

29
00:01:52.143 --> 00:01:56.318
You need to actually
know how to handle that.

30
00:01:56.318 --> 00:01:57.488
Can you lead in JavaScript?

31
00:01:57.488 --> 00:02:01.871
Yes, but we are not going to waste time
with that because we know that we are not

32
00:02:01.871 --> 00:02:03.565
going to do that client side.

33
00:02:03.565 --> 00:02:06.691
But there is a way to
work with streams with,

34
00:02:06.691 --> 00:02:09.743
with a buffer stream where an HTTP stream.

35
00:02:09.743 --> 00:02:16.647
So we can read, in this case, slice by
a slice, chunk by chunk in JavaScript,

36
00:02:16.647 --> 00:02:22.837
we need to get rid of the data colon,
and then parse the string as JSON.

37
00:02:22.837 --> 00:02:28.566
And we're going to receive
basically a delta.

38
00:02:28.566 --> 00:02:31.066
Should we say delta somewhere here, delta.

39
00:02:31.066 --> 00:02:32.252
So what's delta?

40
00:02:32.252 --> 00:02:37.209
It's the part of the full message
that is coming right now.

41
00:02:37.209 --> 00:02:43.631
And then we need to add word by word
while the ChatGPT is typing, okay?

42
00:02:43.631 --> 00:02:45.619
But anyway, most of the time,

43
00:02:45.619 --> 00:02:50.910
unless you are actually going to make
a ChatGPT clone, we don't need the stream.

44
00:02:50.910 --> 00:02:53.738
So streaming the content first,

45
00:02:53.738 --> 00:02:59.598
the answer is longer because now we
have more actually JSON to parse.

46
00:02:59.598 --> 00:03:03.381
And most of the times we don't
need that streaming, okay?

47
00:03:03.381 --> 00:03:04.772
So, we need the answer.

48
00:03:04.772 --> 00:03:07.052
So, we don't typically do that, okay?

49
00:03:07.052 --> 00:03:09.261
But that's just the waiting part.

50
00:03:09.261 --> 00:03:14.913
Now, the worst part is that
our chat is kind of silly.

51
00:03:14.913 --> 00:03:19.715
In fact let me retry so it wasn't
just random, I will ask about myself.

52
00:03:19.715 --> 00:03:22.973
Who is Maximiliano Firtman?

53
00:03:22.973 --> 00:03:27.738
It knows me, so we wait for
the full answer.

54
00:03:27.738 --> 00:03:32.950
And it's an Argentinian author, trainer,
speaker, blah, blah, blah, blah.

55
00:03:32.950 --> 00:03:33.994
Where does he live?

56
00:03:37.180 --> 00:03:42.065
I'm sorry, I don't have a context,
who are you referring to?

57
00:03:42.065 --> 00:03:45.511
So, by default, and
this is something that for

58
00:03:45.511 --> 00:03:49.831
me was a completely surprise
when I realized how this work.

59
00:03:49.831 --> 00:03:55.532
Every request on a chat must
include the previous context,

60
00:03:55.532 --> 00:03:58.048
we need to do it manually.

61
00:03:58.048 --> 00:04:03.315
Server side,
OpenAI is not keeping the history

62
00:04:03.315 --> 00:04:07.671
that in this LLM work we know as memory.

63
00:04:07.671 --> 00:04:11.934
So, memory between conversations.

64
00:04:11.934 --> 00:04:13.752
So, how does it work?

65
00:04:13.752 --> 00:04:16.441
We need to do it manually, how?

66
00:04:16.441 --> 00:04:24.090
On every new message, we need to
add the previous messages as well.

67
00:04:24.090 --> 00:04:25.034
Does it makes sense?

68
00:04:25.034 --> 00:04:27.758
I'm not sure if it makes sense,
but that's how it works.

69
00:04:27.758 --> 00:04:31.654
In fact, that's how ChatGPT works.

70
00:04:31.654 --> 00:04:35.677
Remember will but
sometimes I have very long answers and

71
00:04:35.677 --> 00:04:38.369
then I have a long prompt and
long answers.

72
00:04:38.369 --> 00:04:42.098
Every time I'm sending a new question
he's sending the previous, yeah, and

73
00:04:42.098 --> 00:04:43.918
you're paying for the tokens again.

74
00:04:43.918 --> 00:04:46.268
Yes, you are.

75
00:04:46.268 --> 00:04:50.893
That's why at one point, you probably felt

76
00:04:50.893 --> 00:04:55.893
that with ChatGPT,
ChatGPT is actually lost.

77
00:04:55.893 --> 00:05:00.260
So you say, but we were talking about
something, you were like right and

78
00:05:00.260 --> 00:05:02.060
now you're hallucinating.

79
00:05:02.060 --> 00:05:05.834
That's because it has a limit of tokens.

80
00:05:05.834 --> 00:05:12.026
At one point, it's removing
the first messages in the chat.

81
00:05:12.026 --> 00:05:12.630
Makes sense?

82
00:05:12.630 --> 00:05:16.370
That's why GPT4 has a bigger window,

83
00:05:16.370 --> 00:05:21.933
a larger window of tokens,
then it can hold more context.

84
00:05:21.933 --> 00:05:26.225
That's how it works today at least,
we don't have any other option,

85
00:05:26.225 --> 00:05:27.990
we actually need to do this.

86
00:05:27.990 --> 00:05:34.233
So that's why I already have an array of
messages, so we can start collecting this.

87
00:05:34.233 --> 00:05:38.165
So, we actually need to, for example,

88
00:05:38.165 --> 00:05:44.419
this role can be just the first
message that we have here, okay?

89
00:05:44.419 --> 00:05:50.090
And by the way, we cannot hear your
computer answering questions as

90
00:05:50.090 --> 00:05:55.373
a pirate or always in Spanish,
so and you will use that, okay?

91
00:05:55.373 --> 00:06:02.552
As the system message has the power for
how it answers like in a funny way or

92
00:06:02.552 --> 00:06:07.747
in a confident way, or
in a short way, or whatever.

93
00:06:07.747 --> 00:06:10.609
So what we need to do always, actually,

94
00:06:10.609 --> 00:06:14.495
is to send the whole array
that we have there, always.

95
00:06:14.495 --> 00:06:20.228
And here,
what we need is to add this to the array.

96
00:06:20.228 --> 00:06:25.588
So, messages.push, and
we are going to add that one.

97
00:06:25.588 --> 00:06:30.486
So we parse the whole array of messages,
in fact, we don't need this,

98
00:06:30.486 --> 00:06:33.277
we can just do this in ECMAScript today.

99
00:06:33.277 --> 00:06:34.863
And then this is important.

100
00:06:34.863 --> 00:06:41.061
When we receive the answer,
we also need to save the whole message,

101
00:06:41.061 --> 00:06:47.390
because if not it won't have
the context of what it answered before.

102
00:06:47.390 --> 00:06:54.710
So here, not just the content,
but also the role.

103
00:06:54.710 --> 00:07:00.639
So let me show you that.
So, I'm going to also say messages push,

104
00:07:00.639 --> 00:07:08.079
and we are going to add
json.choices sub 0 .message.

105
00:07:08.079 --> 00:07:12.336
That's the whole message
including the role assistant.

106
00:07:12.336 --> 00:07:17.586
So, let's see if now this works better.

107
00:07:17.586 --> 00:07:22.624
So, who is Maximiliano Firtman?

108
00:07:26.581 --> 00:07:28.784
Blah blah blah, never heard of him.

109
00:07:28.784 --> 00:07:30.647
Come on, you're hallucinating.

110
00:07:30.647 --> 00:07:35.682
Look at that, that's hallucination,
and it's weird.

111
00:07:35.682 --> 00:07:37.800
They say, why is hallucinating?

112
00:07:37.800 --> 00:07:43.569
You were all witnesses that it knew me,
but now it says, never heard of him.

113
00:07:43.569 --> 00:07:47.560
But also, it's funny,
can you look at that?

114
00:07:47.560 --> 00:07:51.099
I bet he's a computer genius.

115
00:07:51.099 --> 00:07:54.922
Well, it's a mix between hallucination and
funny.

116
00:07:54.922 --> 00:07:56.188
So why hallucination?

117
00:07:56.188 --> 00:08:00.967
Well, probably because we
didn't set here temperature,

118
00:08:00.967 --> 00:08:04.131
and by default temperature is 0.5.

119
00:08:04.131 --> 00:08:08.160
So if you don't want that,
send temperature 0.

120
00:08:08.160 --> 00:08:11.568
In that case,
to the same question who received mostly,

121
00:08:11.568 --> 00:08:13.775
the same answer all the time, okay?

122
00:08:13.775 --> 00:08:14.931
Are we gonna remove the funny part?

123
00:08:14.931 --> 00:08:17.252
I don't know, we can keep it or not.

124
00:08:17.252 --> 00:08:19.370
You can play with this.

125
00:08:19.370 --> 00:08:21.185
So let's do that again.

126
00:08:21.185 --> 00:08:25.768
We need a clear button so we can start
from chat who is Maximiliano Firtman?

127
00:08:32.855 --> 00:08:36.631
Okay, now we are back
to the original answer.

128
00:08:36.631 --> 00:08:38.580
And where does he live?

129
00:08:43.998 --> 00:08:47.148
I mean, the answer is not
that he doesn't have context.

130
00:08:47.148 --> 00:08:50.698
It says, I don't know where he lives,
but, yeah,

131
00:08:50.698 --> 00:08:55.485
because the answer is that he knows me,
we're talking about myself.

132
00:08:55.485 --> 00:08:57.913
We can say, I don't know, something else.

133
00:08:57.913 --> 00:09:01.743
Do you know any books written by him?

134
00:09:03.858 --> 00:09:07.435
Anyway, you know that it's
sending the context, okay?

135
00:09:07.435 --> 00:09:10.917
Because even here, we can actually
check the array of messages,

136
00:09:10.917 --> 00:09:13.209
we are accumulating the full chat, okay?

137
00:09:13.209 --> 00:09:15.267
And here, we have some books.

138
00:09:15.267 --> 00:09:19.656
And it's I think it's through,
High Performance Mobile Web, that's fine.

139
00:09:19.656 --> 00:09:23.428
Programming the, yeah,
no you hallucinate with that one.

140
00:09:23.428 --> 00:09:28.302
And never, no,
maybe it's the translation because I wrote

141
00:09:28.302 --> 00:09:31.950
one in Spanish and
maybe that's a translation.

142
00:09:31.950 --> 00:09:36.985
Because I have one that is,
it's something like that in Spanish, yeah.

143
00:09:36.985 --> 00:09:41.254
Okay, I'm sorry GPT for
saying that you were a liar.

144
00:09:41.254 --> 00:09:45.590
Anyway, so we have our ChatGPT clone, and

145
00:09:45.590 --> 00:09:51.379
with this sample,
we understood how ChatGPT APIs work.

146
00:09:51.379 --> 00:09:56.076
So we need to make API calls all the time,
and also,

147
00:09:56.076 --> 00:09:59.809
we have to send the context all the time.

148
00:09:59.809 --> 00:10:01.095
And of course, think about this.

149
00:10:01.095 --> 00:10:04.144
Every time you're sending this
you're using more tokens.

150
00:10:04.144 --> 00:10:10.993
So this is kind of exponential in terms
of how many tokens you will spend.

151
00:10:10.993 --> 00:10:16.910
And if you want the amount of tokens
used are coming back in the response,

152
00:10:16.910 --> 00:10:21.188
so you can accumulate that and
then have some stats.

153
00:10:21.188 --> 00:10:26.160
And also, if you go now to
the OpenAI website, your dashboard,

154
00:10:26.160 --> 00:10:28.737
you can actually see each call and

155
00:10:28.737 --> 00:10:32.811
you can see how many tokens
you have been consuming.

