WEBVTT

1
00:00:00.020 --> 00:00:02.520
&gt;&gt; Scott Moss: I'm gonna
move on to caching.

2
00:00:03.830 --> 00:00:07.367
And when I talk about caching in GraphQL,

3
00:00:07.367 --> 00:00:12.633
when I say the word caching in a server,
what do people think?

4
00:00:12.633 --> 00:00:14.042
&gt;&gt; Student: Redis.
&gt;&gt; Scott Moss: You think Redis?

5
00:00:14.042 --> 00:00:15.292
Okay, what about you?

6
00:00:15.292 --> 00:00:19.186
&gt;&gt; Student2: Well, I'm thinking about
exactly what you're talking about,

7
00:00:19.186 --> 00:00:19.933
so-
&gt;&gt; Scott Moss: Okay,

8
00:00:19.933 --> 00:00:22.014
not you [LAUGH] What about you?

9
00:00:22.014 --> 00:00:28.086
&gt;&gt; Student3: I don't even have an idea.

10
00:00:28.086 --> 00:00:30.157
&gt;&gt; Scott Moss: All right, sure.

11
00:00:30.157 --> 00:00:31.724
&gt;&gt; Student4: It's the request
on the request level.

12
00:00:31.724 --> 00:00:33.028
&gt;&gt; Scott Moss: We're caching the request.

13
00:00:33.028 --> 00:00:38.630
&gt;&gt; Student4: And
then caching in memory units of data.

14
00:00:38.630 --> 00:00:41.434
So like with Redis or something in memory.

15
00:00:41.434 --> 00:00:45.100
&gt;&gt; Scott Moss: Yeah, yeah,
all those are caching on the back end.

16
00:00:45.100 --> 00:00:46.484
And there's tons more, right?

17
00:00:46.484 --> 00:00:50.380
People are caching database crazed,
not even in Redis, so

18
00:00:50.380 --> 00:00:51.510
it's like on the database.

19
00:00:51.510 --> 00:00:52.860
So it's like, my god.

20
00:00:52.860 --> 00:00:55.360
So there's so
many different ways to cache.

21
00:00:55.360 --> 00:00:57.740
What I'm going to specifically
be talking about, at least for

22
00:00:57.740 --> 00:01:03.520
this example, is going to be
caching in memory per request.

23
00:01:03.520 --> 00:01:09.127
And the way we do that is something
called a data loader in GraphQL.

24
00:01:09.127 --> 00:01:12.940
So this one's kinda complicated,
but it is so worth it.

25
00:01:14.410 --> 00:01:17.080
So what I'm gonna do here is,
am I on the right branch?

26
00:01:21.257 --> 00:01:22.360
&gt;&gt; Scott Moss: Let me check
out if it's the right branch.

27
00:01:28.433 --> 00:01:31.444
&gt;&gt; Scott Moss: All right.

28
00:01:31.444 --> 00:01:35.960
So the problem we're having is,
let's talk about the problem first.

29
00:01:35.960 --> 00:01:38.830
Is basically what I've been
discussing this whole time.

30
00:01:38.830 --> 00:01:43.560
So let's say we have,
we're querying for projects.

31
00:01:43.560 --> 00:01:45.570
And then we're querying for tasks.

32
00:01:45.570 --> 00:01:47.410
And then inside that task query,
we're also querying for

33
00:01:47.410 --> 00:01:50.320
projects and
then we're also querying for task.

34
00:01:50.320 --> 00:01:52.690
Okay, and let's just get the name.

35
00:01:53.880 --> 00:01:57.030
Now if we just look at this, I know for

36
00:01:57.030 --> 00:02:01.030
each one of these projects, they're
going to have their own individual task.

37
00:02:01.030 --> 00:02:04.640
Right, and then each one of
these tasks are going to have

38
00:02:04.640 --> 00:02:07.100
the same project that resolved
it in the first place.

39
00:02:07.100 --> 00:02:09.900
So that means we already
created the database again for

40
00:02:09.900 --> 00:02:11.880
something that we already
previously did up here.

41
00:02:11.880 --> 00:02:12.664
So that's inefficient.

42
00:02:12.664 --> 00:02:15.290
And then two,
we did it again when we asked for

43
00:02:15.290 --> 00:02:17.800
the same project that
we already queried for.

44
00:02:17.800 --> 00:02:22.520
What your tasks are, we already did that
up here, so that's also very inefficient.

45
00:02:22.520 --> 00:02:27.880
So this is becoming exponential
every time we go down, right?

46
00:02:27.880 --> 00:02:30.890
So yes, there are ways to get around that,
you can short circuit it, but

47
00:02:30.890 --> 00:02:36.360
you can also make this faster
by caching the results or

48
00:02:36.360 --> 00:02:41.000
batching and caching the results
of what you already resolved.

49
00:02:41.000 --> 00:02:46.560
And that's where data load comes in, it's
going to batch the calls to your database.

50
00:02:46.560 --> 00:02:50.410
And then cache them so when you ask for
them in the same request on the same

51
00:02:50.410 --> 00:02:54.870
level, they'll just get them back to you
instantly because its cached in memory.

52
00:02:54.870 --> 00:02:57.920
And then that memory is not long lived.

53
00:02:57.920 --> 00:02:59.042
It's only per request, so

54
00:02:59.042 --> 00:03:03.240
therefore you're not sharing state across
multiple requests for different people.

55
00:03:03.240 --> 00:03:07.080
It would be dangerous if my request
came in and it cached my data and

56
00:03:07.080 --> 00:03:12.150
then you sent a request with the same
GraphQL query and you got my cached data.

57
00:03:12.150 --> 00:03:13.289
Right, that's dangerous.

58
00:03:13.289 --> 00:03:15.979
So it's only per request, and
that's what a data loader is for.

59
00:03:15.979 --> 00:03:20.229
You could think it was a per request,
in memory batching,

60
00:03:20.229 --> 00:03:22.760
caching layer for your database.

61
00:03:23.950 --> 00:03:25.270
Sound ridiculous, I know.

62
00:03:25.270 --> 00:03:27.960
But let's walk through.

