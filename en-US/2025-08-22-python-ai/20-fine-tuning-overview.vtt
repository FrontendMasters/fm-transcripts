WEBVTT

1
00:00:00.160 --> 00:00:02.206
&gt;&gt; Steve Kinney: What we're going to
talk about next is this idea of fine

2
00:00:02.206 --> 00:00:02.880
tuning, right?

3
00:00:02.880 --> 00:00:04.800
Which is again,
like right now we're going on.

4
00:00:04.800 --> 00:00:06.160
However this model was trained.

5
00:00:06.160 --> 00:00:10.000
Let's say, you know,
you always want a particular format.

6
00:00:10.000 --> 00:00:13.960
You know you want it to
stylistically do a thing.

7
00:00:13.960 --> 00:00:17.930
If you are just trying to get it to know
more content, you are probably better

8
00:00:17.930 --> 00:00:22.273
off using like, as we've been alluding to
that kind of like augmenting the prompts

9
00:00:22.273 --> 00:00:25.520
with the relevant data and
giving it to a giant model, right?

10
00:00:26.590 --> 00:00:31.262
The interesting game as we talk about
fine tuning that we'll go into is

11
00:00:31.262 --> 00:00:35.630
the bigger the model,
the more things you have to fine tune.

12
00:00:35.630 --> 00:00:38.270
That is expensive, right?

13
00:00:38.270 --> 00:00:39.566
So my heuristic for

14
00:00:39.566 --> 00:00:44.831
you is if you are trying to just have it
know about more things, you should just

15
00:00:44.831 --> 00:00:50.430
augment the prompts with the more things
that it should be working with, right?

16
00:00:50.430 --> 00:00:55.423
Whether you do that as you use chatgpt and
just paste in a bunch of stuff,

17
00:00:55.423 --> 00:00:59.752
or whether you use some kind of
vector database to augment it,

18
00:00:59.752 --> 00:01:03.914
if you want to change the style
in which it produces output,

19
00:01:03.914 --> 00:01:08.690
that is usually better for fine tuning,
can you do one or the other?

20
00:01:08.690 --> 00:01:13.850
Either way, of course, it's just like,
do you like your money or not?

21
00:01:13.850 --> 00:01:16.330
And what is more efficient
based on what you want to do?

22
00:01:16.330 --> 00:01:20.620
So if you want to add more context,
using a vector database where you get

23
00:01:20.620 --> 00:01:25.500
the embeddings and just jamming on that
context is probably faster and cheaper.

24
00:01:25.500 --> 00:01:29.162
And the better way to do it if you
would like to change the style,

25
00:01:29.162 --> 00:01:34.020
then you come to the world where doing the
fine tuning makes a little bit more sense.

26
00:01:35.620 --> 00:01:38.420
All right,
let's talk a little bit about fine tuning.

27
00:01:42.740 --> 00:01:47.940
We're gonna do metaphors because
mathematical formulas are boring.

28
00:01:50.030 --> 00:01:50.870
So fine tuning is.

29
00:01:50.870 --> 00:01:53.790
You're not necessarily creating
a new model from scratch.

30
00:01:53.790 --> 00:01:58.395
Remember, we said before, the way
that you train a model, effectively,

31
00:01:58.395 --> 00:02:04.190
figuratively, is you start out with
a bunch of stuff pointed in random places.

32
00:02:04.190 --> 00:02:06.830
You have the answer key already.

33
00:02:06.830 --> 00:02:09.630
You give it the first word,
you tell it to guess the second word.

34
00:02:09.630 --> 00:02:11.950
If it's right, you give it a cookie.

35
00:02:11.950 --> 00:02:14.470
If it's wrong, you don't.

36
00:02:14.470 --> 00:02:15.680
And it begins.

37
00:02:15.680 --> 00:02:19.570
It randomly tweaks the knobs until it
gets good enough that it has somehow

38
00:02:19.570 --> 00:02:23.920
tweaked the knobs to all the right places,
where it is mostly right all the time.

39
00:02:25.680 --> 00:02:26.560
That's expensive.

40
00:02:27.840 --> 00:02:28.920
That takes a lot of time.

41
00:02:28.920 --> 00:02:33.032
And GPU and electricity and in this
current day and age, fossil fuels and

42
00:02:33.032 --> 00:02:34.960
all those fun things.

43
00:02:34.960 --> 00:02:40.229
What's probably better in A lot of
cases is take a model that already

44
00:02:40.229 --> 00:02:47.040
knows most things and just pick up from
there and feed it more stuff, right?

45
00:02:47.040 --> 00:02:48.880
So imagine you speak English.

46
00:02:48.880 --> 00:02:50.720
You want to learn about AI.

47
00:02:52.080 --> 00:02:54.320
At first,
Steve said a bunch of words to you.

48
00:02:54.320 --> 00:02:56.160
You're like,
I don't know what any of those mean.

49
00:02:56.160 --> 00:02:59.018
And hopefully if he keeps saying them at
you, you not only still know English,

50
00:02:59.018 --> 00:03:01.920
which is a good starting, because if you
didn't know English when you walked in

51
00:03:01.920 --> 00:03:04.640
here, everything I've said has probably
not worked out super well for.

52
00:03:04.640 --> 00:03:05.140
You.

53
00:03:06.870 --> 00:03:09.911
But you ideally can over
time kind of pick up from

54
00:03:09.911 --> 00:03:14.030
all of the stuff you already know and
just add more stuff onto it.

55
00:03:14.030 --> 00:03:18.584
You just need to learn what a causal
attention mask is or a vector database or

56
00:03:18.584 --> 00:03:21.476
an embedding and
what that means in this case and

57
00:03:21.476 --> 00:03:25.190
add it into the fact that you
already know what a computer is.

58
00:03:26.310 --> 00:03:29.830
That's effectively what we're
doing with this fine tuning.

59
00:03:31.510 --> 00:03:35.190
Because the model already knows
general how sentences work.

60
00:03:35.190 --> 00:03:37.510
You don't want to teach
a model how sentences work.

61
00:03:37.510 --> 00:03:38.470
I mean, maybe you do.

62
00:03:38.470 --> 00:03:42.630
If you do, OpenAI will happily hire you.

63
00:03:42.630 --> 00:03:44.469
But generally speaking,
for the practical thing,

64
00:03:44.469 --> 00:03:46.990
you're probably going to sit down
to tomorrow starting from scratch.

65
00:03:46.990 --> 00:03:49.510
I mean, they don't even want to
start from scratch tomorrow, right?

66
00:03:49.510 --> 00:03:52.510
They have some models that know
the structure of language.

67
00:03:52.510 --> 00:03:53.190
It's fine.

68
00:03:54.390 --> 00:03:58.807
But if you wanted to show it a specific
set of things to kind of build on top of

69
00:03:58.807 --> 00:04:03.512
standing on the shoulder of giants,
you could do something like fine tuning,

70
00:04:03.512 --> 00:04:04.030
right?

71
00:04:04.030 --> 00:04:06.350
Be like, this is the structure of this.

72
00:04:06.350 --> 00:04:08.750
This is the shape of what
a response should be.

73
00:04:08.750 --> 00:04:10.230
This is kind of like more.

74
00:04:10.230 --> 00:04:12.625
And again,
it's more about the structure and

75
00:04:12.625 --> 00:04:16.681
style than it is about the content,
because they're easier, cheaper ways to

76
00:04:16.681 --> 00:04:20.750
just literally shove the content into
the context window programmatically.

77
00:04:22.030 --> 00:04:26.070
But this will allow you
to basically turn those

78
00:04:26.070 --> 00:04:30.220
knobs on top of the existing
turns of the knobs.

79
00:04:32.060 --> 00:04:32.869
Yeah, and so

80
00:04:32.869 --> 00:04:37.660
it's again, learning new specialty
without having to relearn how to read.

81
00:04:40.380 --> 00:04:45.660
Normal fine tuning, again, the bigger
the model, the harder it is, right?

82
00:04:46.700 --> 00:04:48.780
So normal fine tuning is.

83
00:04:48.780 --> 00:04:53.980
You are literally tweaking every knob,
right?

84
00:04:53.980 --> 00:04:58.806
And obviously on a small like GPT2,
that's still expensive and

85
00:04:58.806 --> 00:05:03.455
you can only imagine what it is like,
like the numbers of like,

86
00:05:03.455 --> 00:05:09.270
even the initial training of like GPT4
were like what, 50, like million?

87
00:05:10.310 --> 00:05:10.910
I don't even.

88
00:05:10.910 --> 00:05:14.550
A big number a big money that
I don't have personally.

89
00:05:16.310 --> 00:05:17.670
And so.

90
00:05:17.670 --> 00:05:21.560
But even doing it on
a regular model isn't great.

91
00:05:21.560 --> 00:05:25.240
So step one, if you decide that I
want to go down the fine tuning road.

92
00:05:25.240 --> 00:05:27.080
You should probably do a small model.

93
00:05:27.080 --> 00:05:30.000
But I'm going to argue you shouldn't
fine tune an entire model.

94
00:05:30.000 --> 00:05:33.047
Anyway, when I say I'm going to argue,

95
00:05:33.047 --> 00:05:37.720
I'm going to say research says hence,
thereby I will argue.

96
00:05:39.880 --> 00:05:41.119
And so with fine tuning,

97
00:05:41.119 --> 00:05:44.280
you are effectively going
through the entire textbook.

98
00:05:44.280 --> 00:05:47.619
The textbook is there,
but you are going and

99
00:05:47.619 --> 00:05:52.090
effectively updating the entire textbook,
right?

100
00:05:52.090 --> 00:05:54.810
And depending on the size of the book,
could be expensive.

101
00:05:56.890 --> 00:06:01.770
Then came this idea of what if instead
of rewriting the entire textbook,

102
00:06:01.770 --> 00:06:05.450
we just added like another
chapter at the end, right?

103
00:06:05.450 --> 00:06:07.290
Like an addendum, right?

104
00:06:07.290 --> 00:06:10.010
Like a post credit scene, if you will.

105
00:06:11.130 --> 00:06:15.807
What if we just added a little
bit of layer on top of it and

106
00:06:15.807 --> 00:06:17.963
tried that out instead?

107
00:06:17.963 --> 00:06:22.600
And that is what we call
parameter efficient fine tuning.

