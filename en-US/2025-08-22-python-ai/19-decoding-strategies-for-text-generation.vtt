WEBVTT

1
00:00:00.640 --> 00:00:03.792
&gt;&gt; Steve Kinney: So in this example,
what I'm going to do is honestly just

2
00:00:03.792 --> 00:00:07.680
going to create five rando tokens
because it doesn't really matter.

3
00:00:07.680 --> 00:00:13.893
And we're going to see like how it's going
to create a little chart for me that is

4
00:00:13.893 --> 00:00:20.122
going to show how it will think about the
five tokens as it processes each token.

5
00:00:20.122 --> 00:00:24.000
Which is going to go about how you expect,
right?

6
00:00:24.000 --> 00:00:28.872
Which is when it is on token one, how many
tokens do you think the decoder how many

7
00:00:28.872 --> 00:00:31.730
tokens do you think it's
going to care about?

8
00:00:33.410 --> 00:00:36.370
The answer showed up, right?

9
00:00:37.410 --> 00:00:42.273
So at this point, which is random tokens
that don't mean anything with a causal

10
00:00:42.273 --> 00:00:46.630
attention mask where we basically
zero out all future tokens, right,

11
00:00:46.630 --> 00:00:47.735
as we saw before.

12
00:00:47.735 --> 00:00:51.912
At the first token is processed,
it only cares about the first token.

13
00:00:51.912 --> 00:00:56.290
And it's only considering the first
token in that understanding, right?

14
00:00:56.290 --> 00:00:57.730
Process the first token.

15
00:00:57.730 --> 00:01:03.090
Now it only and it basically just
nerfs any other tokens out of control.

16
00:01:03.090 --> 00:01:04.370
It does not care.

17
00:01:04.370 --> 00:01:06.392
So at first,
we only care about the first token.

18
00:01:06.392 --> 00:01:09.299
And then when we try to come up with,
we come up with a second token.

19
00:01:09.299 --> 00:01:12.290
We try to come up with a third token,
we only care about the first two.

20
00:01:12.290 --> 00:01:14.210
Bert will look in both directions.

21
00:01:14.210 --> 00:01:18.170
A GPT is only going to look backwards
to figure out how to predict,

22
00:01:18.170 --> 00:01:22.058
but then it will use all the ones
that are predicted previously to

23
00:01:22.058 --> 00:01:25.170
inform the next one along
with all of your prompt.

24
00:01:25.170 --> 00:01:29.300
When we think about, I said this earlier,
when we think about like ChatGPT or

25
00:01:29.300 --> 00:01:32.810
Claude, we think about I ask question,
it gives me answer.

26
00:01:32.810 --> 00:01:34.290
It must be like question answering.

27
00:01:34.290 --> 00:01:38.746
No, it's basically taking your entire
text, drawing a line of demarcation,

28
00:01:38.746 --> 00:01:41.339
looking back at it,
guessing the first word,

29
00:01:41.339 --> 00:01:43.610
it should say guessing the second word.

30
00:01:43.610 --> 00:01:46.257
And you see those
reasoning models like O3 or

31
00:01:46.257 --> 00:01:50.367
doing thinking in Claude where they
will spend a certain amount adding

32
00:01:50.367 --> 00:01:55.120
context themselves before they spit out
the answer, right, reflecting on it.

33
00:01:55.120 --> 00:02:00.440
But that's all just text generation
getting put onto the stack, effectively.

34
00:02:01.560 --> 00:02:07.440
Then all the thinking that it did, right,
is getting stuck on there as well.

35
00:02:07.440 --> 00:02:11.210
And so that's also it's using all that
thinking to predict what the next word

36
00:02:11.210 --> 00:02:12.840
should be, right?

37
00:02:12.840 --> 00:02:15.401
And so
that's just a level of sophistication.

38
00:02:15.401 --> 00:02:19.395
Because all it ever works on is,
is all of the tokens up until now,

39
00:02:19.395 --> 00:02:21.150
what should the next one be?

40
00:02:25.230 --> 00:02:28.290
Now we've got GPT medium.

41
00:02:28.290 --> 00:02:35.390
Pull that down,
you can see there's that vocab JSON again.

42
00:02:36.430 --> 00:02:40.175
There's also of a given one you
noticed before we were using GPT2,

43
00:02:40.175 --> 00:02:41.630
now we grab GPT2 medium.

44
00:02:41.630 --> 00:02:43.640
So there are also sizes.

45
00:02:43.640 --> 00:02:46.599
If you go grab the open source LLMs for

46
00:02:46.599 --> 00:02:50.703
LLAMA 3 there is an 8
billion parameter version,

47
00:02:50.703 --> 00:02:57.400
a 14 billion parameter version,
a 27 billion parameter version.

48
00:02:57.400 --> 00:02:59.634
40, don't quote me on this.

49
00:02:59.634 --> 00:03:03.280
105, I can't run on my machine,
so on and so forth.

50
00:03:03.280 --> 00:03:06.885
So you can get not only different
models in terms of generation,

51
00:03:06.885 --> 00:03:10.110
you can obviously get different
sizes of models as well.

52
00:03:11.790 --> 00:03:12.750
Cool, cool, cool.

53
00:03:14.190 --> 00:03:15.870
&gt;&gt; Male Student 1: Have a question
just about running models.

54
00:03:15.870 --> 00:03:18.110
So we've been talking about
different parameter sizes.

55
00:03:18.110 --> 00:03:20.770
Is this confined to
actual physical memory or

56
00:03:20.770 --> 00:03:23.710
can it do some swapping
on faster virtual memory?

57
00:03:25.150 --> 00:03:26.620
&gt;&gt; Steve Kinney: I think for most things,

58
00:03:26.620 --> 00:03:30.272
it's like you can swap something
between RAM and GPU memory.

59
00:03:30.272 --> 00:03:34.090
But I think for most, it's got to
be loaded in some sort of memory.

60
00:03:34.090 --> 00:03:36.266
Yeah, it's tricky because on a Mac,

61
00:03:36.266 --> 00:03:39.230
I don't even have two kinds of memory,
right?

62
00:03:39.230 --> 00:03:41.486
&gt;&gt; Male Student 1: I'm just thinking
it fakes, even if it's faking it and

63
00:03:41.486 --> 00:03:42.990
writing it to disk, [CROSSTALK].

64
00:03:42.990 --> 00:03:47.870
&gt;&gt; Steve Kinney: Yeah, I don't think
it can like page disk at all, right?

65
00:03:47.870 --> 00:03:50.870
I think the entire model
has to be even more gets.

66
00:03:50.870 --> 00:03:51.750
I don't, yeah.

67
00:03:51.750 --> 00:03:55.284
The tricky part is when I've
run these things on my machine,

68
00:03:55.284 --> 00:03:58.070
a Mac does not separate
GPU RAM from system RAM.

69
00:03:58.070 --> 00:04:02.270
So I always like forget in this case,
right, as well.

70
00:04:02.270 --> 00:04:04.630
But where are we going with this?

71
00:04:09.800 --> 00:04:13.400
&gt;&gt; Steve Kinney: All right,
so we've pulled in pad token.

72
00:04:17.480 --> 00:04:19.080
We've pulled in our model at this point.

73
00:04:20.600 --> 00:04:23.075
So there's a bunch of decoding strategies.

74
00:04:23.075 --> 00:04:27.939
And this is where we can start to play
with, I don't need all of this right here.

75
00:04:27.939 --> 00:04:30.560
We've got the greedy decoding strategy.

76
00:04:30.560 --> 00:04:32.400
Just pick the single
most likely less token.

77
00:04:32.400 --> 00:04:33.540
It's fast and predictable.

78
00:04:33.540 --> 00:04:36.170
We but we will get mostly
the same thing every time.

79
00:04:37.370 --> 00:04:40.330
The other thing we can do is start to
play with that temperature, right?

80
00:04:40.330 --> 00:04:42.890
Let's actually run all of these and
just see some stuff in here.

81
00:04:45.530 --> 00:04:47.130
We'll let all that go for a second.

82
00:04:49.050 --> 00:04:52.170
So here I'm having it run multiple times.

83
00:04:52.170 --> 00:04:53.170
We saw this at the beginning.

84
00:04:53.170 --> 00:04:56.330
This is a temperature of roughly,
at this point, zero.

85
00:04:56.330 --> 00:05:00.143
It's just using that
greedy immediate next one.

86
00:05:00.143 --> 00:05:02.277
I have a max token limit of 20.

87
00:05:02.277 --> 00:05:06.490
And so it did stop at one point,
if we bump that up to maybe 22,

88
00:05:06.490 --> 00:05:08.649
given this particular answer.

89
00:05:18.090 --> 00:05:18.970
&gt;&gt; Steve Kinney: Give that a second.

90
00:05:20.730 --> 00:05:24.410
We will likely see two of the exact
same sentences when it comes out.

91
00:05:26.750 --> 00:05:30.110
Yeah, right, and so
there is no guessing at this point.

92
00:05:30.110 --> 00:05:34.958
It literally will begin to like, you will
always get the same thing every time

93
00:05:34.958 --> 00:05:38.190
because it is picking
the most likely next word.

94
00:05:38.190 --> 00:05:41.710
Now, you can have some questions
on why those are the next words.

95
00:05:41.710 --> 00:05:43.470
I cannot answer that one for you.

96
00:05:43.470 --> 00:05:46.927
But statistically speaking,
that according to GPT2,

97
00:05:46.927 --> 00:05:49.310
did anyone look up when GPT2 dropped?

98
00:05:51.390 --> 00:05:55.330
It might have been like happier times,
yeah.

99
00:05:58.370 --> 00:06:00.490
You will get effectively the same output.

100
00:06:00.490 --> 00:06:02.050
And so then we have that temperature.

101
00:06:02.050 --> 00:06:05.766
We accidentally saw this a little bit
earlier as well when we played around with

102
00:06:05.766 --> 00:06:08.090
it, where we will get
slightly different ones.

103
00:06:08.090 --> 00:06:09.570
But this graphs it out for you.

104
00:06:11.090 --> 00:06:16.290
So for all of the words,
what was my initial beginning one?

105
00:06:16.290 --> 00:06:17.330
What did I write here?

106
00:06:18.540 --> 00:06:19.860
A lot of that's the graphing stuff.

107
00:06:19.860 --> 00:06:23.924
I think it's that, yes,
the artificial intelligence is.

108
00:06:23.924 --> 00:06:26.984
So a lot of this code,
if you're like, what is all of this,

109
00:06:26.984 --> 00:06:28.780
it's drawing the pretty graphs.

110
00:06:29.980 --> 00:06:30.980
Don't worry about the code.

111
00:06:30.980 --> 00:06:32.380
It's just drawing the pretty graphs.

112
00:06:34.940 --> 00:06:42.677
So you can see for each temperature
level what it thinks the next,

113
00:06:42.677 --> 00:06:47.890
what his choices of
the next word should be.

114
00:06:47.890 --> 00:06:52.551
At a very low temperature,
it doesn't have a lot of options, right,

115
00:06:52.551 --> 00:06:54.610
and a lot of creativity there.

116
00:06:55.810 --> 00:07:01.220
As the temperature goes up, the amount
of words it's willing to consider and

117
00:07:01.220 --> 00:07:04.810
sample about to be the next word,
right, goes up.

118
00:07:04.810 --> 00:07:08.930
And so that's where that creativity comes
from versus always getting the same thing.

119
00:07:08.930 --> 00:07:13.930
The lower you turn that knob, the almost
like turns the overall volume down.

120
00:07:13.930 --> 00:07:17.610
So you've gotta be really
loud to get through.

121
00:07:17.610 --> 00:07:22.890
And as you turn the temperature up,
you can get more noise, right?

122
00:07:22.890 --> 00:07:25.159
And so the higher the temperature,

123
00:07:25.159 --> 00:07:28.944
the more words are statistically
available for it to pick.

124
00:07:28.944 --> 00:07:34.170
Which will either get you more creative
answers or complete garbage, right?

125
00:07:34.170 --> 00:07:37.370
And that's why it's a dial, right,
that you can kind of set as well.

126
00:07:39.180 --> 00:07:43.180
By default, you usually see 0.5.

127
00:07:43.180 --> 00:07:47.943
If we did it at 0.7, you get something
a little bit better than when we saw

128
00:07:47.943 --> 00:07:50.300
originally at the lowest setting.

129
00:07:50.300 --> 00:07:54.140
The future of artificial intelligence
is a wider, more complicated topic.

130
00:07:54.140 --> 00:07:57.449
What kind of services and capabilities
will be needed to support the growing

131
00:07:57.449 --> 00:07:59.420
demands of AI in many
areas of everyday life?

132
00:07:59.420 --> 00:08:00.540
Yeah, it's GPT2.

133
00:08:00.540 --> 00:08:03.844
So it still reads those tests
that you took in third grade,

134
00:08:03.844 --> 00:08:06.780
the reading comprehension
standardized test.

135
00:08:06.780 --> 00:08:11.554
It's not necessarily an interesting read,
but you can kind of see

136
00:08:11.554 --> 00:08:16.340
with the graphing how its choices
of the next word tends to come up.

137
00:08:17.380 --> 00:08:19.940
And then we have that top K and
top P Nucleus name.

138
00:08:19.940 --> 00:08:20.536
And again,

139
00:08:20.536 --> 00:08:24.058
I will remind you that these are all
notebooks where you can go in here.

140
00:08:24.058 --> 00:08:26.452
You want to change that
to a different number.

141
00:08:26.452 --> 00:08:28.014
You want to turn on the top K.

142
00:08:28.014 --> 00:08:30.451
You can go ahead and
try out all of these things and

143
00:08:30.451 --> 00:08:33.319
you can kind of see how
the differences will show back up.

144
00:08:37.940 --> 00:08:40.439
&gt;&gt; Steve Kinney: The fun part is you never
really know what you're gonna get with

145
00:08:40.439 --> 00:08:41.060
these things.

146
00:08:42.260 --> 00:08:45.540
The future of artificial
intelligence is like a trap.

147
00:08:45.540 --> 00:08:47.780
Its greatest threat isn't foreseeable.

148
00:08:47.780 --> 00:08:51.140
It's between you and, it says Jon Goodman.

149
00:08:53.300 --> 00:08:55.965
I think John Goodman,
the actor has an H in his name, though.

150
00:08:55.965 --> 00:08:58.500
This might be like it's not
totally wrong on that one.

151
00:08:59.620 --> 00:09:03.345
But, like, one of the things that jumps
out at me that'd be interesting to do is

152
00:09:03.345 --> 00:09:07.180
you then take that output and then throw
it in that, like, named entity recogn and

153
00:09:07.180 --> 00:09:08.560
figure out what comes out.

154
00:09:08.560 --> 00:09:11.440
Always the combination of things
becomes the most interesting.

155
00:09:12.720 --> 00:09:19.080
But yeah, we could also, what would happen
if we just turned this down to 0.1, right?

156
00:09:19.080 --> 00:09:20.640
We don't give it as many things.

157
00:09:20.640 --> 00:09:25.546
I wonder if we end up something very close
to what we saw in the very beginning,

158
00:09:25.546 --> 00:09:26.080
right?

159
00:09:31.280 --> 00:09:34.560
That's very much like Bart Simpson
drawing on the board, right?

160
00:09:34.560 --> 00:09:35.141
But again,

161
00:09:35.141 --> 00:09:39.160
it's just where we turn these knobs to get
the outputs we necessarily want to get.

162
00:09:39.160 --> 00:09:42.769
Like, what happens if we turn this to,
like,

163
00:09:42.769 --> 00:09:47.320
I don't know,
it was like 0.92 originally, right?

164
00:09:51.560 --> 00:09:55.708
And then we'll turn that down and
we'll turn up the top K to kind of like,

165
00:09:55.708 --> 00:09:57.890
view the difference.

166
00:09:57.890 --> 00:09:59.051
Now we're a lot more positive.

167
00:10:05.730 --> 00:10:09.278
&gt;&gt; Steve Kinney: And it's interesting once
you kind of know that every previous word

168
00:10:09.278 --> 00:10:10.850
predicts the next one.

169
00:10:10.850 --> 00:10:14.666
That is almost what makes it beautiful,
which is it will continue on whatever

170
00:10:14.666 --> 00:10:17.730
weird tangent it decided to go
on to a certain extent, right?

171
00:10:17.730 --> 00:10:22.690
It knows it's made up, possibly a real
person, possibly at a real company.

172
00:10:25.490 --> 00:10:29.890
And it will continue with that because
each word informs the next word.

173
00:10:29.890 --> 00:10:33.004
Right, and you do have a corpus of
previous words that it will at least more

174
00:10:33.004 --> 00:10:36.271
likely to stay on track until it starts
blowing through its context window and

175
00:10:36.271 --> 00:10:38.510
forgets the original words.

176
00:10:38.510 --> 00:10:41.648
Because if it gets too long, it will start
dropping the packets at the beginning, and

177
00:10:41.648 --> 00:10:43.470
that's when the hallucinations happen.

178
00:10:43.470 --> 00:10:46.620
But like most lies,
as soon as you start down the path,

179
00:10:46.620 --> 00:10:49.704
you're pretty good at lying
until you've lied for so

180
00:10:49.704 --> 00:10:54.190
long you forgot the original basis,
and that's when your lie falls apart.

181
00:10:54.190 --> 00:10:58.110
The same is true with statistics and AI.

182
00:10:58.110 --> 00:11:00.830
Let's go turn that one back down to zero.

183
00:11:00.830 --> 00:11:05.633
And what happens if we,
I think that one ends up

184
00:11:05.633 --> 00:11:09.956
being an integer, if I'm not mistaken.

185
00:11:09.956 --> 00:11:13.018
I might get yelled at, we'll see.

186
00:11:13.018 --> 00:11:16.736
This is what happens when you
load a JavaScript engine.

187
00:11:16.736 --> 00:11:20.000
Yeah, no,
that one was pretty boring again.

188
00:11:20.000 --> 00:11:22.888
Let's see, only give it a top K this time,

189
00:11:22.888 --> 00:11:27.800
because I think maybe the top P was
also playing around with that as well.

190
00:11:30.886 --> 00:11:36.419
&gt;&gt; Steve Kinney: That's gonna be,
that's a bumper

191
00:11:36.419 --> 00:11:41.250
sticker waiting to happen.

192
00:11:41.250 --> 00:11:43.850
All right,
I think people should be smarter.

193
00:11:43.850 --> 00:11:45.170
They should be smarter than us.

194
00:11:47.650 --> 00:11:48.490
Here's your challenge.

195
00:11:48.490 --> 00:11:49.250
You wanna make money?

196
00:11:49.250 --> 00:11:50.370
This is how you do it.

197
00:11:50.370 --> 00:11:54.888
You just get the lightest weight model,
you turn the knobs all the way up.

198
00:11:54.888 --> 00:11:58.430
And then you just print Instagram ads for
T shirts, right?

199
00:12:00.350 --> 00:12:04.208
Yeah, I think low quality models
plus some free time could

200
00:12:04.208 --> 00:12:08.550
definitely make you a T shirt company or
bumper sticker company.

201
00:12:08.550 --> 00:12:12.994
If anyone would like a free how
to get rich from this workshop,

202
00:12:12.994 --> 00:12:17.870
I think AI generated meme stuff
that just makes literally no sense.

203
00:12:22.590 --> 00:12:26.696
Yeah, so this is taking any
of the top 60 most likely

204
00:12:26.696 --> 00:12:30.345
words after the previous ones and
going for it,

205
00:12:30.345 --> 00:12:36.200
a research outfit which wants to make
us better than our ancestors, right?

206
00:12:36.200 --> 00:12:38.929
Because with the top K,
all I think of the first of

207
00:12:38.929 --> 00:12:43.840
the 60 most likely words are fair game,
and it will sample based on those.

208
00:12:43.840 --> 00:12:46.480
So, yeah, top P better in a lot of cases.

209
00:12:46.480 --> 00:12:51.193
We're talking about how Gemini 2.5,
and Claude 4 Opus,

210
00:12:51.193 --> 00:12:55.500
and 03 have 2 million
tokens of context window.

211
00:12:55.500 --> 00:13:00.300
Our good friend GPT2 over here has
a context window of 1024, right?

212
00:13:03.420 --> 00:13:08.080
And so it will start losing things off the
beginning of the stack if it goes on for

213
00:13:08.080 --> 00:13:08.720
too long.

214
00:13:08.720 --> 00:13:12.439
Which point,
that chain of thought of making stuff up,

215
00:13:12.439 --> 00:13:15.058
it will lose the original seed of that.

216
00:13:15.058 --> 00:13:18.886
Which could end poorly for
you, it might not.

217
00:13:18.886 --> 00:13:22.550
Yeah, like, and then like we said before,

218
00:13:22.550 --> 00:13:27.260
some of those special tokens
come into play, right?

219
00:13:27.260 --> 00:13:33.260
And so, in some way,
it's guessing another word, right?

220
00:13:33.260 --> 00:13:38.578
It can plausibly guess in a lot of cases,
I should probably stop talking now.

221
00:13:38.578 --> 00:13:41.340
Which is a thing, it took me decades.

222
00:13:41.340 --> 00:13:46.220
And my wife would argue one day,
maybe, I'm trying to learn myself.

223
00:13:46.220 --> 00:13:48.187
Which is this idea of like,

224
00:13:48.187 --> 00:13:53.335
it could have an end of sequence token
where you can actually say, hey.

225
00:13:53.335 --> 00:13:57.080
Because again,
the tools don't always know to stop.

226
00:13:57.080 --> 00:13:58.920
So you have to say, okay.

227
00:13:58.920 --> 00:14:03.755
An argument that you can give to this
model in the pipeline right now is,

228
00:14:03.755 --> 00:14:08.600
hey, this particular tokenizer for
whatever model you're using.

229
00:14:08.600 --> 00:14:12.045
This is,
I think if you hover over it, yeah,

230
00:14:12.045 --> 00:14:17.049
the end of sequence token for,
am I using GPT2 medium, I think?

231
00:14:17.049 --> 00:14:21.580
The ID of that token is 50256.

232
00:14:23.020 --> 00:14:26.200
So when you come across that token,

233
00:14:26.200 --> 00:14:32.780
even if we haven't hit the max tokens,
stop talking, right?

234
00:14:32.780 --> 00:14:34.460
And that's a valid token as well.

235
00:14:34.460 --> 00:14:37.705
So then we, hopefully,

236
00:14:37.705 --> 00:14:42.500
and again it becomes a game, right?

237
00:14:42.500 --> 00:14:44.380
We're going to say your
max new tokens are 100.

238
00:14:52.090 --> 00:14:53.370
Like, it might, yeah.

239
00:14:53.370 --> 00:14:56.030
If we didn't come across one
of the like max new tokens,

240
00:14:56.030 --> 00:14:57.450
we can jump this up to 200.

241
00:14:57.450 --> 00:15:01.580
Hopefully, what you would find is
that it will, like if it hits the max

242
00:15:01.580 --> 00:15:06.130
first before it hits an end of sequence
token, it doesn't know like to stop.

243
00:15:06.130 --> 00:15:08.294
But like hopefully if
you give it enough room,

244
00:15:08.294 --> 00:15:10.870
it will eventually come
across one of those.

245
00:15:10.870 --> 00:15:13.110
We could also change the prompt or
something to a little shorter.

246
00:15:15.030 --> 00:15:19.910
But like I said,
these are all to be played with as well.

247
00:15:22.630 --> 00:15:25.024
But my patience for letting it go
is going to run out real fast.

248
00:15:36.720 --> 00:15:41.616
&gt;&gt; Steve Kinney: We got very,
&gt;&gt; Steve Kinney: We got

249
00:15:41.616 --> 00:15:46.080
duplicate up there as well.

250
00:15:46.080 --> 00:15:52.319
But do I have a, also might have
a memory issue I gotta look at too.

251
00:15:52.319 --> 00:15:54.715
Like we have the things overriding.

252
00:15:54.715 --> 00:15:57.585
But yeah, anyway,
you can define the end token.

253
00:15:57.585 --> 00:16:00.550
And hopefully,
you will kind of get a sense of,

254
00:16:00.550 --> 00:16:03.080
it will figure out how to do that as well.

255
00:16:03.080 --> 00:16:06.578
Now we talked about before we saw
the generative is making stuff up and

256
00:16:06.578 --> 00:16:09.120
the extractive will actually
find it in the text.

257
00:16:09.120 --> 00:16:11.800
Right, those are two very different bases.

