WEBVTT

1
00:00:00.208 --> 00:00:02.761
&gt;&gt; Steve Kinney: So
we talked about that with text.

2
00:00:02.761 --> 00:00:05.360
We had the Transformers library.

3
00:00:05.360 --> 00:00:06.000
Great library.

4
00:00:06.000 --> 00:00:08.640
We did all this stuff with
the Transformers library.

5
00:00:08.640 --> 00:00:11.520
And, yeah, I made some charts at
one point with a different library.

6
00:00:11.520 --> 00:00:16.258
And that Bertviz thing,
which was kind of cool but confusing.

7
00:00:16.258 --> 00:00:22.281
That was neat, but that was all
from the Transformers library.

8
00:00:22.281 --> 00:00:30.343
Hugging Face has another kind of
similar library called Diffusers,

9
00:00:30.343 --> 00:00:34.652
and they help us use a different model

10
00:00:34.652 --> 00:00:39.517
architecture called stable diffusion,

11
00:00:39.517 --> 00:00:44.807
in which we turn chaotic
noise into Moo Deng.

12
00:00:44.807 --> 00:00:47.438
I don't care if you remember
what a transformer block is.

13
00:00:47.438 --> 00:00:52.341
Honestly, whatever,
if you're not aware what Moo Deng is,

14
00:00:52.341 --> 00:00:58.396
you owe it to yourself to go see
a delightful, ungovernable, pygmy hippo.

15
00:00:58.396 --> 00:01:04.245
And again, if you imagine the cats walking
across Collab in the very beginning and

16
00:01:04.245 --> 00:01:07.890
me generating pictures of
baby hippo on a weekend,

17
00:01:07.890 --> 00:01:12.747
you can understand why my wife was
not convinced that I was working.

18
00:01:12.747 --> 00:01:15.245
But I used it to get out of lots of stuff,
so it's fine.

19
00:01:15.245 --> 00:01:18.834
And so, effectively, there's like,
is it a Michelangelo quote?

20
00:01:18.834 --> 00:01:21.050
There's a lot of those quotes where
we think they're quoted to somebody.

21
00:01:21.050 --> 00:01:25.042
You find out half the Oscar Wilde
quotes are misattributed.

22
00:01:25.042 --> 00:01:30.037
I long to be somebody when I'm long dead
to have tons of quotes that I didn't say

23
00:01:30.037 --> 00:01:31.255
attributed to me.

24
00:01:31.255 --> 00:01:34.970
But I think the Michelangelo thing is,
how do you make a sculpture?

25
00:01:34.970 --> 00:01:38.890
You chip away all the parts of the rock
that are not the David or what have you.

26
00:01:40.010 --> 00:01:44.874
And that is effectively,
again, if Transformers and

27
00:01:44.874 --> 00:01:48.904
generative AI is simply
guess the next word.

28
00:01:48.904 --> 00:01:50.157
And all this stuff seems like magic.

29
00:01:50.157 --> 00:01:52.241
It's just a statistical
guessing statistic.

30
00:01:52.241 --> 00:01:57.033
Stable diffusion is,
you start out with a chaotic

31
00:01:57.033 --> 00:02:02.167
randomness of just TV static,
but with more colors,

32
00:02:02.167 --> 00:02:09.257
and you keep taking away all the parts
that aren't what you wanted to see.

33
00:02:09.257 --> 00:02:13.731
So when we see all of those
crazy AI images or whatever,

34
00:02:13.731 --> 00:02:19.000
all that's really happening is
you start with absolute chaos.

35
00:02:19.000 --> 00:02:25.653
And then each time, you try to get
a little bit closer to a hippo, right?

36
00:02:25.653 --> 00:02:29.118
Or to a cat, cuz we all know
the Internet is powered by cats.

37
00:02:29.118 --> 00:02:33.766
Right, it has a bunch of real images,
right,

38
00:02:33.766 --> 00:02:37.353
that are labeled like this is a cat.

39
00:02:37.353 --> 00:02:38.212
This is a hippo.

40
00:02:38.212 --> 00:02:41.063
This is a whatever.

41
00:02:41.063 --> 00:02:46.779
And each time it goes,
that's still not a hippo, all right?

42
00:02:46.779 --> 00:02:48.235
Still not a hippo, all right?

43
00:02:48.235 --> 00:02:49.180
Keep going.

44
00:02:49.180 --> 00:02:52.740
Keep doing stuff until we
think we have a hippo, right?

45
00:02:52.740 --> 00:02:55.730
And maybe with the weaker models,

46
00:02:55.730 --> 00:03:01.260
you end up with a hand with
19 fingers on it, right?

47
00:03:01.260 --> 00:03:02.860
And that's how you end up with that.

48
00:03:02.860 --> 00:03:05.340
Cause it's like everyone's like,
well, it's so wrong.

49
00:03:05.340 --> 00:03:06.375
I'm like, yeah,

50
00:03:06.375 --> 00:03:10.066
if you think about the fact that it
just started with rando pixels and

51
00:03:10.066 --> 00:03:14.620
just got its way there, honestly, a six
fingered hand is pretty impressive to me.

52
00:03:16.080 --> 00:03:16.990
And to be clear,

53
00:03:16.990 --> 00:03:20.772
again, we are gonna try to keep this
on the cheapest hardware we can.

54
00:03:20.772 --> 00:03:25.759
So prepare to be underwhelmed,
[LAUGH], right?

55
00:03:25.759 --> 00:03:28.440
And with small models
that will render quickly.

56
00:03:28.440 --> 00:03:33.291
If I had the time to do the thing I did in
high school when I worked on VHS videos

57
00:03:33.291 --> 00:03:37.911
where I would just choose the most
ridiculous render and then go hang out

58
00:03:37.911 --> 00:03:42.149
in the cafeteria in my video editing
class so I didn't have to go.

59
00:03:42.149 --> 00:03:42.752
And it doesn't matter.

60
00:03:42.752 --> 00:03:46.172
But if we're trying to do this quickly,
we're gonna do small models,

61
00:03:46.172 --> 00:03:49.140
we're gonna do on the cheap GPUs and
stuff like that.

62
00:03:49.140 --> 00:03:50.260
Still kinda cool.

63
00:03:51.380 --> 00:03:53.700
So we take the random static.

64
00:03:53.700 --> 00:03:55.380
It knows what real images look like.

65
00:03:55.380 --> 00:03:58.397
And every time, every pass, step by step,

66
00:03:58.397 --> 00:04:03.140
it tries to get you one iteration
closer to a cat wearing sunglasses.

67
00:04:05.780 --> 00:04:09.614
There's a few pieces to this
one we already know and

68
00:04:09.614 --> 00:04:13.271
maybe love unclear,
which is the Text Encoder,

69
00:04:13.271 --> 00:04:17.390
which is usually you can do
image to image too, right?

70
00:04:18.430 --> 00:04:24.270
But usually it is take the string of text,
turn it into an image, right?

71
00:04:24.270 --> 00:04:28.270
So the part where it figures out what
the text means is the same as before.

72
00:04:29.470 --> 00:04:34.590
Then there's a few terms they will appear,
so they're worth explaining.

73
00:04:34.590 --> 00:04:36.190
They are not worth stressing out over.

74
00:04:37.380 --> 00:04:41.426
Because the best way to learn any of
this stuff is to learn just enough to

75
00:04:41.426 --> 00:04:44.180
be dangerous and
throw yourself at it, right?

76
00:04:45.220 --> 00:04:49.620
Feeling like you need to have a PhD
before you can do anything is silly.

77
00:04:50.820 --> 00:04:54.276
So you'll see the term U-Net, which is
that core neural network that predicts

78
00:04:54.276 --> 00:04:55.780
the noise in an image at each step.

79
00:04:55.780 --> 00:04:57.460
So it's like what parts are still noise?

80
00:04:58.980 --> 00:05:04.200
The Scheduler tries to like, how much
noise can we remove at each interval.

81
00:05:04.200 --> 00:05:07.200
This is basically like how
much GPU you got, right?

82
00:05:07.200 --> 00:05:08.520
How much can I use?

83
00:05:08.520 --> 00:05:11.320
Let me dole it out until we get there,
right?

84
00:05:11.320 --> 00:05:16.487
The more GPU you have, the less
iterations you have to go through.

85
00:05:16.487 --> 00:05:18.235
And I will explain some of these
other terms in this next one.

86
00:05:18.235 --> 00:05:20.920
Cuz this one,
even as I was typing it stressed me out.

87
00:05:22.520 --> 00:05:27.793
The Variational Autoencode, it compresses
the image down to a lower latent space.

88
00:05:27.793 --> 00:05:29.903
I'll tell you what that is, don't worry.

89
00:05:29.903 --> 00:05:31.386
For efficient processing and

90
00:05:31.386 --> 00:05:34.535
then decode as a final representation
back to a visible image.

91
00:05:34.535 --> 00:05:38.420
The point that I want you to remember
is that it's not unlike the encoding,

92
00:05:38.420 --> 00:05:42.440
decoding, transformer piece,
which is there were words, there is image.

93
00:05:43.880 --> 00:05:46.110
Turn it into numbers
that we can play with and

94
00:05:46.110 --> 00:05:48.840
then turn it back into image
that I can enjoy, right?

95
00:05:52.520 --> 00:05:56.040
Those pieces are all the kind of the
pillars, you will see them in the code.

96
00:05:57.560 --> 00:05:59.560
There will be no quiz
that you memorize them.

97
00:05:59.560 --> 00:06:02.520
You will get a feel for
them as time goes on.

98
00:06:03.560 --> 00:06:04.840
A whole bunch of knobs to turn.

99
00:06:04.840 --> 00:06:08.890
As you can imagine, I picked the ones
that at the moment that I was making this

100
00:06:08.890 --> 00:06:10.680
slide, felt like the best ones.

101
00:06:12.680 --> 00:06:16.200
So how many steps do you
wanna do to denoise it?

102
00:06:17.240 --> 00:06:21.812
Fewer steps faster, fewer steps worse.

103
00:06:21.812 --> 00:06:28.207
More steps longer,
more steps better, [LAUGH], right?

104
00:06:28.207 --> 00:06:30.059
How big is your GPU, honestly?

105
00:06:30.059 --> 00:06:34.026
The guidance scale is how strictly
the model should adhere to your prompt.

106
00:06:34.026 --> 00:06:38.287
This is effectively not
unlike the temperature,

107
00:06:38.287 --> 00:06:42.147
especially if you're doing image to image.

108
00:06:42.147 --> 00:06:46.469
How much creative license
do you want to give it?

109
00:06:46.469 --> 00:06:49.719
So a lot of the kind of concepts
might have different names cuz they

110
00:06:49.719 --> 00:06:52.673
weren't developed by the same
people at the same time, but

111
00:06:52.673 --> 00:06:54.823
the fundamental principles are the same.

112
00:06:54.823 --> 00:06:58.823
And turns out shocker to everyone,

113
00:06:58.823 --> 00:07:03.631
bigger images take more, [LAUGH], right?

114
00:07:03.631 --> 00:07:08.172
So keep them small if you want
them fast or you don't want to,

115
00:07:08.172 --> 00:07:12.286
I don't know,
have $1,000 electricity bill.

116
00:07:12.286 --> 00:07:13.080
I don't know.

