WEBVTT

1
00:00:00.240 --> 00:00:01.934
&gt;&gt; Steve Kinney: With
stuff like retrieval,

2
00:00:01.934 --> 00:00:04.970
augmented generation of prompts,
effectively,

3
00:00:04.970 --> 00:00:09.344
that's where you take your own data,
you turn it into a bunch of vectors,

4
00:00:09.344 --> 00:00:13.920
you store it in a database,
a vector database, a database of numbers.

5
00:00:13.920 --> 00:00:19.600
And then if you were to say, hey, I wanna
augment this prop I'm sending to ChatGPT,

6
00:00:19.600 --> 00:00:23.280
you would use the same tokenization and
embedding.

7
00:00:23.280 --> 00:00:25.196
You turn your own prompt into numbers,

8
00:00:25.196 --> 00:00:28.050
find your own content that you
would also turn to numbers.

9
00:00:28.050 --> 00:00:32.610
The vector database does effectively
what the transformer is doing.

10
00:00:32.610 --> 00:00:36.371
It finds the chunks of your own text
that are relevant to your prompt and

11
00:00:36.371 --> 00:00:39.850
then kind of like just puts the text
onto the end of your prompt.

12
00:00:39.850 --> 00:00:44.210
And to give you automatically
take maybe all swaths of data.

13
00:00:44.210 --> 00:00:46.762
In fact,
if you open up something like Claude and

14
00:00:46.762 --> 00:00:51.380
you hook it up into Google Drive,
that's effectively what it's doing, right?

15
00:00:51.380 --> 00:00:55.300
It's kind of taking your data,
vectorizing it, and then figuring

16
00:00:55.300 --> 00:01:00.500
out what stuff matches and appending it
onto the query to give it more context.

17
00:01:00.500 --> 00:01:04.192
So even if you're not building your own
transformers, which most of you are not,

18
00:01:04.192 --> 00:01:07.100
you probably shouldn't be building
your own vector database.

19
00:01:07.100 --> 00:01:09.700
I did the other week,
I can't recommend it.

20
00:01:09.700 --> 00:01:10.780
It works on indexeddb.

21
00:01:10.780 --> 00:01:12.340
That was pretty cool.

22
00:01:12.340 --> 00:01:14.420
But you can actually kind of use.

23
00:01:14.420 --> 00:01:18.844
These are the same ways that vector
databases work for basically creating

24
00:01:18.844 --> 00:01:23.280
a search engine for your own data that
you then use on top of these prompts.

25
00:01:23.280 --> 00:01:26.846
So like that idea of turning stuff
into tokens, turning into IDs,

26
00:01:26.846 --> 00:01:30.986
figuring out the pieces of your content
that match is the same concept that you

27
00:01:30.986 --> 00:01:33.359
can use to kind of build
your own knowledge.

28
00:01:33.359 --> 00:01:37.320
Query systems that augment ChatGPT or
Claude or Gemini.

29
00:01:39.240 --> 00:01:43.352
And that is somewhat of a segue to
actually seeing how this attention stuff

30
00:01:43.352 --> 00:01:45.190
works in action.

31
00:01:45.190 --> 00:01:47.750
So I am going to install
some dependencies.

32
00:01:47.750 --> 00:01:51.801
They're nothing particularly crazy,
except for

33
00:01:51.801 --> 00:01:56.511
this bertviz, which will
visualize some of these tokens or

34
00:01:56.511 --> 00:02:00.390
the embeddings and
show you how they can relate.

35
00:02:02.150 --> 00:02:03.110
Cool, cool, cool.

36
00:02:04.470 --> 00:02:09.289
Here's some of the examples from
previously and some of the notes that I

37
00:02:09.289 --> 00:02:13.560
then later turned into slides so
we can move past some of that.

38
00:02:13.560 --> 00:02:17.281
And so here we have the cat sat on
the mat and the dog sat on the park,

39
00:02:17.281 --> 00:02:18.440
played in the park.

40
00:02:21.080 --> 00:02:23.920
And what this library
will do is it's basically

41
00:02:23.920 --> 00:02:28.600
just a way to show you the semantic
relationships between the words.

42
00:02:28.600 --> 00:02:32.200
And so I'll go ahead and hit play,
even though it's already loaded.

43
00:02:32.200 --> 00:02:33.900
But just to make my point, and

44
00:02:33.900 --> 00:02:37.240
you can change any of these
sentences in this Playground.

45
00:02:37.240 --> 00:02:39.578
So if you don't have it open,
come on over,

46
00:02:39.578 --> 00:02:42.614
open it up and play around with
some of these sentences and

47
00:02:42.614 --> 00:02:46.620
we can see like how two different
sentences maybe relate or don't relate.

48
00:02:48.060 --> 00:02:50.986
Cool, so right now it's going to
install some dependencies for me,

49
00:02:50.986 --> 00:02:53.740
even though I had a chart that
I could have just looked at.

50
00:02:53.740 --> 00:02:56.540
It's okay, it'll be here in one second.

51
00:02:56.540 --> 00:03:00.940
But what this will do is like
take two of our sentences, right?

52
00:03:00.940 --> 00:03:02.884
Two different strings of
text that technically,

53
00:03:02.884 --> 00:03:05.410
I guess these are technically
not different lengths.

54
00:03:05.410 --> 00:03:09.010
Even though I'm not sure how
the tokens will work on that.

55
00:03:09.010 --> 00:03:13.250
We're gonna tokenize both of them, right?

56
00:03:13.250 --> 00:03:16.990
And we'll get the outputs and then we're
going to kind of like visualize how they

57
00:03:16.990 --> 00:03:20.290
relate to each other and we'll try
out with a different one in a second.

58
00:03:21.570 --> 00:03:23.490
Getting impatient, could have hit that.

59
00:03:23.490 --> 00:03:24.450
Could have just not re.

60
00:03:24.450 --> 00:03:25.170
Hit the button.

61
00:03:25.170 --> 00:03:26.690
But here we are.

62
00:03:26.690 --> 00:03:29.770
So we're PIP installing.

63
00:03:29.770 --> 00:03:31.999
Hold on,
who doesn't like a good NPM install or

64
00:03:31.999 --> 00:03:35.420
PIP install at the beginning of
trying to get anything done?

65
00:03:35.420 --> 00:03:37.500
So we are downloading some dependencies.

66
00:03:37.500 --> 00:03:40.262
&gt;&gt; Speaker 2: Does it pull everything
down by default every time or

67
00:03:40.262 --> 00:03:43.260
is it smart enough to cache
stuff if it already has it?

68
00:03:43.260 --> 00:03:49.412
&gt;&gt; Steve Kinney: So on the Google Colab
side, if you are running the instance and

69
00:03:49.412 --> 00:03:54.300
you don't disconnect,
it'll have this stuff.

70
00:03:54.300 --> 00:03:57.104
But if you start it up cold and
you get a brand new VM,

71
00:03:57.104 --> 00:03:59.930
you will download the dependencies again.

72
00:03:59.930 --> 00:04:04.030
The thing that I am not doing
on purpose is you can actually

73
00:04:04.030 --> 00:04:06.810
have all the outputs saved.

74
00:04:06.810 --> 00:04:10.108
So you could theoretically run all
your data, produce all your graphs,

75
00:04:10.108 --> 00:04:12.010
all your outputs, send it to your buddy.

76
00:04:14.170 --> 00:04:18.810
They will see the outputs without
having to rerun the code.

77
00:04:18.810 --> 00:04:23.364
But if you want to run the code, you will
most likely get a very fresh VM Right now

78
00:04:23.364 --> 00:04:27.110
if the VM is running and
you're still connected to it, great.

79
00:04:27.110 --> 00:04:30.310
But if you get a fresh VM is
like getting a blank slate.

80
00:04:30.310 --> 00:04:32.070
You will install
everything all over again.

81
00:04:32.070 --> 00:04:33.842
Now, some libraries like Torch,

82
00:04:33.842 --> 00:04:38.070
like I think the Hugging face Transformers
library are just baked in and already.

83
00:04:38.070 --> 00:04:41.430
And there's a Docker container where
you have certain things set up already.

84
00:04:41.430 --> 00:04:44.510
Some of them you will
have to pull in yourself.

85
00:04:44.510 --> 00:04:45.790
&gt;&gt; Speaker 2: What about persistent disk?

86
00:04:45.790 --> 00:04:49.190
Could you write dependencies there and
then pull them in from the disk?

87
00:04:49.190 --> 00:04:49.710
Or is it.

88
00:04:49.710 --> 00:04:53.770
&gt;&gt; Steve Kinney: My sense of my
own experience is no, right?

89
00:04:53.770 --> 00:04:55.090
I've watched this.

90
00:04:55.090 --> 00:05:00.423
It's tricky cuz there is a difference
between if you do restart session and

91
00:05:00.423 --> 00:05:02.010
run all, yes.

92
00:05:02.010 --> 00:05:08.490
If you go into runtime and
you hit disconnect and delete runtime, no.

93
00:05:08.490 --> 00:05:11.051
And then the question is
that if I send it to you,

94
00:05:11.051 --> 00:05:13.450
I don't think you get my runtime, right?

95
00:05:13.450 --> 00:05:19.660
So ish, right, cool.

96
00:05:20.780 --> 00:05:23.380
So now we can run that and we can see.

97
00:05:23.380 --> 00:05:27.375
In this case, at any given point,
you can hover over and

98
00:05:27.375 --> 00:05:30.860
see kind of the relationship
between two words.

99
00:05:30.860 --> 00:05:34.780
So here we've got the two
various different sentences.

100
00:05:34.780 --> 00:05:38.752
There's not a lot of overlap between the
stuff we have in the cat sat on the mat

101
00:05:38.752 --> 00:05:41.180
and the dog played in the park.

102
00:05:41.180 --> 00:05:46.075
There's not a lot of lines getting
drawn there at all other than to

103
00:05:46.075 --> 00:05:50.530
the beginning of sentence and
end of sentence separators.

104
00:05:52.210 --> 00:05:57.238
However, if we change

105
00:05:57.238 --> 00:06:03.325
the strings a little bit and

106
00:06:03.325 --> 00:06:09.942
we can say these rob the bank,

107
00:06:09.942 --> 00:06:13.660
we run it again.

108
00:06:16.540 --> 00:06:20.620
Obviously, there's a lot more
interrelationship between these tokens.

109
00:06:20.620 --> 00:06:23.740
This is visualizing what Bert
is doing under the hood.

110
00:06:23.740 --> 00:06:27.700
Obviously, the could be related
to all these thieves and robbed.

111
00:06:27.700 --> 00:06:29.580
A pretty tight relationship.

112
00:06:29.580 --> 00:06:33.100
But bank, as you can see,
has got a line down to river.

113
00:06:33.100 --> 00:06:37.877
I understand anyone looking at this giant
screen is not seeing it nearly as well as

114
00:06:37.877 --> 00:06:38.660
I am on mine.

115
00:06:38.660 --> 00:06:41.240
So there is a little bit
of a contrast issue here.

116
00:06:41.240 --> 00:06:43.880
But you got it, yeah, on that screen,

117
00:06:43.880 --> 00:06:48.440
it becomes a little harder to see
if you're in the room with me.

118
00:06:48.440 --> 00:06:51.834
But if you pull up the notebook
yourself and take it for a spin,

119
00:06:51.834 --> 00:06:55.640
you will see them in a little more
clarity than it shows on the screen.

120
00:06:55.640 --> 00:06:58.900
But for
those watching the livestream or later,

121
00:06:58.900 --> 00:07:03.680
we can see that in given cases,
there is a line between bank and robbed.

122
00:07:03.680 --> 00:07:07.661
If we look at bank, we get a line
apparently robbed is probably showing up

123
00:07:07.661 --> 00:07:09.640
more than river in a lot of context.

124
00:07:11.580 --> 00:07:16.078
And like, you can see, basically,
this is visualizing for Bert some of

125
00:07:16.078 --> 00:07:21.340
the relationships between all of the words
and how they kind of relate to each other.

126
00:07:21.340 --> 00:07:25.019
So you can kind of just see how
some of those have been, like,

127
00:07:25.019 --> 00:07:27.039
correlated through the, like,

128
00:07:27.039 --> 00:07:31.980
millions of knob turns over time as it
tries to figure out some of that meaning.

129
00:07:31.980 --> 00:07:36.753
And like I said, what we're building
towards is eventually we're

130
00:07:36.753 --> 00:07:41.526
gonna take GPT2 medium and
we're gonna feed it a large data set of,

131
00:07:41.526 --> 00:07:44.710
I think, like, 16,000 quotes.

132
00:07:44.710 --> 00:07:46.830
And those quotes are all
gonna be a certain format.

133
00:07:46.830 --> 00:07:51.565
It's gonna be like, quote from Oscar
Wilde, and then it's gonna be colon and

134
00:07:51.565 --> 00:07:53.230
then some quotes, right?

135
00:07:53.230 --> 00:07:58.070
And we're gonna see it as we kinda train
it and we can just feed it a ton of them.

136
00:07:58.070 --> 00:07:59.630
With the expected outputs,

137
00:07:59.630 --> 00:08:03.470
we're gonna see that we can kind
of strengthen those relationships.

138
00:08:03.470 --> 00:08:05.252
And all of a sudden we'll go from like,

139
00:08:05.252 --> 00:08:08.600
who knows what comes out when we
start a sentence with that too.

140
00:08:08.600 --> 00:08:12.440
Even if it's a made up quote,
it will be in the format that we expect.

141
00:08:12.440 --> 00:08:14.600
It will have quotations around it.

142
00:08:14.600 --> 00:08:19.541
Because the act of sending all of those
things and training it to be like,

143
00:08:19.541 --> 00:08:21.000
this is what I want.

144
00:08:21.000 --> 00:08:22.440
Nope, Slap it on the wrist.

145
00:08:22.440 --> 00:08:23.200
This is what I want.

146
00:08:23.200 --> 00:08:29.560
And you can again tune those parameters
to actually build those relationships up.

147
00:08:32.690 --> 00:08:37.222
Cool, so it's just a way to visualize,
try it out with some different strings,

148
00:08:37.222 --> 00:08:41.956
like things that are widely related, words
that have different meanings, so on and

149
00:08:41.956 --> 00:08:42.570
so forth.

150
00:08:42.570 --> 00:08:46.370
And you can kind of like get a sense and
play around with it as well.

151
00:08:46.370 --> 00:08:47.410
Different things.

152
00:08:47.410 --> 00:08:50.730
Like other, like, dual meanings of
words are worth trying as well.

153
00:08:50.730 --> 00:08:52.263
Right.

154
00:08:58.252 --> 00:08:59.340
I don't know, this one will work.

155
00:08:59.340 --> 00:09:01.529
And I.
I'm already hitting the end of my sports

156
00:09:01.529 --> 00:09:02.460
knowledge.

157
00:09:02.460 --> 00:09:06.836
Right.
Like, I guess stealing first base and

158
00:09:06.836 --> 00:09:10.740
stealing diamonds is the same thing.

159
00:09:10.740 --> 00:09:12.940
I don't know why my
mind is going to crime.

160
00:09:12.940 --> 00:09:13.660
&gt;&gt; Speaker 2: First base.

161
00:09:13.660 --> 00:09:14.220
&gt;&gt; Steve Kinney: What's that?

162
00:09:14.220 --> 00:09:15.660
&gt;&gt; Speaker 2: You can't steal first base.

163
00:09:15.660 --> 00:09:17.340
&gt;&gt; Steve Kinney: Yeah, that's true.

164
00:09:17.340 --> 00:09:20.700
See, this is why I can't do sports
metaphors in these workshops.

165
00:09:20.700 --> 00:09:21.431
Because already.

166
00:09:21.431 --> 00:09:22.490
&gt;&gt; Speaker 2: They don't tag you.

167
00:09:22.490 --> 00:09:23.010
Isn't that a thing?

168
00:09:23.010 --> 00:09:24.210
That's not stealing first base.

169
00:09:24.210 --> 00:09:27.850
&gt;&gt; Steve Kinney: Okay, all right,
we're abandoning the sports metaphors.

170
00:09:27.850 --> 00:09:28.730
Okay.

171
00:09:28.730 --> 00:09:31.130
I clearly can't even make us.

172
00:09:31.130 --> 00:09:33.290
Like, that was a safe part of this.

173
00:09:33.290 --> 00:09:35.130
That was my second one.

174
00:09:35.130 --> 00:09:36.196
Yeah.
&gt;&gt; Speaker 2: Steal second,

175
00:09:36.196 --> 00:09:36.970
third, and home.

176
00:09:36.970 --> 00:09:37.930
&gt;&gt; Steve Kinney: All right, all right.

177
00:09:37.930 --> 00:09:39.290
No more sports metaphors.

178
00:09:39.290 --> 00:09:40.050
Play around with this.

179
00:09:40.050 --> 00:09:40.810
See how some of the words.

180
00:09:40.810 --> 00:09:44.354
But like, the neat part is you can,
like, get a sense of, like,

181
00:09:44.354 --> 00:09:47.450
seeing like, how this model is tuned.

182
00:09:47.450 --> 00:09:48.940
&gt;&gt; Speaker 2: What does the levels mean?

183
00:09:48.940 --> 00:09:50.220
Where it's at or the layer.

184
00:09:50.220 --> 00:09:52.140
Layer.
Cuz I was playing around with that.

185
00:09:52.140 --> 00:09:54.620
&gt;&gt; Steve Kinney: Yeah.
I'm not like, let's see.

186
00:09:54.620 --> 00:09:56.140
&gt;&gt; Speaker 2: Go to like,
layer two or layer.

187
00:09:57.420 --> 00:09:59.580
Yeah.
And then go take a look.

188
00:09:59.580 --> 00:10:00.980
&gt;&gt; Steve Kinney: I'm not totally
sure off the top of my head.

189
00:10:00.980 --> 00:10:02.113
&gt;&gt; Speaker 2: Okay.
&gt;&gt; Steve Kinney: Yep, but

190
00:10:02.113 --> 00:10:06.060
I do know that the visuals
are a lot better, right?

191
00:10:06.060 --> 00:10:08.940
It's probably as it's going through,
like the various, like.

192
00:10:10.140 --> 00:10:14.380
Yeah.
Cause look how, like, that's interesting.

193
00:10:14.380 --> 00:10:16.140
Like, I'm not totally sure on that one.

194
00:10:16.140 --> 00:10:18.540
It's probably as like, I'm guessing
as it goes through each layer.

195
00:10:18.540 --> 00:10:20.918
I'm curious, like,
if I grabbed a different model, like,

196
00:10:20.918 --> 00:10:22.770
if we would get more layers and
stuff like that.

197
00:10:22.770 --> 00:10:23.890
That'd be fascinating.

