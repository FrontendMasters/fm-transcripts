WEBVTT

1
00:00:00.160 --> 00:00:02.224
&gt;&gt; Steve Kinney: There
are some challenges though,

2
00:00:02.224 --> 00:00:07.120
because you're doing the math between two
different strings to see if they relate.

3
00:00:07.120 --> 00:00:12.091
And this is true if you do like we talk
about that retrieval augmented generation,

4
00:00:12.091 --> 00:00:16.628
which is, hey, I want to take all my
data and I want to turn it into vectors,

5
00:00:16.628 --> 00:00:17.294
numbers.

6
00:00:17.294 --> 00:00:22.373
And I want to, on a prompt, go figure
out what of my data I want to add onto

7
00:00:22.373 --> 00:00:27.814
the prompt so I can get a response back
from ChatGPT more related to my stuff.

8
00:00:27.814 --> 00:00:32.540
The problem with a lot of
the algorithms is they are expecting

9
00:00:32.540 --> 00:00:37.175
mathematical sets of numbers
that are the same length and so

10
00:00:37.175 --> 00:00:42.470
what do you do when you have two
strings that are not the same length?

11
00:00:44.230 --> 00:00:46.654
You just make them the same length, and

12
00:00:46.654 --> 00:00:49.670
you do that by filling
them up with nothingness.

13
00:00:49.670 --> 00:00:54.496
It's kind of like in JavaScript, if you do
like new array 10, it's weird because you

14
00:00:54.496 --> 00:00:59.590
get like some weird undefined thing and
then you got a map, it's not important.

15
00:00:59.590 --> 00:01:02.573
You can make arrays of nothingness, and

16
00:01:02.573 --> 00:01:07.749
effectively what you do in this case
is a special token, that pad token

17
00:01:07.749 --> 00:01:12.861
at the bottom where if we had like
the cat sat and the cat sat on the bed.

18
00:01:12.861 --> 00:01:17.375
You would just fill in the shorter
sentence with the empty tokens, and

19
00:01:17.375 --> 00:01:20.893
then you'd have two that were
the exact same length and

20
00:01:20.893 --> 00:01:24.970
now you can do all of the math that
you need to do to figure it out.

21
00:01:24.970 --> 00:01:27.939
In which a liberal arts
major discusses math,

22
00:01:27.939 --> 00:01:32.502
you do all the math you need to do,
but effectively, for our purposes,

23
00:01:32.502 --> 00:01:37.170
that is accurate and so basically we
just add nothingness onto the end.

24
00:01:37.170 --> 00:01:41.707
And then we have a lot of times
what's called an attention mask,

25
00:01:41.707 --> 00:01:46.159
which is, let's say you had
a string of tokens and it's 10,

26
00:01:46.159 --> 00:01:50.380
it was 3, 4, 5 tokens long,
you needed to have 10.

27
00:01:50.380 --> 00:01:54.356
You'd also have a mask, which is like
however many tokens of real actual content

28
00:01:54.356 --> 00:01:56.700
would be ones and
then the zeros would be zeros.

29
00:01:56.700 --> 00:02:00.476
So you can figure out how much of that to
ignore when we actually go deal with it

30
00:02:00.476 --> 00:02:01.540
again and decode it.

31
00:02:01.540 --> 00:02:04.733
And so we have a way to like
normalize everything and

32
00:02:04.733 --> 00:02:09.580
then we have a cheat code to go back to
what it actually was to denormalize it.

33
00:02:09.580 --> 00:02:12.012
And we'll see this in a second,
we'll see it on a slide, but

34
00:02:12.012 --> 00:02:14.398
we'll also see it in it,
we're doing the game where like,

35
00:02:14.398 --> 00:02:16.630
let's talk about the concepts
get a little confused.

36
00:02:16.630 --> 00:02:17.844
And then see in practicality,

37
00:02:17.844 --> 00:02:20.048
where we already have some
flags planted in the ground.

38
00:02:20.048 --> 00:02:23.089
Is the reason that they're all
the same length is because you're

39
00:02:23.089 --> 00:02:24.590
feeding into a neural network.

40
00:02:24.590 --> 00:02:25.551
Yeah.
And it's just, like,

41
00:02:25.551 --> 00:02:28.070
you have this many inputs, so
you have to give it everything, something.

42
00:02:28.070 --> 00:02:30.996
Exactly, and so it's comparing
the relationship of two strings,

43
00:02:30.996 --> 00:02:32.390
they need to be the same length.

44
00:02:33.430 --> 00:02:38.227
And so in this case, if we had one
where these are all the actual tokens

45
00:02:38.227 --> 00:02:41.230
in this case, I think the period counted.

46
00:02:41.230 --> 00:02:44.023
I did this on purpose,
I definitely measured it out,

47
00:02:44.023 --> 00:02:46.950
I forget my rationale on it and
you had two pads on there.

48
00:02:48.230 --> 00:02:52.647
You would, I think, because the initial,
the beginning of the sentence token and

49
00:02:52.647 --> 00:02:57.062
then the separator, you end up with the
special tokens in there, so you're like,

50
00:02:57.062 --> 00:02:59.670
that's three words, beginning, end, five.

51
00:03:01.270 --> 00:03:04.503
You would end up with all the ones
that actually relate to tokens

52
00:03:04.503 --> 00:03:08.350
we care about and zeros representing
we don't actually care about these.

53
00:03:08.350 --> 00:03:09.287
We just, again,

54
00:03:09.287 --> 00:03:12.850
had to have equal length of vectors
to pass into the neural network.

55
00:03:14.690 --> 00:03:18.472
And so those are our initial
tokenizations, we've got a tokenization

56
00:03:18.472 --> 00:03:22.189
playground that we're going to play
with before we get too excited and

57
00:03:22.189 --> 00:03:23.650
talk about Transformers.

