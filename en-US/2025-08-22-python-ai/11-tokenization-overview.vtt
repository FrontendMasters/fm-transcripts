WEBVTT

1
00:00:00.640 --> 00:00:03.787
&gt;&gt; Steve Kinney: So we spend a lot of time
talking about all the things you can do

2
00:00:03.787 --> 00:00:06.000
and ways you could do stuff.

3
00:00:06.000 --> 00:00:10.400
We didn't really get super far into it,
like how does any of that work?

4
00:00:10.400 --> 00:00:12.550
Right?
Which is the thing that we kind of need

5
00:00:12.550 --> 00:00:15.994
to do before we can like fine tune a model
to bend it to our whim is now we know

6
00:00:15.994 --> 00:00:17.920
the things you can do with the model.

7
00:00:17.920 --> 00:00:18.640
That's neat.

8
00:00:19.920 --> 00:00:21.120
We hand wavy.

9
00:00:21.120 --> 00:00:22.400
I think we know how they work.

10
00:00:24.160 --> 00:00:28.739
Let's actually get into how they work and
then we can weaponize that for

11
00:00:28.739 --> 00:00:30.660
our own nefarious purposes.

12
00:00:31.780 --> 00:00:33.700
So there's kind of two things
we need to talk about.

13
00:00:33.700 --> 00:00:35.220
One is this idea of tokenization.

14
00:00:35.220 --> 00:00:40.420
And then we'll get into encoding and
decoding and then attention.

15
00:00:41.540 --> 00:00:45.754
A lot of these models that we
see today in the current AI hype

16
00:00:45.754 --> 00:00:50.484
cycle are based on this paper
that came out of Google in 2017,

17
00:00:50.484 --> 00:00:54.784
2018, around then called
attention is all you need and

18
00:00:54.784 --> 00:00:58.710
introduces this idea of Transformers,
right?

19
00:00:58.710 --> 00:01:00.551
So we are going to start
with tokenization and

20
00:01:00.551 --> 00:01:03.030
we'll get all the way through
attention and Transformers.

21
00:01:03.030 --> 00:01:07.489
We'll do a little playing around some
other notebooks to explore this stuff, but

22
00:01:07.489 --> 00:01:11.818
we'll get that conceptual framework and
then we will weaponize that in order to

23
00:01:11.818 --> 00:01:16.070
figure out how to fine tune a model to
do the thing that we want to do, right?

24
00:01:16.070 --> 00:01:20.559
Because as I said earlier, and I will say
like three more times in the next hour,

25
00:01:20.559 --> 00:01:24.981
sometimes a really big model that's
a general purpose thing that can do almost

26
00:01:24.981 --> 00:01:27.180
anything is really good.

27
00:01:27.180 --> 00:01:30.165
Sometimes a tiny little lightweight,

28
00:01:30.165 --> 00:01:34.420
low resource model that you
can fine tune super easily.

29
00:01:34.420 --> 00:01:37.740
Because the bigger the model,
the harder it is to fine tune it, right?

30
00:01:37.740 --> 00:01:40.580
So the smaller the model,
the easier it is to kind of tune.

31
00:01:40.580 --> 00:01:43.060
And we'll look at even
ways to better tune it.

32
00:01:43.060 --> 00:01:46.842
But first we need to understand how all
these things work under the hood of all

33
00:01:46.842 --> 00:01:50.070
the things we just saw, so
that we can then leverage that.

34
00:01:50.070 --> 00:01:56.230
So the first of those topics that we'll
talk about is this idea of tokenization.

35
00:01:56.230 --> 00:01:59.270
You've definitely probably at this point
heard the word token at some point.

36
00:02:00.630 --> 00:02:06.489
And basically tokenization is a process
of taking some big wad of text and

37
00:02:06.489 --> 00:02:10.550
breaking it up into smaller pieces, right?

38
00:02:10.550 --> 00:02:15.280
And that's not always just words, right?

39
00:02:15.280 --> 00:02:18.640
Because sometimes,
we can break it into pieces.

40
00:02:18.640 --> 00:02:20.960
Like if you look at the word can't, right?

41
00:02:20.960 --> 00:02:25.040
There's kind of two pieces to
the word can't, can and not, right?

42
00:02:25.040 --> 00:02:29.000
And that right part is like, well, yeah,
that kind of works with isn't too,

43
00:02:29.000 --> 00:02:31.520
doesn't it or wasn't, and
a lot of other words.

44
00:02:31.520 --> 00:02:32.689
So like, in those cases,

45
00:02:32.689 --> 00:02:35.520
you break those kind of terms
up into maybe two tokens, right?

46
00:02:35.520 --> 00:02:39.730
Or a lot of, you know,
tokenization, for instance, right?

47
00:02:39.730 --> 00:02:43.090
Token and
then ization might be two different.

48
00:02:43.090 --> 00:02:47.995
So to say that, like,
a token is a word is, in a lot of cases,

49
00:02:47.995 --> 00:02:50.610
true, but not always, right?

50
00:02:51.650 --> 00:02:54.539
And it all depends a lot
of times on the model and

51
00:02:54.539 --> 00:02:57.290
the tokenizer and stuff along those lines.

52
00:02:57.290 --> 00:02:59.290
Because different models will
choose to do stuff differently.

53
00:02:59.290 --> 00:03:03.179
Like common words that maybe appear all
the time, even if it fell into one of

54
00:03:03.179 --> 00:03:07.430
those categories that we just said would,
might actually still be one token.

55
00:03:07.430 --> 00:03:08.750
The answer, it depends.

56
00:03:08.750 --> 00:03:12.750
But the idea is that we take these things,
we break them up into these tokens.

57
00:03:12.750 --> 00:03:15.670
These tokens have numeric representations,
right?

58
00:03:15.670 --> 00:03:19.025
And then we do math to find out how
they're related to each other in

59
00:03:19.025 --> 00:03:20.430
multidimensional space.

60
00:03:20.430 --> 00:03:22.230
And you're like, I'm a liberal arts major.

61
00:03:22.230 --> 00:03:23.750
Me too.

62
00:03:23.750 --> 00:03:24.950
So don't worry about it.

63
00:03:24.950 --> 00:03:26.550
It'll be fine.

64
00:03:26.550 --> 00:03:30.922
And so the process of tokenization is
we break down text into smaller words,

65
00:03:30.922 --> 00:03:32.110
subwords, sub.

66
00:03:32.110 --> 00:03:33.630
Sometimes a character, right?

67
00:03:35.070 --> 00:03:38.190
And then we kind of convert each one
of them into some kind of number.

68
00:03:38.190 --> 00:03:43.176
We have some special tokens which might
be the start or end of a sentence,

69
00:03:43.176 --> 00:03:45.070
other meaningful things.

70
00:03:45.070 --> 00:03:49.695
In the case of something like a chat, one
that's trained for chat, like the tokens

71
00:03:49.695 --> 00:03:53.396
that separate the assistant from
the user from the system prompt,

72
00:03:53.396 --> 00:03:57.625
there are special tokens you don't ever
see, but they are in there to kind of

73
00:03:57.625 --> 00:04:01.300
draw invisible lines of demarcation
between sections, right?

74
00:04:02.740 --> 00:04:06.822
And then we also then figure out
various different ways to find

75
00:04:06.822 --> 00:04:09.340
out the importance of various words.

76
00:04:09.340 --> 00:04:13.208
Cuz a lot of ways this is gonna work,
not to ruin some of the surprise for

77
00:04:13.208 --> 00:04:17.700
you, is it's about any given word and
its relation to words around it, right?

78
00:04:17.700 --> 00:04:22.926
And there's some words that we probably
that are very related to other words,

79
00:04:22.926 --> 00:04:26.420
like the word the that shows
up around a lot of words.

80
00:04:26.420 --> 00:04:28.300
A lot of words have
relationships to the word.

81
00:04:28.300 --> 00:04:31.330
The not particularly
important relationships.

82
00:04:32.530 --> 00:04:34.850
Sometimes maybe, but not always.

83
00:04:34.850 --> 00:04:39.242
And so then we do various different
things to try to figure out, one,

84
00:04:39.242 --> 00:04:43.970
what is the relationship of that word
to the other words in the sentence?

85
00:04:43.970 --> 00:04:46.530
And then two,
how strong are those relationships?

86
00:04:46.530 --> 00:04:49.890
And then given those relationships,
what could that word mean?

87
00:04:49.890 --> 00:04:52.284
And I will use this on a later slide, but

88
00:04:52.284 --> 00:04:57.005
one of the examples is the concept of
going down to the banks of the Mississippi

89
00:04:57.005 --> 00:05:00.750
river versus robbing a bunch
of banks tomorrow, right?

90
00:05:00.750 --> 00:05:01.710
Same word.

91
00:05:01.710 --> 00:05:03.550
Context matters, right?

92
00:05:03.550 --> 00:05:08.110
And we as humans just have brains
that happen to do that, right?

93
00:05:08.110 --> 00:05:09.470
Computers don't.

94
00:05:09.470 --> 00:05:11.589
So we had to figure out
how to teach them to.

95
00:05:11.589 --> 00:05:17.310
And that's effectively the higher level
concept that we're going to talk about.

96
00:05:18.990 --> 00:05:22.030
Like I said,
there are various different tokenizers.

97
00:05:23.850 --> 00:05:25.130
You do not need to memorize this.

98
00:05:25.130 --> 00:05:26.890
There will not be a quiz on it later.

99
00:05:26.890 --> 00:05:31.895
Just to say, for some of these, for some
of the ones we saw earlier in our journey

100
00:05:31.895 --> 00:05:37.450
together, they have how many vocabulary is
how many tokens does it know about, right?

101
00:05:39.050 --> 00:05:41.290
And then how does it do it?

102
00:05:41.290 --> 00:05:44.490
It's not really on
a practical day to day basis.

103
00:05:44.490 --> 00:05:48.980
All you kind of need to know is
that they're different, right?

104
00:05:48.980 --> 00:05:50.140
They have different vocabularies.

105
00:05:50.140 --> 00:05:57.220
And obviously GPT2, you can just imagine
has a smaller vocabulary than GPT4, right?

106
00:05:58.260 --> 00:06:00.826
And all those things matter
to a certain extent, but

107
00:06:00.826 --> 00:06:03.140
not necessarily in our day to day usage.

108
00:06:03.140 --> 00:06:06.980
Just knowing that one, that you might
see differences across the different

109
00:06:06.980 --> 00:06:09.860
models and have to change your
approach is totally true.

110
00:06:10.900 --> 00:06:12.420
Does it affect your day to day life?

111
00:06:12.420 --> 00:06:13.620
Most days it does not.

112
00:06:14.750 --> 00:06:18.670
Cool, so then we split things in tokens.

113
00:06:18.670 --> 00:06:23.743
In this case for whatever library,
possibly a tokenized method,

114
00:06:23.743 --> 00:06:29.359
you can treat it as pseudocode at this
point which will take a given word and

115
00:06:29.359 --> 00:06:31.550
split it into these tokens.

116
00:06:31.550 --> 00:06:36.110
So tokenization might,
I think this one is GPTs.

117
00:06:36.110 --> 00:06:37.310
I think it's GPT2's.

118
00:06:37.310 --> 00:06:40.776
If I'm doing this from memory
from when I made the slides,

119
00:06:40.776 --> 00:06:44.330
is how that would break apart
tokenization into two words.

120
00:06:44.330 --> 00:06:48.732
And you can kind of see that there is at
least a little bit of a hint that one of

121
00:06:48.732 --> 00:06:50.530
these is a suffix, right?

122
00:06:50.530 --> 00:06:52.330
I'll let you guess which
one is the suffix.

123
00:06:52.330 --> 00:06:55.370
But you can kind of break and
then you can figure out.

124
00:06:55.370 --> 00:06:58.580
Cuz token, you don't wanna just figure
out tokenization's relationship to

125
00:06:58.580 --> 00:06:59.170
other words.

126
00:06:59.170 --> 00:07:00.450
Right.
Sometimes.

127
00:07:00.450 --> 00:07:01.570
What does token mean?

128
00:07:01.570 --> 00:07:03.930
Like swimming and swim.

129
00:07:03.930 --> 00:07:06.398
You want to definitely like swim
is probably important context for

130
00:07:06.398 --> 00:07:07.410
the other words around.

131
00:07:07.410 --> 00:07:10.573
You don't want to have different ways
of swimming than you want to have for

132
00:07:10.573 --> 00:07:11.950
swim versus for swam.

133
00:07:11.950 --> 00:07:13.470
Swam would technically be it's own token.

134
00:07:13.470 --> 00:07:14.350
But whatever.

135
00:07:14.350 --> 00:07:17.110
Like the idea is that you break
them apart so they are meaningful.

136
00:07:17.110 --> 00:07:24.030
So a token is effectively,
you know, a piece of a word.

137
00:07:24.030 --> 00:07:27.648
What is the heuristic that if you're
just trying to guess how many tokens

138
00:07:27.648 --> 00:07:28.430
something is.

139
00:07:28.430 --> 00:07:32.190
It's like 0.75 the number of characters.

140
00:07:32.190 --> 00:07:33.230
That's just a heuristic.

141
00:07:33.230 --> 00:07:34.190
That's not true.

142
00:07:34.190 --> 00:07:38.025
But it's like not as many Characters
are in it because there are small words,

143
00:07:38.025 --> 00:07:41.990
there are big words, some words will get
broken up into two, so on and so forth.

144
00:07:41.990 --> 00:07:44.030
So it's a close enough representation.

145
00:07:44.030 --> 00:07:48.470
The point is we break apart words,
we turn them into numbers, and

146
00:07:48.470 --> 00:07:50.750
we have these special tokens.

147
00:07:50.750 --> 00:07:55.175
So then when we encode them is when the
process of taking those tokens that then

148
00:07:55.175 --> 00:07:56.196
map to a number and

149
00:07:56.196 --> 00:08:00.030
like the same word will always map
to effectively the same number.

150
00:08:01.320 --> 00:08:03.320
And we have these special tokens.

151
00:08:03.320 --> 00:08:05.120
Is it the same word or the same token?

152
00:08:05.120 --> 00:08:05.680
Same token.

153
00:08:05.680 --> 00:08:06.280
Good catch.

154
00:08:08.520 --> 00:08:10.978
Absolutely.
Same token, not the same word,

155
00:08:10.978 --> 00:08:15.302
will map to the same ID at this point,
these special ones have known IDs

156
00:08:15.302 --> 00:08:19.080
classification token is at
the beginning of a sequence.

157
00:08:19.080 --> 00:08:21.834
Separator token is to
separate the segments,

158
00:08:21.834 --> 00:08:26.760
two different sentences, we'll have
a separator in between them as well.

159
00:08:26.760 --> 00:08:29.382
These start to give it
a sense of that shape and

160
00:08:29.382 --> 00:08:33.620
the ability to figure out where to
stop and end and how this stuff works.

161
00:08:33.620 --> 00:08:37.576
In the case of something like a overly
simplified version of if you're building

162
00:08:37.576 --> 00:08:41.765
your own chatgpt kind of like model, there
will be those special ones that will say,

163
00:08:41.765 --> 00:08:44.599
okay, here's the beginning
of the assistant message,

164
00:08:44.599 --> 00:08:48.700
end of the assistant message, beginning
of the user message, so on and so forth.

165
00:08:48.700 --> 00:08:52.614
Things that kind of act as lines of
demarcation between the tokens or

166
00:08:52.614 --> 00:08:54.900
tokens in and of themselves, right?

167
00:08:54.900 --> 00:08:57.380
But it all becomes a set of numbers.

168
00:08:57.380 --> 00:09:00.730
So for instance,
you might see something like hello world.

169
00:09:00.730 --> 00:09:02.890
Obviously you go to encode it and

170
00:09:02.890 --> 00:09:06.490
you will end up with the IDs
of all of those tokens.

171
00:09:06.490 --> 00:09:13.290
If you then were to go decode it,
you lose a little bit of meaning.

172
00:09:13.290 --> 00:09:15.929
So obviously you lose the capitalization.

173
00:09:15.929 --> 00:09:20.503
Because we don't think that hello means in
some cases, depending on the vocabulary,

174
00:09:20.503 --> 00:09:25.290
certain things, all caps do mean something
different than the lowercase version.

175
00:09:25.290 --> 00:09:27.807
But generally speaking,
right, we break it apart and

176
00:09:27.807 --> 00:09:31.310
we will have first of all the addition
of the special characters.

177
00:09:31.310 --> 00:09:33.077
But then like, you know, and

178
00:09:33.077 --> 00:09:37.150
the comma and the exclamation
point are also tokenized as well.

179
00:09:37.150 --> 00:09:41.021
And we break it apart and we kind of can
see if we take a given string of text,

180
00:09:41.021 --> 00:09:44.708
encode it, and then immediately decode it,
like we gain some stuff,

181
00:09:44.708 --> 00:09:46.910
we lose some stuff along the way, right?

182
00:09:46.910 --> 00:09:49.024
But that's effectively kind of showing,

183
00:09:49.024 --> 00:09:52.440
like how this process will
normalize a bunch of those words.

184
00:09:52.440 --> 00:09:54.000
So encoding turns into numbers.

185
00:09:54.000 --> 00:09:57.240
You can take a lucky
guess what decoding does.

186
00:09:57.240 --> 00:10:01.725
Decoding is able to then turn it
back into a string because sure,

187
00:10:01.725 --> 00:10:06.794
your prompt goes in, it gets turned
into a bunch of numbers stuff happens

188
00:10:06.794 --> 00:10:11.960
to figure out what the next set of
tokens should be in terms of numbers.

189
00:10:11.960 --> 00:10:13.923
But if you just Type
something into ChatGPT and

190
00:10:13.923 --> 00:10:17.560
you got back a bunch of numbers, you'd be
like, well, I'm never using this again.

191
00:10:18.800 --> 00:10:23.112
So then there is the process of
putting it back together and

192
00:10:23.112 --> 00:10:25.400
that is the decoding process.

193
00:10:25.400 --> 00:10:27.840
And different models
will use it differently.

194
00:10:27.840 --> 00:10:31.415
You can imagine that the ones that
are looking forward and backwards

195
00:10:31.415 --> 00:10:35.806
will engage with this differently than the
ones that are only looking backwards and

196
00:10:35.806 --> 00:10:38.720
trying to generate the way forward.

197
00:10:38.720 --> 00:10:41.647
Is this where model watermarking for
outputs happens too,

198
00:10:41.647 --> 00:10:44.310
when they're decoding to print a screen?

199
00:10:44.310 --> 00:10:45.590
Basically.

200
00:10:45.590 --> 00:10:48.790
Are those proprietary libraries or
are those published as well?

201
00:10:48.790 --> 00:10:50.950
Depends if it's an open source model or
not, right?

202
00:10:53.030 --> 00:10:56.630
Insofar that I know
that obviously GPT2 is.

203
00:10:56.630 --> 00:10:59.370
I think GPT3 had a model card and

204
00:10:59.370 --> 00:11:05.270
then at some point they became less
OpenAI became less open about AI.

205
00:11:06.710 --> 00:11:09.498
So in this case you can
take a series of tokens and

206
00:11:09.498 --> 00:11:11.610
you decode it back in the token id.

207
00:11:11.610 --> 00:11:12.730
No surprises here.

