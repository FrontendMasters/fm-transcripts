WEBVTT

1
00:00:00.080 --> 00:00:01.440
&gt;&gt; Steve Kinney: So
I'm going to start with the dependencies.

2
00:00:03.280 --> 00:00:07.553
We can look at some of this real quick
though because again we're going to

3
00:00:07.553 --> 00:00:11.280
switch in that other lower VRAM
scheduler and stuff like that.

4
00:00:11.280 --> 00:00:16.033
But actually I want to open
this piece that I hid for

5
00:00:16.033 --> 00:00:20.446
a second, so
I will grab the diffusers instead

6
00:00:20.446 --> 00:00:25.840
of Transformers stuff where
I get rid of the output.

7
00:00:25.840 --> 00:00:32.609
The other thing that I'm doing here which
is just trying to detect whether or

8
00:00:32.609 --> 00:00:39.610
not we have an Nvidia card which if we
have like let's see which runtime am I in?

9
00:00:40.730 --> 00:00:45.237
So I'm on the free one, the T4,
like whatever, we have an Nvidia card,

10
00:00:45.237 --> 00:00:49.093
note that we have that available and
we're going to use that and

11
00:00:49.093 --> 00:00:50.570
set that as the device.

12
00:00:50.570 --> 00:00:55.514
We'll see that down here as well,
send it to the graphics card effectively,

13
00:00:55.514 --> 00:00:59.547
so there's some stuff in here
that is maybe not interesting but

14
00:00:59.547 --> 00:01:01.850
arguably important or vice versa.

15
00:01:02.890 --> 00:01:05.713
For the model ID, I think later I'll have,

16
00:01:05.713 --> 00:01:09.300
there's ones where you could
change the one we use, but

17
00:01:09.300 --> 00:01:14.258
you can also just any of the text to image
models on hugging face are fair game or

18
00:01:14.258 --> 00:01:19.370
the ones I had in that previous slide,
they're also fair game.

19
00:01:19.370 --> 00:01:23.498
I will seek to turn that
into a drop-down later, so

20
00:01:23.498 --> 00:01:27.530
instead of before you saw
that auto tokenizer from

21
00:01:27.530 --> 00:01:32.150
pre-trained now it's auto pipeline for
text to image.

22
00:01:33.430 --> 00:01:38.959
We give it the model ID that we want
to use the safe tensors that we're

23
00:01:38.959 --> 00:01:44.585
seeking to use low CPU usage here,
so on and so forth, memory usage,

24
00:01:44.585 --> 00:01:49.830
so on and so forth,
swap idle layers with the cpu, absolutely.

25
00:01:53.160 --> 00:01:56.360
Again, we're just trying
to slice it up here,

26
00:01:56.360 --> 00:01:59.800
we got a Pygmy Hippo watching
Instagram on a phone.

27
00:02:01.320 --> 00:02:05.668
Can't remember if that was one that
was going to go well for us or

28
00:02:05.668 --> 00:02:10.592
if that's where I left playing around
with it, I need to grant access to

29
00:02:10.592 --> 00:02:15.211
that token from earlier,
that's the one that lives over in here.

30
00:02:18.271 --> 00:02:21.737
Once you do that once, you're good, but

31
00:02:21.737 --> 00:02:27.087
I started the playground so
it's technically a new notebook and

32
00:02:27.087 --> 00:02:32.930
for some of these it will become
a lot more interesting to watch this.

33
00:02:32.930 --> 00:02:38.248
I will say I tried in a lot of cases
given the way that we saw that

34
00:02:38.248 --> 00:02:43.900
you run one cell and then that
variable is accessible in memory.

35
00:02:45.900 --> 00:02:50.156
A lot of these are not like if you
were just running a python script,

36
00:02:50.156 --> 00:02:54.300
it would run, you'd have your image,
you'd be done.

37
00:02:54.300 --> 00:02:56.261
There are some times where it's like,

38
00:02:56.261 --> 00:02:59.471
obviously if you run the same thing
over and over and over again,

39
00:02:59.471 --> 00:03:03.100
like it doesn't necessarily free up
all that memory in the same way.

40
00:03:03.100 --> 00:03:07.726
So if you truly end up in a place where
you're getting out of memory errors,

41
00:03:07.726 --> 00:03:12.794
just go to runtime, hit, disconnect and
delete runtime and start it from crash,

42
00:03:12.794 --> 00:03:18.470
in some cases, I did start going out of
my way to trigger garbage collection.

43
00:03:18.470 --> 00:03:21.357
And then I stopped and asked myself,
how many times are people gonna run these

44
00:03:21.357 --> 00:03:23.990
things over and over and over and
over again other than me rehearsing?

45
00:03:26.470 --> 00:03:30.705
And this is pretty fast, on the free GPU,

46
00:03:30.705 --> 00:03:36.150
he have a baby pygmy hippo
in plausibly New York City,

47
00:03:36.150 --> 00:03:40.143
not quite sure, He could be on a phone,

48
00:03:40.143 --> 00:03:44.630
we don't know what's going on down there.

49
00:03:47.590 --> 00:03:52.348
But like, kind of uses this built in
the built-in layers on these GPUs,

50
00:03:52.348 --> 00:03:54.390
watching Instagram.

51
00:03:54.390 --> 00:03:59.915
Also like question of when
this was even trained,

52
00:03:59.915 --> 00:04:04.785
not bad, all things considered, fairly,

53
00:04:04.785 --> 00:04:10.980
fairly quickly using again
these same basic concepts.

54
00:04:10.980 --> 00:04:16.881
Again, this is an open source model, this
is presently other than a PIP install,

55
00:04:16.881 --> 00:04:20.460
this is presently all
of the code involved.

56
00:04:20.460 --> 00:04:26.310
And most of this, as you can see
from my own notes to myself,

57
00:04:26.310 --> 00:04:30.522
was to get the memory
down on the free tier,

58
00:04:30.522 --> 00:04:35.085
if you didn't even have
those restrictions,

59
00:04:35.085 --> 00:04:38.950
one could theoretically do even more.

60
00:04:41.750 --> 00:04:45.645
And again, the number of inference steps
are how many times you would like it to

61
00:04:45.645 --> 00:04:49.128
go, the guidance scale is how seriously
it should take the prompt, so

62
00:04:49.128 --> 00:04:51.510
you can play around with
these numbers as well.

63
00:04:53.110 --> 00:04:59.722
So for instance, I don't know what
happens if we turn that to a 20,

64
00:04:59.722 --> 00:05:05.080
I should, while I'm teaching,
go to the fancier GPU.

65
00:05:05.080 --> 00:05:09.013
This is not taking all that long anyway,
so

66
00:05:09.013 --> 00:05:13.385
later when I show you
something that will like,

67
00:05:13.385 --> 00:05:17.920
make your day, I will go to the fast GPU.

68
00:05:17.920 --> 00:05:21.014
Hey, it's something plausibly
looking like an iPhone.

69
00:05:21.014 --> 00:05:21.925
&gt;&gt; Student: [LAUGH]
&gt;&gt; Steve Kinney: Again,

70
00:05:21.925 --> 00:05:26.052
this is like I have the cats walking
across the top of the screen at this

71
00:05:26.052 --> 00:05:26.950
point.

72
00:05:26.950 --> 00:05:31.197
When my wife came in and said,
would you like to spend time with me?,

73
00:05:31.197 --> 00:05:35.990
I said, honey, I'm working, and
she believes me because she's the best.

74
00:05:37.350 --> 00:05:40.070
&gt;&gt; Student: She gave you that look?

75
00:05:40.070 --> 00:05:41.990
&gt;&gt; Steve Kinney: Yeah,
I mean she also loves Moo Deng,

76
00:05:41.990 --> 00:05:45.830
that was the inspiration of this,
I was pandering, I knew what I was doing.

77
00:05:45.830 --> 00:05:51.465
But so these are in here right
now with some of these tools,

78
00:05:51.465 --> 00:05:59.310
again, number of inference steps,
like lower is faster, higher is better.

79
00:05:59.310 --> 00:06:00.470
Let's play with it for a second.

80
00:06:00.470 --> 00:06:03.550
Since t hings seem to
be pretty fast today.

81
00:06:03.550 --> 00:06:05.230
What happens if I do five?

82
00:06:08.990 --> 00:06:11.870
I should keep looking at this
number up here for like.

83
00:06:11.870 --> 00:06:14.865
That was pretty fast, but

84
00:06:14.865 --> 00:06:19.910
the results speak for themselves, right?

85
00:06:19.910 --> 00:06:22.163
Because again,
if we think about the process,

86
00:06:22.163 --> 00:06:24.030
it started with random chaotic noise.

87
00:06:25.600 --> 00:06:29.680
And I kept trying to peel away
the random chaotic noise.

88
00:06:29.680 --> 00:06:33.994
As you can see, that didn't work.

89
00:06:33.994 --> 00:06:37.000
What is guidance scale?

90
00:06:37.000 --> 00:06:42.280
It's how closely you want
it to stick to the script.

91
00:06:42.280 --> 00:06:44.720
&gt;&gt; Student: Is higher better or lower?

92
00:06:44.720 --> 00:06:48.185
&gt;&gt; Steve Kinney: I think,
it's like temperature, so

93
00:06:48.185 --> 00:06:52.624
this is one round of denoising,
from the chaos,

94
00:06:52.624 --> 00:06:57.623
it's not much of anything,
it is kind of cool though.

95
00:06:57.623 --> 00:06:58.465
[LAUGH] Yes,

96
00:06:58.465 --> 00:07:04.490
it does not look like a pygmy hippo
in New York City on an iPhone, noted.

97
00:07:04.490 --> 00:07:10.205
I don't hate it, I like contemporary art,

98
00:07:10.205 --> 00:07:14.221
like modern art, if you will,

99
00:07:14.221 --> 00:07:23.579
what if we gave it 100
&gt;&gt; Steve Kinney: And

100
00:07:23.579 --> 00:07:27.144
not quite sure where we're
going with this one?

101
00:07:27.144 --> 00:07:33.445
I again, strangely don't hate it, but that
was with the guidance turned all the way.

102
00:07:33.445 --> 00:07:34.637
All the way down.

103
00:07:34.637 --> 00:07:37.631
What did we have before?

104
00:07:41.206 --> 00:07:43.963
&gt;&gt; Steve Kinney: But this, this is why
I'm saying enough to be dangerous, and

105
00:07:43.963 --> 00:07:47.726
you know what, you're gonna really learn
how to like tweak these by tweaking them,

106
00:07:47.726 --> 00:07:49.070
and you know what?

107
00:07:49.070 --> 00:07:54.293
I get it, I get it, I get it,
the text wasn't really rewarding, but

108
00:07:54.293 --> 00:07:59.892
I couldn't have started with the text or
the images that gone to the text.

109
00:07:59.892 --> 00:08:02.270
&gt;&gt; [LAUGH] Whoa, what is that?

110
00:08:02.270 --> 00:08:06.753
&gt;&gt; Steve Kinney: I don't know,
it's got the six toes, though.

111
00:08:06.753 --> 00:08:12.800
[LAUGH] What else did I have
in my actual guidance skills?

112
00:08:14.240 --> 00:08:15.920
&gt;&gt; Student: Can you go backwards?

113
00:08:15.920 --> 00:08:17.760
&gt;&gt; Steve Kinney: To previous versions?

114
00:08:17.760 --> 00:08:21.120
&gt;&gt; Student: No, from an image to a prompt?

115
00:08:21.120 --> 00:08:24.154
&gt;&gt; Steve Kinney: There
are image to text classifiers.

116
00:08:24.154 --> 00:08:27.600
&gt;&gt; Student: That's not stable diffusion,
that's generating images.

117
00:08:27.600 --> 00:08:31.647
&gt;&gt; Steve Kinney: I think I don't remember
exactly which because I've not done a lot

118
00:08:31.647 --> 00:08:34.030
of the image to text stuff that famous,

119
00:08:34.030 --> 00:08:37.950
The Silicon Valley episode of is it
a hot dog or is it not a hot dog?

120
00:08:39.550 --> 00:08:44.668
Those are trained effectively though, all
these things kind of learn the same way

121
00:08:44.668 --> 00:08:49.807
though, you show it picture, that picture
is labeled hippo, all these things.

122
00:08:49.807 --> 00:08:53.824
Like the same way if you
pull out your phone and

123
00:08:53.824 --> 00:08:58.670
you type in dog,
you will get those pictures of a dog.

124
00:08:58.670 --> 00:09:02.904
And so like, that's just actually it
uses a different technique where it

125
00:09:02.904 --> 00:09:06.950
kind of like looks for the edges and
all the shapes and stuff like that.

126
00:09:06.950 --> 00:09:10.779
And like makes them super high
contrast and just goes based on these,

127
00:09:10.779 --> 00:09:15.071
like the pixels next to this pixel,
over time, the knobs return to show dog,

128
00:09:15.071 --> 00:09:16.480
hot dog, what have you.

129
00:09:17.840 --> 00:09:19.714
That's terrifying, though,
I didn't like that one.

130
00:09:26.758 --> 00:09:30.510
&gt;&gt; Steve Kinney: I talked about this,
which is basically like we do need to

131
00:09:30.510 --> 00:09:35.666
figure out if we can send it to a GPU,
which you saw the initial block and that's

132
00:09:35.666 --> 00:09:41.410
effectively happening in the pipeline,
I send it off to the GPU effectively.

133
00:09:41.410 --> 00:09:46.614
So the device is set up here,
device go to Cuda,

134
00:09:46.614 --> 00:09:51.951
which is Nvidia's thing,
if it's available,

135
00:09:51.951 --> 00:09:55.170
otherwise stick to the CPU.

136
00:09:55.170 --> 00:09:59.967
And so we start with the pipeline,
we send it to the device and again,

137
00:09:59.967 --> 00:10:03.605
like you can go up to a higher card and
turn these off,

138
00:10:03.605 --> 00:10:07.412
tweak all the numbers,
like everything is fair game,

139
00:10:07.412 --> 00:10:11.919
this is code you can use and
have because you should play with it.

140
00:10:16.580 --> 00:10:21.649
&gt;&gt; Steve Kinney: I think I set that at
some point because I wanted some amount

141
00:10:21.649 --> 00:10:26.730
of stability,
it's like where we start the randomness.

142
00:10:26.730 --> 00:10:31.306
The forward process, we take a clear
image and gradually turn into noise, and

143
00:10:31.306 --> 00:10:34.930
then reversing that process gets
us to the Stable Diffusion.

144
00:10:37.170 --> 00:10:38.910
All the stuff I had in the slides earlier,

145
00:10:38.910 --> 00:10:41.010
the prompt structure I had
in the slides earlier.

146
00:10:42.130 --> 00:10:45.082
Here's one where I've got,
this is with negative prompts, and

147
00:10:45.082 --> 00:10:48.370
this is where I was smart enough to put
the ability to swap out the models.

148
00:10:52.720 --> 00:10:57.786
So you can pick any one of these in
a Jupyter notebook, this weird comment

149
00:10:57.786 --> 00:11:03.680
is what makes a variable controllable
from these things, so if I literally.

150
00:11:03.680 --> 00:11:08.258
Unfortunately, I'm in the playground now,
but

151
00:11:08.258 --> 00:11:14.471
I could do it if I pasted this up here,
no, now you don't like that.

152
00:11:17.926 --> 00:11:20.669
&gt;&gt; Student: Whatever, leave me alone.

153
00:11:20.669 --> 00:11:25.604
&gt;&gt; Steve Kinney: It's just that this
isn't one of the ones on that list,

154
00:11:25.604 --> 00:11:30.656
it's like, now my variable is wrong,
now it's right.

155
00:11:35.112 --> 00:11:37.702
&gt;&gt; Steve Kinney: So now I could
theoretically swap this one out,

156
00:11:37.702 --> 00:11:40.618
anyone remember what my
original numbers were like,

157
00:11:40.618 --> 00:11:44.530
I think it was like 50 inference steps and
20 on the guidance scale.

158
00:11:45.570 --> 00:11:48.247
Yes, seems to be, so
now I can pick a different model,

159
00:11:48.247 --> 00:11:51.799
obviously I'm going to pay a cost to
download that model for a second, so

160
00:11:51.799 --> 00:11:54.771
let's go talk about something
else while that downloads and

161
00:11:54.771 --> 00:11:58.290
then we will hopefully be pleasantly
surprised when we scroll back up.

162
00:11:59.490 --> 00:12:04.800
So again, smaller models will be faster,
but like time doesn't seem

163
00:12:04.800 --> 00:12:10.830
to be totally bothering us at this point,
I am just going to say show me the code or

164
00:12:10.830 --> 00:12:14.700
hide the form for a second so
we can see everything.

165
00:12:15.900 --> 00:12:20.178
So here we've got the prompt,
the positive, ultra realistic cinematic

166
00:12:20.178 --> 00:12:24.594
photo of pygmy hippo Jay walking through
a neon lit downtown street at dusk,

167
00:12:24.594 --> 00:12:27.768
you can tell I took the original one and
put in ChatGPT.

168
00:12:27.768 --> 00:12:31.912
I was like, make this more detailed
at some point because I didn't have

169
00:12:31.912 --> 00:12:33.790
the patience to write this one.

170
00:12:34.980 --> 00:12:39.200
And I definitely wouldn't have
thought about a Kodak Portra 800 and

171
00:12:39.200 --> 00:12:42.643
then like things I don't want blurry,
grainy, low res,

172
00:12:42.643 --> 00:12:47.700
overexposed watermark text,
logo, extra limbs, cars, people.

173
00:12:47.700 --> 00:12:51.135
This one, I definitely was angry, that
one definitely looks like I wrote it from

174
00:12:51.135 --> 00:12:53.220
scratch, this one,
I wasn't feeling up to it.

175
00:12:55.380 --> 00:12:59.720
So we'll run that one as well,
let's see, is my other one ready?, no,

176
00:12:59.720 --> 00:13:04.989
we're almost, almost, almost, we're gonna
have the big reveal all at the same time.

177
00:13:10.365 --> 00:13:14.053
&gt;&gt; Steve Kinney: I don't know if I'd
fully give that studio Ghibli, but

178
00:13:14.053 --> 00:13:19.630
like I also said probably photorealistic
in here or something, no, I don't know.

179
00:13:21.150 --> 00:13:23.948
But it's worth playing with
the different models and

180
00:13:23.948 --> 00:13:27.492
seeing what the different results again,
I don't hate it though,

181
00:13:27.492 --> 00:13:30.244
is it what I wanted?, no,
do I hate it?, also no.

182
00:13:30.244 --> 00:13:34.157
Scrolling back down to
the negative prompt again,

183
00:13:34.157 --> 00:13:38.800
like we shouldn't see something too much,
I like that one.

184
00:13:38.800 --> 00:13:40.400
&gt;&gt; Student: It's the best one yet.

185
00:13:40.400 --> 00:13:44.337
&gt;&gt; Steve Kinney: Yeah, I am using a
slightly different model on that one, no,

186
00:13:44.337 --> 00:13:45.030
it chose.

187
00:13:48.897 --> 00:13:52.280
&gt;&gt; Steve Kinney: I wonder if I said like

188
00:13:58.705 --> 00:14:02.750
&gt;&gt; Steve Kinney: Because if you say
something like Renaissance oil painting,

189
00:14:02.750 --> 00:14:05.794
you owe it to yourself to do that,
to be clear.

190
00:14:05.794 --> 00:14:10.554
I feel bad doing this because like
the creator was like really salty when

191
00:14:10.554 --> 00:14:15.810
Sam Alton changed his profile picture
when he's made by like 4o or whatever.

192
00:14:15.810 --> 00:14:19.349
And like, I should probably just do
oil paintings or something like that,

193
00:14:19.349 --> 00:14:20.090
but, well.

194
00:14:22.600 --> 00:14:27.646
But at least these aren't very good,
with this cheap model that

195
00:14:27.646 --> 00:14:32.598
will run on a free GPU with all
the memory stuff that I'm doing,

196
00:14:32.598 --> 00:14:35.880
so I don't feel that bad with that model.

197
00:14:39.560 --> 00:14:44.570
Let's just do one more and let's grab.

198
00:14:48.953 --> 00:14:52.039
&gt;&gt; Steve Kinney: Let's actually do
that one so we don't have to pay,

199
00:14:52.039 --> 00:14:54.364
when I say pay, I just mean pay in time.

200
00:14:54.364 --> 00:15:02.424
Download a new model, we'll say like
&gt;&gt; Steve Kinney: But

201
00:15:02.424 --> 00:15:04.765
it's worth playing around
with both the prompts and

202
00:15:04.765 --> 00:15:08.168
the negative prompts to kind of figure
out the feel of getting what you want,

203
00:15:08.168 --> 00:15:11.250
and the rest of these too,
like make it smaller.

204
00:15:11.250 --> 00:15:13.637
Play around with the guidance scale and

205
00:15:13.637 --> 00:15:17.325
the number of inference steps
why I stopped at 28 was, some

206
00:15:17.325 --> 00:15:22.098
of these are the end result of towards the
end of an evening of preparing all this

207
00:15:22.098 --> 00:15:27.044
stuff when I was just like, I deserve to
make pictures of hippos for a little bit.

208
00:15:33.123 --> 00:15:38.220
&gt;&gt; Steve Kinney: New favorite,
not a Renaissance oil painting, though,

209
00:15:38.220 --> 00:15:42.910
so talking a little bit
about the attention slicing.

210
00:15:44.430 --> 00:15:48.502
If it's too big to fit on the table,
instead of doing the whole page at once,

211
00:15:48.502 --> 00:15:52.894
it'll try to break it into smaller pieces
and do each kind of chunk of it separately

212
00:15:52.894 --> 00:15:57.630
and pull it back together, because, again,
it's trying to denoise it effectively.

213
00:15:57.630 --> 00:16:01.255
So there's various ways to make it
a little bit more and if you want a very,

214
00:16:01.255 --> 00:16:05.115
you know, like, what are all these
models?, they honestly were just four or

215
00:16:05.115 --> 00:16:07.360
five different ones of
me scrolling around.

216
00:16:07.360 --> 00:16:11.600
I went to text image and just picked
lightweight ones and went for it.

217
00:16:11.600 --> 00:16:13.120
&gt;&gt; Student: Can you try watercolor?

218
00:16:13.120 --> 00:16:13.816
&gt;&gt; Steve Kinney: Yeah.

219
00:16:25.863 --> 00:16:28.808
&gt;&gt; Steve Kinney: And sometimes watercolor,
I would probably have better results,

220
00:16:28.808 --> 00:16:31.220
watercolor painting stuff,
you know what I mean?

221
00:16:31.220 --> 00:16:31.720
&gt;&gt; Student: Yeah.

222
00:16:33.060 --> 00:16:37.650
&gt;&gt; Steve Kinney: And the parts of live,
whatever you would call what I'm doing,

223
00:16:37.650 --> 00:16:43.700
that are not super interesting to watch
are definitely will get, like Absolutely.

224
00:16:43.700 --> 00:16:48.211
Like, if you've ever played with
something like Midjourney before or

225
00:16:48.211 --> 00:16:51.895
even ChatGPT's 4.0,
you definitely find that more so

226
00:16:51.895 --> 00:16:56.420
with text, there is definitely
a science to getting the prompt right.

227
00:16:59.550 --> 00:17:05.446
So let's try that out, and again,
these are on the free GPU, to be clear,

228
00:17:05.446 --> 00:17:11.623
with older models that I picked, now,
are they watercoloring super well?,

229
00:17:11.623 --> 00:17:15.950
does that look like a walrus?,
it's not important.

230
00:17:18.750 --> 00:17:22.902
Might I, at some point, go and
turn off some of the optimizations and

231
00:17:22.902 --> 00:17:26.696
grab a bigger model, and
we might see something, I might, but

232
00:17:26.696 --> 00:17:30.510
I want to show you some other
interesting things that we can do.

