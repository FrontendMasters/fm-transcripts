WEBVTT

1
00:00:00.240 --> 00:00:02.142
&gt;&gt; Steve Kinney: Another one we
talked about is again, I said before,

2
00:00:02.142 --> 00:00:04.440
like that text generation is
interesting because it is almost.

3
00:00:04.440 --> 00:00:07.939
And we'll see this visually in
a little bit with the text generation,

4
00:00:07.939 --> 00:00:11.793
it's only looking at the past and it's
making up the future as it goes along,

5
00:00:11.793 --> 00:00:13.520
just like most of us.

6
00:00:13.520 --> 00:00:15.040
Fill mask is an interesting one.

7
00:00:15.040 --> 00:00:18.080
Bert kind of acts differently than GPT.

8
00:00:18.080 --> 00:00:21.542
BERT's job is to look to the left and
look to the right, right?

9
00:00:21.542 --> 00:00:24.357
And use not only so
like a film mask is effectively,

10
00:00:24.357 --> 00:00:28.460
instead of coming up with what is
the next word, you're in the middle.

11
00:00:28.460 --> 00:00:32.540
You've got the words that come after,
you've got the words that come before.

12
00:00:32.540 --> 00:00:38.140
Given looking both directions,
what is the most likely fill in the blank?

13
00:00:38.140 --> 00:00:42.396
It's not necessarily Mad Libs,
because that would just be random, but

14
00:00:42.396 --> 00:00:46.180
this is trying to statistically
guess what the right mad lib is.

15
00:00:46.180 --> 00:00:47.420
Looking at the entire mad Lib.

16
00:00:47.420 --> 00:00:53.149
Before you give your nouns and
verbs, the one thing to be mindful

17
00:00:53.149 --> 00:00:58.390
of is depending on the model,
what they use as the mask.

18
00:00:58.390 --> 00:01:00.950
This is the token that is
like fill in the blank.

19
00:01:00.950 --> 00:01:03.526
You do need to look at the docs for
whichever one you're using and

20
00:01:03.526 --> 00:01:04.870
figure out what the right one is.

21
00:01:06.870 --> 00:01:09.070
Here's some strings that we can use for.

22
00:01:09.070 --> 00:01:12.125
Again, I chose purposely to
use the default models for

23
00:01:12.125 --> 00:01:15.990
all of these because one,
they are small and two, they are the ones.

24
00:01:15.990 --> 00:01:18.310
If you didn't specify something,
what you would get.

25
00:01:18.310 --> 00:01:21.000
It seemed like
the responsible thing to do.

26
00:01:21.000 --> 00:01:22.920
And so obviously we can have one at the.

27
00:01:22.920 --> 00:01:24.400
Is that actually not
the end of the sentence?

28
00:01:24.400 --> 00:01:30.840
Because that period is a token, but
given the context of the sentence.

29
00:01:30.840 --> 00:01:33.934
And again, when we get to attention and
transformers in a little bit,

30
00:01:33.934 --> 00:01:36.199
we'll actually see how
this works under the hood.

31
00:01:36.199 --> 00:01:37.800
But right now,
let's just see it in practice.

32
00:01:37.800 --> 00:01:44.920
First, Python is a popular blank language.

33
00:01:44.920 --> 00:01:47.680
Machine learning is a subset
of blank intelligence.

34
00:01:47.680 --> 00:01:50.786
The blank of gravity was
discovered by Newton and

35
00:01:50.786 --> 00:01:53.840
Shakespeare wrote the play Blank, right?

36
00:01:53.840 --> 00:01:57.071
And so it will use the context
of the words around it,

37
00:01:57.071 --> 00:01:59.760
which is not what GPT does, right?

38
00:01:59.760 --> 00:02:00.800
It's what Bert does.

39
00:02:02.880 --> 00:02:05.840
Why someone didn't make a model
called Ernie, I don't know.

40
00:02:08.880 --> 00:02:10.760
So we'll run this real quick
as it downloads the model.

41
00:02:10.760 --> 00:02:11.480
How big is this?

42
00:02:11.480 --> 00:02:13.320
331 megs.

43
00:02:13.320 --> 00:02:16.760
And so at this point we have it giving
a bunch of guesses as it goes along.

44
00:02:16.760 --> 00:02:21.214
So where I said top K is three,
that's why you will see three guesses and

45
00:02:21.214 --> 00:02:23.480
you're like, what is top K?

46
00:02:23.480 --> 00:02:24.400
There are slides.

47
00:02:24.400 --> 00:02:24.960
They're coming.

48
00:02:24.960 --> 00:02:25.800
I Promise you.

49
00:02:27.160 --> 00:02:33.222
But in this case, the good enough
right now answer is we're asking for

50
00:02:33.222 --> 00:02:40.120
the three most likely choices and this
is arguably for these very factual ones.

51
00:02:41.870 --> 00:02:44.670
We can change the argument for
this very factual one.

52
00:02:46.430 --> 00:02:49.585
You probably definitely don't want
to give it a lot of temperature and

53
00:02:49.585 --> 00:02:52.910
guesswork on what the capital
of France is, right?

54
00:02:52.910 --> 00:02:57.870
The thing it was the most confident about
by an order of magnitude was Paris.

55
00:02:57.870 --> 00:03:01.390
But if you tell it to keep going,
it's wrong.

56
00:03:01.390 --> 00:03:03.818
&gt;&gt; Speaker 2: How do you find
the right type of model for

57
00:03:03.818 --> 00:03:05.470
what you're trying to do?

58
00:03:05.470 --> 00:03:07.031
&gt;&gt; Steve Kinney: Yeah, so
if you go in there, you're looking for

59
00:03:07.031 --> 00:03:08.030
the right type model, right?

60
00:03:08.030 --> 00:03:11.847
Like a lot of times for obviously for what
you're trying to do, whether it's text

61
00:03:11.847 --> 00:03:15.540
generation, obviously there's 42
different tags or whatever, right?

62
00:03:15.540 --> 00:03:20.740
They are classified,
tagged in some way, shape or form.

63
00:03:23.380 --> 00:03:26.980
It's called tasks is
the term they're using.

64
00:03:28.100 --> 00:03:31.340
It doesn't feel like the right term,
but tasks is the term.

65
00:03:31.340 --> 00:03:36.062
So if you are just looking for all of
the zero shot classification models,

66
00:03:36.062 --> 00:03:40.934
you can go into hugging face, you go into
the models, you go into the task and

67
00:03:40.934 --> 00:03:46.113
you hit all right, here are the ones that
have been labeled that they are good at or

68
00:03:46.113 --> 00:03:49.040
tuned for zero shot image classifications.

69
00:03:49.040 --> 00:03:51.985
You can also pick like hey,
I want to do text to text and

70
00:03:51.985 --> 00:03:55.200
I want to do that's zero
shot image classification.

71
00:03:55.200 --> 00:03:56.960
If I wanted to do.

72
00:03:56.960 --> 00:04:00.375
I think there was a question answering
one document question answering,

73
00:04:00.375 --> 00:04:02.640
you will get the models
that are suited for that.

74
00:04:03.980 --> 00:04:05.827
And then like most libraries,

75
00:04:05.827 --> 00:04:10.100
it becomes a game of what vanity
metrics are you interested in, right?

76
00:04:10.100 --> 00:04:11.420
There are some very real vanity.

77
00:04:11.420 --> 00:04:12.340
Well, not vanity metrics.

78
00:04:12.340 --> 00:04:16.376
The real metrics of like how big is it,
how many parameters does it have,

79
00:04:16.376 --> 00:04:17.740
when was it updated?

80
00:04:17.740 --> 00:04:21.260
Then you start playing the game
of downloads and likes, right?

81
00:04:21.260 --> 00:04:25.685
In the same way, if I am looking for
what NPM package I'm going to use,

82
00:04:25.685 --> 00:04:30.260
sometimes it is open them all up on
npmjs.org which everyone has the most

83
00:04:30.260 --> 00:04:34.920
downloads and has been updated
recently is the one that I try first.

84
00:04:34.920 --> 00:04:39.560
But yeah, to the question of
how do I find other models?

85
00:04:39.560 --> 00:04:42.549
A lot of times they have been
kind of labeled and tagged for

86
00:04:42.549 --> 00:04:45.240
that given I guess task is
the right word after all.

87
00:04:46.600 --> 00:04:52.440
It doesn't feel very fulfilling as
being the answer, but allegedly.

88
00:04:52.440 --> 00:04:56.072
But yeah, you go in there, you can find
it for what you're looking for and

89
00:04:56.072 --> 00:04:59.761
then you play a game of whether you want
to do it by trending most downloads,

90
00:04:59.761 --> 00:05:02.397
most likes, recently updated,
so on and so forth and

91
00:05:02.397 --> 00:05:06.614
kind of do that piece after that size and
again, if nobody's using it, especially in

92
00:05:06.614 --> 00:05:10.100
a world where anyone can pull down
a model, fine tune it and push it up.

93
00:05:13.140 --> 00:05:16.100
You have nothing wrong with that,
but you probably want the base ones.

94
00:05:16.100 --> 00:05:20.617
In a lot of cases I think you can usually
see if it is forked from another model,

95
00:05:20.617 --> 00:05:21.740
so on and so forth.

96
00:05:21.740 --> 00:05:23.220
Yeah, the model tree.

97
00:05:23.220 --> 00:05:26.340
And you can see what models it comes from,
so on and so forth.

98
00:05:28.660 --> 00:05:32.597
So this is one that is pretty popular.

99
00:05:32.597 --> 00:05:33.620
No, not that one per se.

100
00:05:33.620 --> 00:05:37.000
But you can see where they're forced from,
so on and so forth.

101
00:05:37.000 --> 00:05:37.600
But then it becomes.

102
00:05:37.600 --> 00:05:39.640
Yeah, like I said before,
size obviously matters.

103
00:05:39.640 --> 00:05:42.472
Memory footprint parameters is gonna
be more than memory footprint,

104
00:05:42.472 --> 00:05:45.544
which is going to be the limiting factor
before download size is ever gonna be

105
00:05:45.544 --> 00:05:46.600
the thing that hurts you.

106
00:05:47.800 --> 00:05:49.880
And then it becomes the usage.

107
00:05:49.880 --> 00:05:54.571
And the way you would evaluate any library
but task is apparently the right answer to

108
00:05:54.571 --> 00:05:57.800
that question mark just
doesn't feel very fulfilling.

109
00:05:59.560 --> 00:06:03.560
Cool, so, yeah, we've got the various
things that it fills in here.

110
00:06:03.560 --> 00:06:06.860
As you can see, Python is the most
popular Python language, but

111
00:06:06.860 --> 00:06:08.890
it did give that incredibly low score.

112
00:06:08.890 --> 00:06:10.833
So you would set thresholds obviously,

113
00:06:10.833 --> 00:06:13.930
which again can just become
a greater than, a less than symbol.

114
00:06:13.930 --> 00:06:18.866
In your code, the law of gravity
was governed by Newton's theory,

115
00:06:18.866 --> 00:06:24.330
center of gravity, but again they get
increasingly lower as time goes on.

116
00:06:28.970 --> 00:06:33.980
That one I'm at,
it's not a lot of text in a tiny model.

117
00:06:33.980 --> 00:06:35.243
I don't feel good about that one, but

118
00:06:35.243 --> 00:06:37.940
it doesn't feel particularly better about
that one than the other ones either.

119
00:06:37.940 --> 00:06:42.670
So there's that a part of
this is like I said before,

120
00:06:42.670 --> 00:06:48.380
the GPT models were not necessarily
trained to look both ways.

121
00:06:48.380 --> 00:06:51.460
The BERT family of models were
trained to look both ways.

122
00:06:51.460 --> 00:06:54.778
And some of that both is in terms of
the training debt, the training data, and

123
00:06:54.778 --> 00:06:56.580
then also the architecture of the model.

124
00:06:57.860 --> 00:07:02.565
When we see the GPT models later,
effectively one of the things they do

125
00:07:02.565 --> 00:07:07.690
is when you look at the score of
a word and how closely it is related.

126
00:07:07.690 --> 00:07:12.490
The GPT models intentionally
just nerf every future word.

127
00:07:12.490 --> 00:07:14.730
So they're all given
a negative infinity score.

128
00:07:14.730 --> 00:07:17.313
And that's how you keep it
from looking forward and

129
00:07:17.313 --> 00:07:20.810
only thinking about the previous
words as you generate the next one.

130
00:07:20.810 --> 00:07:24.999
The BERT ones are kind of set up to weight
things, so you look on both sides and

131
00:07:24.999 --> 00:07:26.250
get the right answer.

132
00:07:26.250 --> 00:07:30.555
And so that's where the art of that, like
why some models are good at some things

133
00:07:30.555 --> 00:07:34.282
and some are not, is they are either
the architecture of that model or

134
00:07:34.282 --> 00:07:36.870
the tuning of said model
is what gets you there.

135
00:07:39.750 --> 00:07:41.110
Cool.

136
00:07:41.110 --> 00:07:42.550
Then we've got summarization.

137
00:07:42.550 --> 00:07:45.670
This one kind of does
what you think it does.

138
00:07:47.110 --> 00:07:49.819
You can do stuff like the max length and
the min length and

139
00:07:49.819 --> 00:07:53.670
stuff along those lines, but
it will take strings of text.

140
00:07:53.670 --> 00:07:56.550
It will take the parameters you set,
it will do the thing.

