WEBVTT

1
00:00:00.160 --> 00:00:04.230
&gt;&gt; Steve Kinney: If we go back
to the site that we had earlier,

2
00:00:04.230 --> 00:00:09.621
this is our second little
notebook here on tokenization.

3
00:00:09.621 --> 00:00:13.115
So we actually see this with real code.

4
00:00:13.115 --> 00:00:18.131
So there's some of the words that I
said earlier and we can kind of get

5
00:00:18.131 --> 00:00:23.513
a sense we can actually grab a bunch
of different models of hugging face.

6
00:00:23.513 --> 00:00:28.310
And we just ask them like yo,
like what's your vocab size?

7
00:00:28.310 --> 00:00:29.110
Right?

8
00:00:29.110 --> 00:00:33.649
And So I grabbed Bert,
GPT2 and Roberta and

9
00:00:33.649 --> 00:00:38.434
T5 and
I basically went through all of them and

10
00:00:38.434 --> 00:00:42.007
I said how many tokens do you have?

11
00:00:42.007 --> 00:00:42.637
Right?

12
00:00:42.637 --> 00:00:47.513
And this auto tokenizer before
when we saw pipeline it was all

13
00:00:47.513 --> 00:00:50.390
of the pieces of the chain, right?

14
00:00:50.390 --> 00:00:53.440
The tokenizer, the,
the model weights everything.

15
00:00:53.440 --> 00:00:56.711
In this case I'm just
grabbing the tokenizer.

16
00:00:56.711 --> 00:00:59.296
So I'm using this auto tokenizer
which is just kind of also

17
00:00:59.296 --> 00:01:01.160
getting whatever other accoutrements.

18
00:01:01.160 --> 00:01:03.600
I'm not wiring everything
up myself by hand.

19
00:01:03.600 --> 00:01:06.400
But it's not the full
pipeline in this case.

20
00:01:06.400 --> 00:01:09.680
And you can see that it is going to
pull those down for me right now.

21
00:01:11.440 --> 00:01:13.360
It's got my libraries,
we're pulling those in.

22
00:01:14.560 --> 00:01:20.260
We'll have them momentarily as we spin
up our little disk, so on and so forth.

23
00:01:20.260 --> 00:01:21.060
Here they come.

24
00:01:23.380 --> 00:01:29.860
You can see it pulling down the vocab,
JSON, all the fun files and here we go.

25
00:01:29.860 --> 00:01:31.860
It's almost like that slide
I showed you earlier.

26
00:01:33.139 --> 00:01:34.660
I wrote code to figure it out.

27
00:01:36.260 --> 00:01:37.860
So here you can see,
you can grab all of them.

28
00:01:37.860 --> 00:01:39.380
They are different sizes.

29
00:01:39.380 --> 00:01:42.465
What that means to you on a practical
day to day level is not much,

30
00:01:42.465 --> 00:01:44.800
just the fact that models are different.

31
00:01:44.800 --> 00:01:47.560
One tokenizer is not the same
as another tokenizer.

32
00:01:47.560 --> 00:01:54.320
They are all usually at least
somewhat bespoke to that given model.

33
00:01:54.320 --> 00:01:56.160
So we got those vocabularies and

34
00:01:56.160 --> 00:02:00.000
then what would it look like
to start tokenizing things?

35
00:02:00.000 --> 00:02:04.160
And so here you can change between the
different tokenizers if you want to see.

36
00:02:05.680 --> 00:02:07.440
So we take this string.

37
00:02:07.440 --> 00:02:09.280
You can also change the string
to whatever you want.

38
00:02:11.300 --> 00:02:12.020
We'll hit play.

39
00:02:14.260 --> 00:02:16.780
And so first of all it breaks
it up into the tokens.

40
00:02:16.780 --> 00:02:19.540
And that's what Bert's
tokenization looks like.

41
00:02:19.540 --> 00:02:22.450
Hello, we got the comma,
the world exclamation point.

42
00:02:22.450 --> 00:02:25.620
Tokenization is the yadda yadda yadda.

43
00:02:25.620 --> 00:02:28.244
You don't need me to read you
a sentence you can read, but

44
00:02:28.244 --> 00:02:29.940
one by one in tokens, right?

45
00:02:29.940 --> 00:02:33.780
We can do the same thing again
if we want to Change it to GPT2.

46
00:02:34.990 --> 00:02:39.100
And you can see that GPT2 has
a slightly different way of

47
00:02:39.100 --> 00:02:43.070
demarcating the division between things.

48
00:02:43.070 --> 00:02:45.314
It's probably Just a special,

49
00:02:45.314 --> 00:02:49.310
like ASCII character that
renders weird as it spits out.

50
00:02:49.310 --> 00:02:52.670
The important part here, of course,
is that they are slightly different.

51
00:02:52.670 --> 00:02:53.953
Right.
T5 does it.

52
00:02:53.953 --> 00:02:58.722
Even the way that T5 broke
up the words is different.

53
00:02:58.722 --> 00:02:59.352
Right.
And

54
00:02:59.352 --> 00:03:03.466
let's just look at one more to
further drive the point home.

55
00:03:03.466 --> 00:03:05.968
So Roberto just looks
a lot closer to GPTs.

56
00:03:05.968 --> 00:03:10.988
Again, that is most likely
a special ASCII code that

57
00:03:10.988 --> 00:03:15.791
happens to just render into
a G with a XM arc on top.

58
00:03:15.791 --> 00:03:16.404
And you can change it out.

59
00:03:16.404 --> 00:03:17.368
You try different things.

60
00:03:17.368 --> 00:03:18.721
You can experiment, you can.

61
00:03:18.721 --> 00:03:27.360
Like if we go back to one that
looked a little bit better for us.

62
00:03:27.360 --> 00:03:31.162
All right, so those count as full words
because they're probably common enough.

63
00:03:31.162 --> 00:03:35.296
Tokenization in the real world,
when you're not talking about AI models,

64
00:03:35.296 --> 00:03:39.508
probably doesn't come up nearly as much
as swimming does in the body of work.

65
00:03:39.508 --> 00:03:43.294
The vocabulary is effectively a set
of those suffixes and core words.

66
00:03:43.294 --> 00:03:47.828
But some common enough words are going
to come up as we go through.

67
00:03:47.828 --> 00:03:51.240
You can try out different ones if
you're curious of how they end up.

68
00:03:51.240 --> 00:03:53.000
Anyone else want to try one for funsies?

69
00:03:56.160 --> 00:03:57.520
You look like you have one
on the top of your head.

70
00:03:57.520 --> 00:03:58.498
&gt;&gt; Speaker 3: I have a question.

71
00:03:58.498 --> 00:03:59.284
&gt;&gt; Steve Kinney: All right, let's do it.

72
00:03:59.284 --> 00:04:00.280
Let's switch to a question.

73
00:04:00.280 --> 00:04:01.040
What do you got?

74
00:04:01.040 --> 00:04:04.029
&gt;&gt; Speaker 3: I'm curious, is there a case
where you'd ever go between tokenized

75
00:04:04.029 --> 00:04:05.840
format to a different tokenized format?

76
00:04:05.840 --> 00:04:08.080
Or is best practice always
go from text to token?

77
00:04:08.080 --> 00:04:13.032
&gt;&gt; Steve Kinney: I mean, most of the time
on the way into a model, as you notice,

78
00:04:13.032 --> 00:04:16.309
these tokenizers look like model names.

79
00:04:16.309 --> 00:04:17.142
Right?

80
00:04:17.142 --> 00:04:18.196
Right.
And so they are.

81
00:04:18.196 --> 00:04:21.457
Usually you cannot use tokens
from Bert with GPT, right.

82
00:04:21.457 --> 00:04:22.449
So no.

83
00:04:23.569 --> 00:04:25.089
Was there ever a reason?

84
00:04:25.089 --> 00:04:29.089
I'm not gonna say there's never a reason,
but no, there's probably not a reason.

85
00:04:30.369 --> 00:04:33.449
&gt;&gt; Speaker 2: What does it do with
something that's not in its vocabulary?

86
00:04:33.449 --> 00:04:38.369
Like if you just put in a made up word or
just like ASDF or something.

87
00:04:38.369 --> 00:04:40.209
&gt;&gt; Steve Kinney: So this one will
probably still get the whole word.

88
00:04:40.209 --> 00:04:43.765
I wonder when we get to the decoding part,
if we actually get a value for

89
00:04:43.765 --> 00:04:45.169
it, if we end up with some.

90
00:04:47.240 --> 00:04:48.040
That's interesting.

91
00:04:49.560 --> 00:04:50.520
&gt;&gt; Speaker 4: That is interesting.

92
00:04:50.520 --> 00:04:51.960
&gt;&gt; Steve Kinney: That's fascinating.

93
00:04:51.960 --> 00:04:54.204
There's certain things that you never
think about until you're teaching and

94
00:04:54.204 --> 00:04:55.360
somebody asks you a question like that.

95
00:04:55.360 --> 00:04:56.809
I'm like, I don't know.

96
00:04:56.809 --> 00:04:57.602
That's a.

97
00:04:57.602 --> 00:04:59.805
Now what I want to do is stop teaching for
the day and

98
00:04:59.805 --> 00:05:01.760
now go play around with
a bunch of decoders.

99
00:05:01.760 --> 00:05:02.679
I'm not going to do that right now.

100
00:05:02.679 --> 00:05:04.040
But I'm probably going to do that tonight.

101
00:05:05.880 --> 00:05:09.439
That's going to live rent free in my head.

102
00:05:09.439 --> 00:05:10.795
Yeah, go.

103
00:05:10.795 --> 00:05:13.650
&gt;&gt; Speaker 3: The higher token count
typically mean it's a better model.

104
00:05:14.850 --> 00:05:16.290
&gt;&gt; Steve Kinney: Sure.

105
00:05:16.290 --> 00:05:19.486
You know, it's like one of those things
where it's like does it being bigger and

106
00:05:19.486 --> 00:05:21.810
more parameters make it a better model?

107
00:05:21.810 --> 00:05:23.410
Usually.

108
00:05:23.410 --> 00:05:25.250
But like not always.

109
00:05:25.250 --> 00:05:25.810
Right.

110
00:05:25.810 --> 00:05:30.283
Like it is a not wrong heuristic
that I'm sure there are tons of

111
00:05:30.283 --> 00:05:32.010
counter examples for.

112
00:05:32.010 --> 00:05:32.850
You know what I mean?

113
00:05:35.170 --> 00:05:37.820
So like possibly.

114
00:05:37.820 --> 00:05:40.608
But keep in mind then the bigger
the model, the harder it is to fine tune.

115
00:05:40.608 --> 00:05:41.504
You know what I mean?

116
00:05:41.504 --> 00:05:42.540
Yes.

117
00:05:42.540 --> 00:05:43.580
Ish.

118
00:05:43.580 --> 00:05:47.215
The problem with computer
science is that nothing is easy,

119
00:05:47.215 --> 00:05:50.500
everything is hard and
most rules have an exception.

120
00:05:50.500 --> 00:05:53.164
That's really annoying.

121
00:05:53.164 --> 00:05:54.580
And then from tokens to IDs.

122
00:05:54.580 --> 00:05:57.603
So here we've got one from earlier.

123
00:05:57.603 --> 00:05:58.876
I'm actually going to.

124
00:05:58.876 --> 00:06:01.141
I'll probably try to remember
to change this later.

125
00:06:01.141 --> 00:06:06.110
If I do param and I do type string.

126
00:06:07.630 --> 00:06:09.470
I know, I know, I'm getting there.

127
00:06:10.590 --> 00:06:12.133
I can change it here really easily.

128
00:06:12.133 --> 00:06:13.871
Could it have been just as
easy changing the code?

129
00:06:13.871 --> 00:06:16.490
Absolutely, but here we are,

130
00:06:16.490 --> 00:06:22.150
we can run this through where we
can see it turn into those numbers.

131
00:06:22.150 --> 00:06:27.390
Which now raises the question of
what happens with my garbage word.

132
00:06:33.960 --> 00:06:37.557
I need to like break this into like other
like I need to like have all these things

133
00:06:37.557 --> 00:06:40.625
in one thing so I could see
the transformation all the way through.

134
00:06:40.625 --> 00:06:45.080
Because those are probably those like
weird pieces that we saw earlier.

135
00:06:45.080 --> 00:06:46.678
I don't know.
This is gonna live run free in my head.

136
00:06:46.678 --> 00:06:47.577
I don't have an answer for you.

137
00:06:47.577 --> 00:06:51.358
But I will unfortunately to
everyone in my family's chagrin,

138
00:06:51.358 --> 00:06:54.440
probably have one shortly
because that's gonna.

139
00:06:54.440 --> 00:06:59.089
Yeah, I'm gonna be thinking
about that while I talk now.

140
00:06:59.089 --> 00:06:59.803
Yeah.

141
00:06:59.803 --> 00:07:04.682
And so but to for the core point before
because as you can see I'm still

142
00:07:04.682 --> 00:07:09.395
spending cycles on this now is
obviously we split apart the words,

143
00:07:09.395 --> 00:07:11.354
we turn them into numbers.

144
00:07:11.354 --> 00:07:17.320
Ideally the final step is to turn
those numbers back into words, right.

145
00:07:17.320 --> 00:07:18.100
Actually hold on.

146
00:07:18.100 --> 00:07:20.380
So I think I have the way I did
this because they're all in memory.

147
00:07:20.380 --> 00:07:22.510
So I've got the token IDs.

148
00:07:22.510 --> 00:07:26.750
Those I think in memory
are still is garbage.

149
00:07:26.750 --> 00:07:28.510
And so then what happens?

150
00:07:30.750 --> 00:07:31.959
It did manage to put it back.

151
00:07:31.959 --> 00:07:35.441
I guess like at a certain point
the letters are in there is like my guess

152
00:07:35.441 --> 00:07:36.590
is at a certain point.

153
00:07:38.030 --> 00:07:41.339
&gt;&gt; Speaker 3: And certain pairs of letters
get put together as a single token for

154
00:07:41.339 --> 00:07:41.950
whatever.

155
00:07:41.950 --> 00:07:43.670
&gt;&gt; Steve Kinney: Reason that would be.

156
00:07:43.670 --> 00:07:47.150
My guess is that like at some level
individual letters are tokens.

157
00:07:48.440 --> 00:07:53.929
There's enough of the common character
yeah, let's go back up and look.

158
00:07:53.929 --> 00:07:57.260
Yeah, because in this case,
like just L and

159
00:07:57.260 --> 00:08:00.760
the idea that it had stuff before,
it just.

160
00:08:00.760 --> 00:08:06.379
Yeah, these have probably got broken
up into tokens in those vocabs.

161
00:08:06.379 --> 00:08:07.720
I probably can't do much with this.

162
00:08:07.720 --> 00:08:10.518
They're probably all like related
to almost nothing in that sequence.

163
00:08:10.518 --> 00:08:14.565
But I think at a certain point there
are small enough granular pieces in there

164
00:08:14.565 --> 00:08:17.560
because if you think about all of the.

165
00:08:17.560 --> 00:08:21.368
Obviously initial letters only get you to
like 26 and probably enough of the common

166
00:08:21.368 --> 00:08:24.840
combinations because a lot of times
it did fall back to a single letter.

167
00:08:24.840 --> 00:08:30.246
Like of the 50,000 probably
don't take up that much space.

168
00:08:30.246 --> 00:08:31.240
Right.

169
00:08:31.240 --> 00:08:33.400
So we all learned
something together today.

170
00:08:33.400 --> 00:08:35.600
And that's why having these
notebooks is super cool though.

171
00:08:35.600 --> 00:08:37.120
The idea that you can just like.

172
00:08:37.120 --> 00:08:40.636
I don't know, let's like run the code and

173
00:08:40.636 --> 00:08:46.309
see is super fun with these colab
jupyter notebooks as we go through.

174
00:08:47.669 --> 00:08:48.471
Cool.

175
00:08:48.471 --> 00:08:50.309
So let's see.

176
00:08:50.309 --> 00:08:51.309
Let's turn this back.

177
00:08:51.309 --> 00:08:55.542
Just my own.

178
00:08:55.542 --> 00:08:56.369
Just so I don't like.

179
00:08:56.369 --> 00:09:01.338
I don't remember exactly if that will bite
me by having garbage words in there at

180
00:09:01.338 --> 00:09:02.163
some point.

181
00:09:02.163 --> 00:09:04.589
So yeah,
here we can say we've got that again.

182
00:09:04.589 --> 00:09:06.309
That's the beginning.

183
00:09:06.309 --> 00:09:10.300
That is the separator in this case,
if I remember correctly.

184
00:09:10.300 --> 00:09:10.900
Hello world.

185
00:09:14.660 --> 00:09:21.540
I think it will just be
around the actual again.

186
00:09:21.540 --> 00:09:22.180
Yeah, it does.

187
00:09:22.180 --> 00:09:24.356
Like the separation is just for
that given thing.

188
00:09:24.356 --> 00:09:28.531
What you think about is like the prompts
in a chatgpt are then separated by like,

189
00:09:28.531 --> 00:09:30.100
this is one thing.

190
00:09:30.100 --> 00:09:32.625
And then you know, again,
in the chatbot ones,

191
00:09:32.625 --> 00:09:37.030
they are trained with special tokens
to delineate what role it was.

192
00:09:37.030 --> 00:09:39.034
But it kind of shows the start and

193
00:09:39.034 --> 00:09:43.785
end of a given thing which when we do
the fine tuning, we'll see roughly when

194
00:09:43.785 --> 00:09:48.784
a quote should start and end and stuff
along those lines and the format of it.

195
00:09:48.784 --> 00:09:50.950
Look, apparently previous me took notes.

196
00:09:53.030 --> 00:09:54.550
If truly.

197
00:09:54.550 --> 00:09:59.998
If you truly found a way to put something
in there that it does not know at all,

198
00:09:59.998 --> 00:10:04.090
you will get a special token for
an unknown vocab word.

199
00:10:04.090 --> 00:10:04.639
Cool.
And

200
00:10:04.639 --> 00:10:07.679
we saw like that might happen sometime
later because like with these,

201
00:10:07.679 --> 00:10:10.730
these are when we try to match two of
them is where we put the padding in.

202
00:10:10.730 --> 00:10:11.379
Right.

203
00:10:11.379 --> 00:10:13.614
So it'd be interesting to see.

204
00:10:13.614 --> 00:10:14.376
I got it.
Like,

205
00:10:14.376 --> 00:10:16.054
I'm pretty sure you can visualize it.

206
00:10:16.054 --> 00:10:17.756
I just have to get creative later and

207
00:10:17.756 --> 00:10:19.924
see if I can come up with
a way to visualize it.

208
00:10:19.924 --> 00:10:22.675
&gt;&gt; Speaker 4: So I just went to look
up like how many words are in English

209
00:10:22.675 --> 00:10:26.072
language and the entry from like
Merriam Webster says, it's like,

210
00:10:26.072 --> 00:10:29.990
it's hard to estimate, but
it's like could be roughly a million.

211
00:10:29.990 --> 00:10:32.642
Right.
And these tokens are like 50,000 of them.

212
00:10:32.642 --> 00:10:34.070
&gt;&gt; Steve Kinney: Yeah.

213
00:10:34.070 --> 00:10:38.788
So it'd be curious, I don't know
if we know the token counts of

214
00:10:38.788 --> 00:10:43.774
like Claude Opus or whatever like
these are if someone wants to like

215
00:10:43.774 --> 00:10:48.512
live research the Release date of GPT2,
you know what I mean?

216
00:10:48.512 --> 00:10:53.053
Which was none of us were like AIs
changing everything with GPT2,

217
00:10:53.053 --> 00:10:54.523
you know what I mean?

218
00:10:54.523 --> 00:10:59.363
These are also several hundred megabyte
models versus like I think we saw

219
00:10:59.363 --> 00:11:01.517
like hugging face or whatever.

220
00:11:01.517 --> 00:11:06.767
Like some of them are like I have models
on my computer that are 46 gigabytes

221
00:11:06.767 --> 00:11:12.510
like my suspicion is a 46-gigabyte model
most likely has a larger vocabulary and

222
00:11:12.510 --> 00:11:14.165
those are open source.

223
00:11:14.165 --> 00:11:15.952
We could probably figure it out.

224
00:11:15.952 --> 00:11:21.740
I just probably nobody wants to watch me
download a 46 gigabyte model right now.

225
00:11:21.740 --> 00:11:26.509
That said, well, not on the plane, but at
some point that'd be interesting to kind

226
00:11:26.509 --> 00:11:29.550
of see because there
are large open-source models.

227
00:11:29.550 --> 00:11:32.348
Just because we can't see Claude opus or

228
00:11:32.348 --> 00:11:37.634
Gemini 2.5 Pro does not mean we can't
pull down like meta's libraries.

229
00:11:37.634 --> 00:11:40.288
Their models are open source, right?

230
00:11:40.288 --> 00:11:45.057
So you can grab like llama like 3,
the 105 billion parameter 1.

231
00:11:45.057 --> 00:11:47.590
And I'm curious to see
what its vocab size.

232
00:11:47.590 --> 00:11:51.740
I mean I can't run that on
my laptop unfortunately, but

233
00:11:51.740 --> 00:11:55.641
I'd be curious to see what,
what its vocab size is.

234
00:11:55.641 --> 00:11:58.230
&gt;&gt; Speaker 4: Have you
played with Deep Seek like.

235
00:11:58.230 --> 00:12:00.390
&gt;&gt; Steve Kinney: Like on a consumer level.

236
00:12:00.390 --> 00:12:01.190
Do you know what I mean?

237
00:12:01.190 --> 00:12:04.950
Have I like downloaded deepseek and
like spun it up and talked to it?

238
00:12:04.950 --> 00:12:05.510
Yes.

239
00:12:05.510 --> 00:12:07.111
Right.
I haven't like gone down into

240
00:12:07.111 --> 00:12:07.936
the weeds with it.

241
00:12:07.936 --> 00:12:10.864
Like I've done the I chat to it.

242
00:12:10.864 --> 00:12:13.430
It reasons,
it gives me answers level of it.

243
00:12:13.430 --> 00:12:17.270
But like not on a, like let's pull
it down and like pull it apart.

244
00:12:17.270 --> 00:12:21.915
Right but like some of the larger
open-source ones are somewhat interesting

245
00:12:21.915 --> 00:12:23.820
to do that with, right.

246
00:12:23.820 --> 00:12:27.088
Where you can just like yeah, you can
pull down this 20-gigabyte model and

247
00:12:27.088 --> 00:12:28.051
you can like one load.

248
00:12:28.051 --> 00:12:30.121
There are some apps like LM Studio or

249
00:12:30.121 --> 00:12:34.399
whatever where they pull down from
hugging face and they'll pull down and

250
00:12:34.399 --> 00:12:37.504
they'll give you like
a desktop app that looks like.

251
00:12:37.504 --> 00:12:40.374
It's your average electron app and
like load them up and

252
00:12:40.374 --> 00:12:42.340
you can tweak a lot of the parameters.

253
00:12:42.340 --> 00:12:45.615
Like how many like CPU cores do
you want to let it use what,

254
00:12:45.615 --> 00:12:49.793
what is the temperature and top K and
top P, which we'll see in a second.

255
00:12:49.793 --> 00:12:51.445
And you can chat with them.

256
00:12:51.445 --> 00:12:53.904
And it'll put the chat interface and

257
00:12:53.904 --> 00:12:58.430
it'll actually put a OpenAI compliant
API layer on top of them too.

258
00:12:58.430 --> 00:13:02.043
So you can actually take
any of these larger 20, 40,

259
00:13:02.043 --> 00:13:06.032
60 gig models, depending on again,
the size of the RAM and or

260
00:13:06.032 --> 00:13:10.196
GPU will be the limiting factor,
not the space on your computer.

261
00:13:10.196 --> 00:13:14.058
A model that is maybe 8 gigabytes is still
going to take up like 32 gigs of RAM or

262
00:13:14.058 --> 00:13:16.770
something like that,
depending on how you tune it, but

263
00:13:16.770 --> 00:13:19.210
those would be interesting
to pull apart as well.

