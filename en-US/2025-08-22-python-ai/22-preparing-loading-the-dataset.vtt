WEBVTT

1
00:00:00.400 --> 00:00:08.640
&gt;&gt; Steve Kinney: So we're gonna go into
this fine tuning notebook and C onnect.

2
00:00:08.640 --> 00:00:10.080
I have too many sessions.

3
00:00:10.080 --> 00:00:11.440
You might have gotten this earlier.

4
00:00:11.440 --> 00:00:13.040
So I'll show you what to do.

5
00:00:13.040 --> 00:00:13.840
Manage sessions.

6
00:00:13.840 --> 00:00:15.680
I've opened up many
a notebook at this point.

7
00:00:16.880 --> 00:00:18.560
Terminate other sessions.

8
00:00:24.640 --> 00:00:29.770
We'll go ahead and
we're gonna hit connect.

9
00:00:32.330 --> 00:00:36.490
I am actually probably gonna grab a.

10
00:00:36.490 --> 00:00:42.126
I'm gonna change the runtime
personally just for

11
00:00:42.126 --> 00:00:47.770
all of our sakes and
grab a beefier graphics card.

12
00:00:47.770 --> 00:00:50.793
Just simply like this will
work probably in like five or

13
00:00:50.793 --> 00:00:53.010
ten minutes on the cheaper one.

14
00:00:53.010 --> 00:00:55.490
Again, not bad considering what it is.

15
00:00:55.490 --> 00:00:56.610
But I don't wanna wait that long.

16
00:00:57.970 --> 00:00:59.570
So we talked a little about this.

17
00:00:59.570 --> 00:01:02.210
This is my own notes
before I made the slides.

18
00:01:02.210 --> 00:01:04.050
You do need to be on a GPU for this one.

19
00:01:04.050 --> 00:01:06.998
So if you are like,
you should at least check, go to runtime,

20
00:01:06.998 --> 00:01:08.849
change runtime, type.

21
00:01:08.849 --> 00:01:11.890
You probably only have CPU and T4 GPU.

22
00:01:11.890 --> 00:01:13.970
Choose the T4 GPU.

23
00:01:13.970 --> 00:01:18.359
I'm gonna sit on an A100 because
I'm worth it cuz I pay the 10 or

24
00:01:18.359 --> 00:01:20.290
$20 a month in it account.

25
00:01:21.890 --> 00:01:24.210
And honestly you're the real
beneficiaries of this.

26
00:01:24.210 --> 00:01:27.810
So go ahead on this other runtime.

27
00:01:27.810 --> 00:01:30.050
I'm just going to make sure
we've got everything downloaded.

28
00:01:30.050 --> 00:01:34.326
All right, so first we are going to,
quote, unquote, find and

29
00:01:34.326 --> 00:01:40.050
prepare our data which I grabbed this data
set which is just quotes from people.

30
00:01:40.050 --> 00:01:44.212
So it's like a quote from Oscar Wilde,
a quote from Albert Einstein,

31
00:01:44.212 --> 00:01:46.010
a quote from Dustin.

32
00:01:46.010 --> 00:01:48.890
So we're gonna grab this data set.

33
00:01:48.890 --> 00:01:53.850
If I was a nicer person to myself,
I would have linked to it at the time.

34
00:01:53.850 --> 00:01:59.376
But let's go and

35
00:01:59.376 --> 00:02:06.090
we can go and we can.

36
00:02:09.770 --> 00:02:11.300
No, I don't want a model.

37
00:02:11.300 --> 00:02:12.500
I want the data set.

38
00:02:12.500 --> 00:02:13.220
There it is.

39
00:02:14.660 --> 00:02:17.500
And so in Hugging Face you can
actually kinda see the data set.

40
00:02:17.500 --> 00:02:21.938
So this data set is effectively
a spreadsheet where, you know,

41
00:02:21.938 --> 00:02:26.786
it's probably JSON really, but
we've got a quote from Oscar Wilde,

42
00:02:26.786 --> 00:02:30.500
the author and
some tags I'm not actually gonna use.

43
00:02:31.540 --> 00:02:35.380
I'm just gonna use the quotes and whatnot.

44
00:02:35.380 --> 00:02:39.520
And what I would say as a challenge to
you that you should 100% do is I grabbed

45
00:02:39.520 --> 00:02:41.420
this one cause it was there to grab.

46
00:02:41.420 --> 00:02:44.760
Seem there are a ton of data sets.

47
00:02:44.760 --> 00:02:48.400
So you can literally grab any
data set you want, right?

48
00:02:49.840 --> 00:02:52.160
And find a different one.

49
00:02:52.160 --> 00:02:54.800
There are no shortage of data sets.

50
00:02:54.800 --> 00:02:55.920
Go here, just go to text.

51
00:02:57.680 --> 00:03:02.560
There are 300,000 different data sets.

52
00:03:02.560 --> 00:03:04.480
Grab one that makes you happy.

53
00:03:04.480 --> 00:03:08.810
You can find out what
the cauldron Is whatever.

54
00:03:08.810 --> 00:03:10.170
No idea, never clicked on it.

55
00:03:13.770 --> 00:03:17.450
But I grabbed this English quotes
because it's the first one I saw.

56
00:03:18.490 --> 00:03:21.090
And I'm gonna put them in this format.

57
00:03:21.090 --> 00:03:23.705
I'm gonna say Quote by
(author): (quote) and

58
00:03:23.705 --> 00:03:26.090
then I'm gonna say end of statement,
right?

59
00:03:26.090 --> 00:03:27.850
And that's gonna help the model predict.

60
00:03:27.850 --> 00:03:29.050
I should stop talking now.

61
00:03:29.050 --> 00:03:32.970
Cause that is a token that I'm
including in there, right?

62
00:03:32.970 --> 00:03:37.040
And I've got the data set.

63
00:03:37.040 --> 00:03:38.560
We're going to load the data set in.

64
00:03:40.080 --> 00:03:43.920
You can see I showed you what
the first one of the JSON is.

65
00:03:45.360 --> 00:03:47.200
We've got the quote, we've got the author.

66
00:03:47.200 --> 00:03:48.760
I don't actually care about the tags.

67
00:03:48.760 --> 00:03:49.600
They're there though.

68
00:03:52.000 --> 00:03:57.577
Then for the data preparation, all I'm
doing is I'm taking this JSON object and

69
00:03:57.577 --> 00:04:01.550
I'm turning it into a string
that looks like this.

70
00:04:01.550 --> 00:04:05.470
When we say prepare the data,
I'm just making a string.

71
00:04:05.470 --> 00:04:07.230
That's what I mean by prepare the data.

72
00:04:07.230 --> 00:04:11.034
We go ahead, we make a string and
we can go ahead and

73
00:04:11.034 --> 00:04:14.030
effectively we're just going to map.

74
00:04:16.190 --> 00:04:18.910
I got to run all my cells,
otherwise they're not in memory.

75
00:04:21.470 --> 00:04:22.350
That one ran.

76
00:04:22.350 --> 00:04:24.110
Little checkbox knows that they ran.

77
00:04:24.110 --> 00:04:24.990
That one ran.

78
00:04:26.760 --> 00:04:27.320
There we go.

79
00:04:29.640 --> 00:04:30.680
So it ran through.

80
00:04:33.553 --> 00:04:35.640
No, it's only 2500.

81
00:04:35.640 --> 00:04:37.960
That bigger number is how
many I did per second.

82
00:04:40.440 --> 00:04:43.720
And so we ran through all those,
we just formatted them as strings.

83
00:04:43.720 --> 00:04:47.560
So actually this is a relatively
small data set, right.

84
00:04:47.560 --> 00:04:51.047
I remember when I think I was
just glancing at it earlier,

85
00:04:51.047 --> 00:04:53.250
I just didn't look hard enough.

86
00:04:53.250 --> 00:04:56.370
That's the number of records that 2805.

87
00:04:56.370 --> 00:04:57.890
That's just how fast it went.

88
00:04:57.890 --> 00:05:04.456
So really we're only taking a relatively
small Data set of 2,500 pieces of

89
00:05:04.456 --> 00:05:10.930
strings and we are just going to hammer
at GPT2 medium until it gets the point.

90
00:05:13.010 --> 00:05:17.707
So we'll pull in GPT2 medium and
I'm gonna like,

91
00:05:17.707 --> 00:05:21.870
I'm doing some,
I have the fancy pants gpu.

92
00:05:21.870 --> 00:05:23.510
I'm doing some quantization.

93
00:05:23.510 --> 00:05:24.630
What is quantization?

94
00:05:24.630 --> 00:05:29.190
That is those numbers can be 32 bit or
whatever.

95
00:05:29.190 --> 00:05:31.670
We're gonna say that you're
all gonna be four bit.

96
00:05:31.670 --> 00:05:36.054
So you lose some granularity in
that giant space of how many

97
00:05:36.054 --> 00:05:40.150
knobs you have to turn and
stuff along those lines.

98
00:05:40.150 --> 00:05:43.230
But you can fit it in smaller amounts.

99
00:05:43.230 --> 00:05:49.294
So if you grab an open source model,
like a big boy model, a big girl model or

100
00:05:49.294 --> 00:05:54.590
big person model, like Llama 3 or
whatever, and you're like,

101
00:05:54.590 --> 00:06:00.950
I want all the parameters and
your MacBook's like, absolutely not.

102
00:06:00.950 --> 00:06:03.923
You can get like at 16 bit,
8 bit, 4 bit and

103
00:06:03.923 --> 00:06:07.830
you will lose some fidelity
because that's how that works.

104
00:06:09.030 --> 00:06:10.510
But you can fit it in memory.

105
00:06:10.510 --> 00:06:13.750
So some of this stuff is to
keep you on the free tier.

106
00:06:14.950 --> 00:06:20.570
So I'll like bring it down to four bits,
so on and so forth.

107
00:06:20.570 --> 00:06:21.490
I'm trying to think, whatever.

108
00:06:21.490 --> 00:06:22.241
All right, so

109
00:06:22.241 --> 00:06:26.450
then I will pull in our model name which
in this case was gpt2-medium, right?

110
00:06:26.450 --> 00:06:27.770
Again, I'm not using the full pipeline.

111
00:06:27.770 --> 00:06:31.327
I'm just using this like get me the stuff
I need for a pre trained model for

112
00:06:31.327 --> 00:06:32.970
the quantization.

113
00:06:32.970 --> 00:06:34.530
This is a library called bits and bytes.

114
00:06:34.530 --> 00:06:37.930
It does the quantization
trust remote code.

115
00:06:37.930 --> 00:06:38.970
It's not my machine.

116
00:06:40.810 --> 00:06:42.690
And we'll pull it in.

117
00:06:42.690 --> 00:06:43.690
We won't use the cache.

118
00:06:43.690 --> 00:06:45.580
Great.

119
00:06:45.580 --> 00:06:48.110
This is a method that's just pulling
the model and getting it for me.

120
00:06:48.110 --> 00:06:49.540
You could just grab the model normally.

121
00:06:49.540 --> 00:06:52.220
But if I run this more than once,
things are bad.

122
00:06:52.220 --> 00:06:56.445
So I'm just getting a new one every time
because I was running it more than once

123
00:06:56.445 --> 00:06:57.100
last week.

124
00:06:59.900 --> 00:07:01.500
We don't need a cache.

125
00:07:01.500 --> 00:07:05.571
Then we're gonna get the tokenizer of the
same model because we need to turn it into

126
00:07:05.571 --> 00:07:06.540
the same tokens.

127
00:07:06.540 --> 00:07:10.220
So a pipeline will get you the tokenizer,
it'll get you the model.

128
00:07:10.220 --> 00:07:12.020
Do all the encoding and encoding here.

129
00:07:12.020 --> 00:07:19.600
I've grabbed the model and
I've got the tokenizer for it.

130
00:07:19.600 --> 00:07:24.432
And we know that we can see
what that tokenizer's got to

131
00:07:24.432 --> 00:07:29.600
run this code,
what its end of statement token is.

132
00:07:29.600 --> 00:07:32.400
And so I'll actually print it for
a second here too.

133
00:07:32.400 --> 00:07:38.116
So for GPT2, the end of text token or
the end of statement

134
00:07:38.116 --> 00:07:43.250
token is this very heavy
metal looking end of text.

135
00:07:45.410 --> 00:07:51.330
Our sample data effectively will really
actually be that heavy metal looking one.

136
00:07:51.330 --> 00:07:56.330
But we're going to keep feeding
it 2500 sentences that look like

137
00:07:56.330 --> 00:07:59.010
this that is going to turn the knobs.

138
00:08:00.050 --> 00:08:02.690
So if it sees Quote by.

139
00:08:02.690 --> 00:08:07.906
We've already started to
prime the statistical model

140
00:08:07.906 --> 00:08:13.130
to start going in the direction
that we want it to go in.

141
00:08:13.130 --> 00:08:16.810
Effectively we'll go through.

142
00:08:19.050 --> 00:08:19.610
We get them all.

143
00:08:19.610 --> 00:08:21.290
We've seen all of this.

144
00:08:21.290 --> 00:08:26.130
There's my own notes and
we have again some of what's going on.

145
00:08:26.130 --> 00:08:31.297
This data_collater basically breaks
it up into smaller pieces so

146
00:08:31.297 --> 00:08:34.220
that we load it on there in chunks.

147
00:08:34.220 --> 00:08:38.380
A lot of this is again the hardest part of
this was like keeping it on the free tier.

148
00:08:38.380 --> 00:08:38.940
Honestly.

