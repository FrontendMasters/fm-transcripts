WEBVTT

1
00:00:00.160 --> 00:00:04.320
&gt;&gt; Steve Kinney: The next one is
called zero shot classification.

2
00:00:06.320 --> 00:00:10.460
And this is basically looking at strings
of text and maybe having a bunch

3
00:00:10.460 --> 00:00:14.800
of categories and figuring out how
to correctly categorize stuff.

4
00:00:14.800 --> 00:00:18.736
So if you wanted to take either
a piece of content of some sort and

5
00:00:18.736 --> 00:00:22.377
have a set of tags and
be able to automatically tag stuff and

6
00:00:22.377 --> 00:00:27.358
classify stuff, which could work for
the very first thing that probably popped

7
00:00:27.358 --> 00:00:32.180
into all of our heads, which is like,
I could tag blog posts with that.

8
00:00:32.180 --> 00:00:34.100
Yes, you could.

9
00:00:34.100 --> 00:00:37.540
That was the first thing that popped
into my head, so I'm not judging you.

10
00:00:37.540 --> 00:00:39.860
Still the first thing that
popped in my head today.

11
00:00:39.860 --> 00:00:42.742
The other things you can
do is start to figure out,

12
00:00:42.742 --> 00:00:45.831
okay, could we then fine tune a model,
figure out for

13
00:00:45.831 --> 00:00:49.885
the data coming into our applications
that's probably fraudulent,

14
00:00:49.885 --> 00:00:54.580
that's probably bad in some way, shape or
form or unwanted in our application.

15
00:00:55.630 --> 00:00:58.945
And when we start to put some
of these other things together,

16
00:00:58.945 --> 00:01:02.520
plus zero shop classification
plus fine tuning a model, again,

17
00:01:02.520 --> 00:01:05.770
the interesting part is no,
given one topic that we cover,

18
00:01:05.770 --> 00:01:10.909
the interesting part is the kind of wild
intersections amongst multiple parts.

19
00:01:10.909 --> 00:01:13.150
So you can apply one label,
multiple labels.

20
00:01:13.150 --> 00:01:16.270
We'll see all this stuff
when we play around with it.

21
00:01:16.270 --> 00:01:21.403
The next one that we'll kind of
look at in our first tour of,

22
00:01:21.403 --> 00:01:26.537
of machine learning,
AI what have you is if text generation

23
00:01:26.537 --> 00:01:31.670
is guess the next word,
a fill mask is almost like mad libs,

24
00:01:31.670 --> 00:01:36.700
right where there's a blank
in the sentence somewhere and

25
00:01:36.700 --> 00:01:42.655
it uses both the words that came before
it and the words that come after

26
00:01:42.655 --> 00:01:49.260
to provide enough context to make a solid
guess on what the next best word is.

27
00:01:51.580 --> 00:01:54.601
And so effectively,
if you think about ChatGPT and

28
00:01:54.601 --> 00:01:57.622
all these other things
with your text generation,

29
00:01:57.622 --> 00:02:02.610
they're almost tying one hand behind their
back, which is you have the ability with

30
00:02:02.610 --> 00:02:06.940
something like film mask to look
before that word and after that word.

31
00:02:06.940 --> 00:02:10.725
But if you're just trying to generate
text and not necessarily fill in pieces,

32
00:02:10.725 --> 00:02:14.172
then obviously looking forward,
you don't have a forward to look at and

33
00:02:14.172 --> 00:02:16.410
you might not even want
to look at forward.

34
00:02:16.410 --> 00:02:17.680
So you only wanna look it back.

35
00:02:17.680 --> 00:02:22.640
And we'll actually see how that works
when we get to like tokenization and

36
00:02:22.640 --> 00:02:27.520
embeddings and all of the like inner
workings and plumbing of a lot of this

37
00:02:27.520 --> 00:02:34.000
stuff in the second ish chapter, arguably,
if sentiment analysis is the hello world.

38
00:02:34.000 --> 00:02:39.287
Summarization is probably like the first
thing that we think of with a lot of the,

39
00:02:39.287 --> 00:02:42.395
you know, the original,
prior to, you know,

40
00:02:42.395 --> 00:02:47.682
all the LLMs tools of just like taking
long pieces of text, making shorter ones,

41
00:02:47.682 --> 00:02:52.170
then you play a game called Brevity
versus Information Retention.

42
00:02:52.170 --> 00:02:57.130
You make it too short, you've sucked out
all the meaning and it doesn't matter.

43
00:02:57.130 --> 00:03:00.250
And if you make it too long,
then why did you do this?

44
00:03:01.850 --> 00:03:03.850
So it becomes an interesting
game there as well.

45
00:03:03.850 --> 00:03:05.765
But you have all those parameters and

46
00:03:05.765 --> 00:03:10.170
knobs that you can play along with,
play around with, so on and whatnot.

47
00:03:10.170 --> 00:03:12.760
And then the other one,
which is super cool.

48
00:03:12.760 --> 00:03:15.120
Actually, I don't necessarily.

49
00:03:15.120 --> 00:03:20.087
I can be like,
I have an immediate use case for this one,

50
00:03:20.087 --> 00:03:24.633
but we could probably riff and
come up with a bunch,

51
00:03:24.633 --> 00:03:32.120
which is take a look at some text and
let's figure out all of the nouns, right?

52
00:03:32.120 --> 00:03:38.120
You can figure out who are all the people,
who are all the places, organizations.

53
00:03:38.120 --> 00:03:41.960
There's just large corpuses of
data that help you just pull out.

54
00:03:41.960 --> 00:03:47.420
Okay, is Jimmy Carter mentioned in this?

55
00:03:47.420 --> 00:03:51.180
You can pull out effectively all of
the various different things I mentioned.

56
00:03:51.180 --> 00:03:55.660
Use that again and think about how that
fills in with some of these other tools.

57
00:03:55.660 --> 00:04:00.380
And the last one, which we won't touch
too much today, is just translation.

58
00:04:00.380 --> 00:04:07.100
And there are a whole bunch of models
out there that obviously can power this.

59
00:04:07.100 --> 00:04:12.374
The nice part about those is
that the names of the models

60
00:04:12.374 --> 00:04:17.200
tend to suggest exactly
what those models might.

61
00:04:17.200 --> 00:04:18.520
The translations that they do.

62
00:04:18.520 --> 00:04:26.800
For instance, you can take a lucky
guess what the translation en2de does.

63
00:04:29.520 --> 00:04:31.920
It translates from English to German.

64
00:04:34.160 --> 00:04:37.760
And so a lot of those tools as well,
I think are super interesting.

