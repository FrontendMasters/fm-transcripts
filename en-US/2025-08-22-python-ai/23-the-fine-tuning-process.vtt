WEBVTT

1
00:00:00.480 --> 00:00:00.990
&gt;&gt; Steve Kinney: Cool.
And so

2
00:00:00.990 --> 00:00:03.040
we know how many parameters do we have?

3
00:00:04.080 --> 00:00:08.932
So our layers, we're gonna train

4
00:00:08.932 --> 00:00:13.600
of the 355 million again.

5
00:00:13.600 --> 00:00:18.476
If you were to get the tiniest
open source model today,

6
00:00:18.476 --> 00:00:21.663
you would like, [SOUND] 7 billion.

7
00:00:21.663 --> 00:00:23.760
That's a weak model, right?

8
00:00:27.510 --> 00:00:30.350
Yeah, like the tiny ones,
I'll show you LLM Studio in a little bit.

9
00:00:30.350 --> 00:00:34.468
You can see the big LLMs on hugging
face that you can download and

10
00:00:34.468 --> 00:00:38.910
easily run on your MacBook today
are in the billions of parameters.

11
00:00:38.910 --> 00:00:42.470
This one is 355 million parameters.

12
00:00:42.470 --> 00:00:46.390
But also, does anyone wanna
watch me do this for six hours?

13
00:00:48.230 --> 00:00:52.150
And we're only gonna train
a subset of those, right?

14
00:00:52.150 --> 00:00:53.350
With our extra data.

15
00:00:55.550 --> 00:01:00.302
So we've got this library from Hugging
Face where we can pull in this parameter

16
00:01:00.302 --> 00:01:04.190
efficient fine tuning with that model and
this configuration.

17
00:01:07.790 --> 00:01:11.070
The task type is that
causallm that we saw before.

18
00:01:11.070 --> 00:01:15.790
Some other little settings that we
can talk about if we're interested.

19
00:01:17.150 --> 00:01:19.240
Then we tokenize.

20
00:01:19.240 --> 00:01:21.140
Basically each example,

21
00:01:21.140 --> 00:01:25.640
we pad it to whatever the maximum
length is of that tokenizer.

22
00:01:25.640 --> 00:01:27.640
So we want them all to be the same length.

23
00:01:27.640 --> 00:01:29.737
So this is going to be like
whatever the quote is,

24
00:01:29.737 --> 00:01:31.280
I don't care how long the quote is.

25
00:01:31.280 --> 00:01:33.160
The quote is how long the quote is.

26
00:01:33.160 --> 00:01:37.240
And then pad it with that extra space so
they're all equal.

27
00:01:39.640 --> 00:01:41.080
And so put that in place.

28
00:01:43.240 --> 00:01:44.899
And this is what like our first,

29
00:01:44.899 --> 00:01:48.290
like this is what the first record
is going to end up looking like.

30
00:01:50.130 --> 00:01:55.530
So we can see that this is our
blank tokens at this point, right?

31
00:01:55.530 --> 00:01:56.610
So all the stuff we saw before.

32
00:01:56.610 --> 00:02:01.880
So this quote from Oscar Wilde is going
to be quote by Oscar Wilde, colon,

33
00:02:01.880 --> 00:02:07.890
space, open quote, the quote,
close quote, end of statement, right?

34
00:02:07.890 --> 00:02:11.180
And so you can see the tokens.

35
00:02:11.180 --> 00:02:13.500
It breaks it up into the various tokens.

36
00:02:13.500 --> 00:02:14.420
It's like cool.

37
00:02:14.420 --> 00:02:15.820
It was this long.

38
00:02:15.820 --> 00:02:18.220
We don't actually care
about the rest of them.

39
00:02:18.220 --> 00:02:22.940
They're all these BS tokens which
are slightly different for each one.

40
00:02:22.940 --> 00:02:25.444
If we chose a different model and
a different tokenizer,

41
00:02:25.444 --> 00:02:27.540
the I don't care token
is probably different.

42
00:02:27.540 --> 00:02:29.260
This is the padding token in this case.

43
00:02:31.820 --> 00:02:37.620
And so that is effectively what the,
that is the number version of this string.

44
00:02:37.620 --> 00:02:40.850
Because this is the text that we made
really when we mapped everything.

45
00:02:42.290 --> 00:02:46.182
And then we just say, basically,
here's all the fancy numbers,

46
00:02:46.182 --> 00:02:50.770
I'm going to feed them into you and
you're going to go for it.

47
00:02:50.770 --> 00:02:54.605
So the trainer API has a training
loop where we evaluate it, we log it,

48
00:02:54.605 --> 00:02:56.690
we do some checkpoints along the way.

49
00:02:58.130 --> 00:02:59.730
These are all the arguments
we're going to take.

50
00:02:59.730 --> 00:03:02.610
We're gonna write it to a file
called gpt2-medium-quotes.

51
00:03:03.970 --> 00:03:06.130
We're gonna do one full
pass through the data.

52
00:03:07.450 --> 00:03:09.290
You can do more full passes if you want.

53
00:03:10.330 --> 00:03:14.439
Obviously doing more full passes with
a law of diminishing returns is good,

54
00:03:14.439 --> 00:03:17.210
but then we gotta wait for
that and I don't want to.

55
00:03:18.330 --> 00:03:19.210
So play with this.

56
00:03:19.210 --> 00:03:20.330
Of course, right?

57
00:03:21.610 --> 00:03:24.010
We don't necessarily need larger
batches cuz everything's small.

58
00:03:26.330 --> 00:03:27.170
Yeah, these are.

59
00:03:27.170 --> 00:03:31.934
I think I was just tweaking stuff,
trying to make stuff that it wasn't

60
00:03:31.934 --> 00:03:36.779
particularly long because again,
if you are doing it for production or

61
00:03:36.779 --> 00:03:41.464
for something that you're trying to do,
letting fine tuning taking

62
00:03:41.464 --> 00:03:46.330
30 minutes is not the end of
the world to have it forever.

63
00:03:46.330 --> 00:03:49.192
If people are watching you on a Friday,

64
00:03:49.192 --> 00:03:53.050
Dustin would like to
go home at some point.

65
00:03:53.050 --> 00:03:55.850
We do the shorter version for
the fine tuning.

66
00:03:55.850 --> 00:03:58.720
How does the model know which
parameters are tunable?

67
00:03:58.720 --> 00:04:02.960
Well, that's the cool part is because
we're freezing the model, right?

68
00:04:02.960 --> 00:04:05.040
We're not actually fine tuning the model.

69
00:04:05.040 --> 00:04:08.401
We're putting like a layer of
effectively zeroed out layer,

70
00:04:08.401 --> 00:04:12.680
like a few layers of zeroed out stuff on
top of it and fine tuning those, right?

71
00:04:12.680 --> 00:04:14.800
And so it's just effectively like.

72
00:04:14.800 --> 00:04:17.751
And that's the nice part,
you don't have to do the whole model and

73
00:04:17.751 --> 00:04:21.560
you don't have to even do a subset and the
underlying model doesn't change, right?

74
00:04:21.560 --> 00:04:24.000
And so
that's what makes it like pluggable.

75
00:04:24.000 --> 00:04:27.930
We can take that off and you don't have to
store a whole second version of the model.

76
00:04:27.930 --> 00:04:30.130
You can kind of just
keep that piece to it.

77
00:04:33.330 --> 00:04:36.170
Yeah, so we train this.

78
00:04:36.170 --> 00:04:37.996
And again like I think
it took like five or

79
00:04:37.996 --> 00:04:40.610
10 minutes on when I did on the free one.

80
00:04:40.610 --> 00:04:44.050
But for our own sanity I bumped up to
a bigger one where it took two minutes.

81
00:04:44.050 --> 00:04:47.043
And like, if it is truly like
you are trying to train a model,

82
00:04:47.043 --> 00:04:49.334
get like stuff in a different format or
style,

83
00:04:49.334 --> 00:04:53.250
whatever, honestly, three hours would
be fine, half an hour would be fine.

84
00:04:53.250 --> 00:04:54.770
Right, like whatever.

85
00:04:54.770 --> 00:05:00.286
But four, for the purpose of all of us
in a room today I went with a relatively

86
00:05:00.286 --> 00:05:05.470
small model with a relatively small
data set on a relatively fast GPU.

87
00:05:05.470 --> 00:05:09.900
But I did this on that T4 and it's yeah,
go make yourself a cup of coffee or

88
00:05:09.900 --> 00:05:12.630
something and
it'll be done and not too bad.

89
00:05:12.630 --> 00:05:16.515
And like you could also, if you've
got a little more patience and again,

90
00:05:16.515 --> 00:05:20.350
if you've got like a gaming machine,
go do it on your Computer, right?

91
00:05:20.350 --> 00:05:28.020
Absolutely, on a M2 Pro I didn't
have the patience for that.

92
00:05:28.020 --> 00:05:29.340
But you can do it right?

93
00:05:29.340 --> 00:05:31.220
Like easily.

94
00:05:31.220 --> 00:05:33.060
But like you can also like there are many,

95
00:05:33.060 --> 00:05:35.780
many a service out there that
will rent you a GPU these days.

96
00:05:35.780 --> 00:05:42.740
Turns out that is a widely invested
in part of the ecosystem for reasons.

97
00:05:42.740 --> 00:05:45.100
So we did train this model and
like it's saved.

98
00:05:45.100 --> 00:05:49.120
You know where we have this GB two
Medium quotes, final whatever.

99
00:05:51.600 --> 00:05:54.604
As you can tell I'm still a JavaScript
engineer because I did not use underscores

100
00:05:54.604 --> 00:05:56.560
in my naming convention
because I don't like them.

101
00:05:58.320 --> 00:06:01.487
So that's the telltale sign, that and
when I can't tell the difference between

102
00:06:01.487 --> 00:06:03.920
integers and floats in most
languages is another way to tell.

103
00:06:05.280 --> 00:06:10.404
Okay, so what I'm gonna do is I've
got the base model where we're gonna

104
00:06:10.404 --> 00:06:15.440
have a pipeline base generator and
we're gonnasay text generation.

105
00:06:15.440 --> 00:06:18.058
We're gonna give it gpt2-medium and

106
00:06:18.058 --> 00:06:22.430
the tokenizer which I probably
could have left out but I didn't.

107
00:06:23.870 --> 00:06:27.824
And we'll look at the result for
one sequence and

108
00:06:27.824 --> 00:06:32.744
we'll look at the response and
then we'll go to our fine tune

109
00:06:32.744 --> 00:06:37.590
model where I pass it in directly
here with a few more stuff.

110
00:06:37.590 --> 00:06:39.030
Low CPU memory usage.

111
00:06:39.030 --> 00:06:41.670
Again you can play around
with some of this.

112
00:06:41.670 --> 00:06:46.589
You'll obviously get better results if
you don't do all of the please don't

113
00:06:46.589 --> 00:06:51.370
blow through the free tier that I did but
also I did that for you so whatever.

114
00:06:53.450 --> 00:06:55.210
Cool, cool, cool.

115
00:06:55.210 --> 00:06:58.490
And yeah, we grab the fine tuned model.

116
00:06:58.490 --> 00:07:01.458
Yeah because I'm loading from
memory when you put the string in

117
00:07:01.458 --> 00:07:05.210
Hugging Face Transformer library grabs
from Hugging Face knows how to do that.

118
00:07:05.210 --> 00:07:09.473
Obviously I could theoretically if I
wanted to push this model up that I made

119
00:07:09.473 --> 00:07:11.650
up to Hugging Face and you would have.

120
00:07:11.650 --> 00:07:15.020
My model didn't seem worth it.

121
00:07:15.020 --> 00:07:16.660
Happy to do it if somebody really cares.

122
00:07:18.340 --> 00:07:23.208
But so it could just be a model on Hugging
Face and that's the GitHub aspect of

123
00:07:23.208 --> 00:07:27.100
Hugging Face where you could
fine tune a model to something.

124
00:07:27.100 --> 00:07:29.530
You could push it up,
somebody else could fork it,

125
00:07:29.530 --> 00:07:32.420
fine tune it in a different way, right?

126
00:07:32.420 --> 00:07:35.540
It's like the fun days of GitHub, right?

127
00:07:35.540 --> 00:07:38.740
Not that GitHub's not fun, but
GitHub is very much a utility.

128
00:07:38.740 --> 00:07:41.441
We all take it for
granted but it seemed new and

129
00:07:41.441 --> 00:07:44.370
interesting that you could fork code and
repos.

130
00:07:46.130 --> 00:07:47.090
You can still do that.

131
00:07:47.090 --> 00:07:50.182
It just doesn't seem as amazing
anymore just cuz we're used to it,

132
00:07:50.182 --> 00:07:51.170
we're dead inside.

133
00:07:53.410 --> 00:07:54.930
That's a pre generated one.

134
00:07:54.930 --> 00:07:56.090
That's from last time I ran that.

135
00:07:56.090 --> 00:07:58.850
That's my newest, latest and
greatest fine tuned one.

136
00:07:58.850 --> 00:08:00.370
So let's rerun that.

137
00:08:03.650 --> 00:08:08.300
We'll see the difference where.

138
00:08:08.300 --> 00:08:10.786
That's the original one where okay,

139
00:08:10.786 --> 00:08:15.300
it starts with again I wrote quote
by Bob Dylan was my initial prompt.

140
00:08:15.300 --> 00:08:17.220
So it's picking up from there.

141
00:08:17.220 --> 00:08:19.260
So quote by Bob Dylan comes from me.

142
00:08:19.260 --> 00:08:24.566
Everything after that comes from
the model where you can see like okay,

143
00:08:24.566 --> 00:08:26.700
it doesn't know the colon.

144
00:08:28.780 --> 00:08:33.569
I mean it plausibly seems like a quote,
but

145
00:08:33.569 --> 00:08:37.490
it's not keeping that format.

146
00:08:37.490 --> 00:08:40.755
And it's kind of like rambly and
going on and

147
00:08:40.755 --> 00:08:46.370
probably until it hits the max tokens,
if it ever hits the max tokens.

148
00:08:46.370 --> 00:08:50.290
And then like I don't even
know what it's talking about.

149
00:08:50.290 --> 00:08:51.010
Right.

150
00:08:51.010 --> 00:08:55.461
Versus the one that came into my fine
tune model where if you think about

151
00:08:55.461 --> 00:09:00.439
the string that I formatted that first
Oscar Wilde quote that we saw where it's

152
00:09:00.439 --> 00:09:03.985
like quote by Oscar Wilde,
colon, open parentheses,

153
00:09:03.985 --> 00:09:08.080
like a reasonable size quote,
end your conversation, right?

154
00:09:09.680 --> 00:09:13.419
Even before, I think some of the stuff
we were doing before wasn't like there

155
00:09:13.419 --> 00:09:15.430
weren't a lot of those end of, you know,

156
00:09:15.430 --> 00:09:18.320
end of sentence, end of whatever,
end of segment tokens.

157
00:09:18.320 --> 00:09:21.505
So even when I said hey, listen for
the end of segment token and stop talking,

158
00:09:21.505 --> 00:09:22.800
GPT2 was still rambling.

159
00:09:22.800 --> 00:09:26.790
But now it has been pretty well trained.

160
00:09:26.790 --> 00:09:29.714
All those quotes are pretty short cause
they're just quotes that you would see on

161
00:09:29.714 --> 00:09:30.710
an inspirational poster.

162
00:09:32.150 --> 00:09:36.870
And then they all have that end of
sequence token which causes it to stop.

163
00:09:36.870 --> 00:09:41.841
And so you can see that
two minutes of training on

164
00:09:41.841 --> 00:09:47.433
2,500 lines of an open
source data set radically

165
00:09:47.433 --> 00:09:52.790
changed how that relatively
small model behaved.

166
00:09:54.180 --> 00:09:56.180
All right, so who's tired of text?

167
00:09:57.860 --> 00:09:58.740
Is everyone bored with?

168
00:09:58.740 --> 00:09:59.820
Dustin's bored with text.

169
00:09:59.820 --> 00:10:01.460
Everyone's bored with text.

170
00:10:01.460 --> 00:10:02.660
Who wants to make images?

171
00:10:04.180 --> 00:10:05.540
Dustin wants to make images.

