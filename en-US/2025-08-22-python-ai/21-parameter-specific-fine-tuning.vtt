WEBVTT

1
00:00:00.080 --> 00:00:01.906
&gt;&gt; Steve Kinney: LoRa is like
if you took that textbook and

2
00:00:01.906 --> 00:00:05.285
you shoved a bunch of sticky notes on
the pages with some extra context, but

3
00:00:05.285 --> 00:00:06.960
you didn't rewrite the whole book.

4
00:00:06.960 --> 00:00:10.392
You're only taking
a small layer of them and

5
00:00:10.392 --> 00:00:15.495
you're fine tuning those and
it turns out low drink adaptation,

6
00:00:15.495 --> 00:00:21.363
that's the part I forgot where you
don't touch most of the knobs, right?

7
00:00:21.363 --> 00:00:27.330
You add a few tiny extra layers on top and
you just tune those, right?

8
00:00:27.330 --> 00:00:30.410
You're, that seems like
that's not gonna work.

9
00:00:30.410 --> 00:00:31.450
I don't think so.

10
00:00:33.290 --> 00:00:37.610
Turns out that it works like 90% as good,
right?

11
00:00:38.970 --> 00:00:42.090
It's one of those things where it's,
much to everyone's surprise kinda thing.

12
00:00:43.130 --> 00:00:46.759
It's the same laws that gave us
the fact that JavaScript became

13
00:00:46.759 --> 00:00:49.850
the most popular programming
language in the world.

14
00:00:49.850 --> 00:00:54.879
You ask anyone to predict that in 1995,
you're not gonna get a lot of takers.

15
00:00:54.879 --> 00:00:57.371
Turns out it works, and so basically,

16
00:00:57.371 --> 00:01:02.051
you can get to the point where just
taking a small subset of extra layers and

17
00:01:02.051 --> 00:01:05.393
tuning them will get you most
of where you need to go.

18
00:01:05.393 --> 00:01:07.610
And this is what we're
gonna do in a second.

19
00:01:07.610 --> 00:01:10.844
We're gonna take GPT2,
we're going to take a data set,

20
00:01:10.844 --> 00:01:14.738
we're gonna train it on that data set and
we're gonna see that it, for

21
00:01:14.738 --> 00:01:17.400
the amount of effort
that we're gonna put in.

22
00:01:17.400 --> 00:01:18.680
Is it gonna be perfect?

23
00:01:18.680 --> 00:01:23.860
It's not gonna be perfect, for
the time spent doing it, that's very good.

24
00:01:25.620 --> 00:01:30.401
So yeah, we take a relatively
small end very specific, right,

25
00:01:30.401 --> 00:01:35.540
again a chat tone, right, a style or
something like that, right?

26
00:01:35.540 --> 00:01:40.737
And effectively, hey,
the pattern of the first

27
00:01:40.737 --> 00:01:45.870
few words are gonna be the same, right?

28
00:01:45.870 --> 00:01:48.872
And we're gonna hit it
with 16,000 examples,

29
00:01:48.872 --> 00:01:51.870
which is not a lot if you think about it,
right?

30
00:01:51.870 --> 00:01:56.910
On any scale, we're hit with 16,000
examples, 14,000 maybe examples.

31
00:01:56.910 --> 00:02:01.602
Just repeatedly saying quote by name,
colon, beginning,

32
00:02:01.602 --> 00:02:04.990
quote, some words, end quote, right?

33
00:02:04.990 --> 00:02:07.190
And hammer it with just 16,000.

34
00:02:07.190 --> 00:02:08.830
That's not a huge amount.

35
00:02:08.830 --> 00:02:14.046
And we'll see that should we give it
that initial first few characters,

36
00:02:14.046 --> 00:02:17.430
it will mostly stay in line, right?

37
00:02:17.430 --> 00:02:19.270
And we're not retraining the entire model.

38
00:02:19.270 --> 00:02:25.889
We're simply adding a few layers
of extra knobs on top of it and

39
00:02:25.889 --> 00:02:31.490
we'll see that we get mostly
the way there, right,

40
00:02:31.490 --> 00:02:38.230
full fine tuning will still
most of the times outperform.

41
00:02:38.230 --> 00:02:41.661
But if it's one of those
always this trade off, okay,

42
00:02:41.661 --> 00:02:45.700
maybe it's 10% better, but
it costs you 28 times as much.

43
00:02:46.740 --> 00:02:51.060
I don't know, sometimes that's
the right answer, do it right on.

44
00:02:53.140 --> 00:02:57.327
And the nice part though is
when you fine tune a model,

45
00:02:57.327 --> 00:03:02.261
let's say you took a model that
clocked in at 48 gigabytes and

46
00:03:02.261 --> 00:03:04.700
you fine tune it, guess what?

47
00:03:04.700 --> 00:03:10.880
You have a new 48 gigabyte
model with these extra layers.

48
00:03:12.160 --> 00:03:14.720
You only have to store the extra layers.

49
00:03:14.720 --> 00:03:19.193
It's, you're putting a hat on top of
GPT2 that like fine tune it to do what

50
00:03:19.193 --> 00:03:20.240
you want.

51
00:03:20.240 --> 00:03:22.680
And then you can put that
hat on something else.

52
00:03:22.680 --> 00:03:24.560
You can take the hat off, right?

53
00:03:24.560 --> 00:03:26.040
It's just an extra plugin.

54
00:03:26.040 --> 00:03:28.080
It's effectively plug ins for
a model, right?

55
00:03:28.080 --> 00:03:32.330
And so you can fine tune it way cheaper,
way faster.

56
00:03:32.330 --> 00:03:34.490
You don't have to store
an entirely new model.

57
00:03:34.490 --> 00:03:36.245
You just have an extra layer and

58
00:03:36.245 --> 00:03:39.290
adapter on top of it to do
the thing that you want to do.

