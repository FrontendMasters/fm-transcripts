WEBVTT

1
00:00:00.053 --> 00:00:02.394
&gt;&gt; Scott Moss: This is the part
that I struggle with the most.

2
00:00:02.394 --> 00:00:04.526
And it makes sense, because this
is the part that's most valuable.

3
00:00:04.526 --> 00:00:08.958
If this is what you were good at,
is deciding what metrics to use and

4
00:00:08.958 --> 00:00:12.360
how to put those metrics,
you would be employed for

5
00:00:12.360 --> 00:00:15.940
a long time,
because this is the hard part, I think.

6
00:00:15.940 --> 00:00:16.531
So, yeah,

7
00:00:16.531 --> 00:00:20.459
none of this really matters if you don't
really know what you're measuring.

8
00:00:20.459 --> 00:00:24.644
And again, people are just really,
it's so funny, it's like,

9
00:00:24.644 --> 00:00:28.774
people are getting really creative
with how they measure stuff.

10
00:00:28.774 --> 00:00:29.795
But at the end of the day,

11
00:00:29.795 --> 00:00:32.962
there's basically three different
types of measurements that I've seen.

12
00:00:32.962 --> 00:00:36.902
There's LLM-based measurements,
where you use an LLM at the end.

13
00:00:36.902 --> 00:00:41.573
I think it's called just like,
AI as a judge, or LLM as a judge,

14
00:00:41.573 --> 00:00:45.205
where you use some form of
an LLM with some prompt or

15
00:00:45.205 --> 00:00:48.854
multi-turn LLM to grade for
some type of metric.

16
00:00:48.854 --> 00:00:54.326
So it might be, let's say you have an LLM
whose job is to take in some messages and

17
00:00:54.326 --> 00:00:55.398
summarize it.

18
00:00:55.398 --> 00:00:56.387
How would you measure that?

19
00:00:56.387 --> 00:01:00.945
Well, a really good metric would be, I
wanna take the output of this summary, and

20
00:01:00.945 --> 00:01:03.402
I wanna take all
the messages that I gave it.

21
00:01:03.402 --> 00:01:07.664
And then I wanna use another LLM to look
at those messages in this summary to

22
00:01:07.664 --> 00:01:11.116
see if there's some semantic
similarities between them.

23
00:01:11.116 --> 00:01:14.316
And then if so,
score it from zero to one, and

24
00:01:14.316 --> 00:01:19.642
then that'll give me a number on whether
or not this was semantic related.

25
00:01:19.642 --> 00:01:22.831
So that's like a semantic similarity
score, or something like that.

26
00:01:22.831 --> 00:01:23.915
And that's just one metric.

27
00:01:23.915 --> 00:01:26.924
You might have many of those metrics,
you might wanna check to see.

28
00:01:26.924 --> 00:01:31.640
Well, I specifically told this LLM
that when it summarizes that it should

29
00:01:31.640 --> 00:01:35.456
include entities, so names,
places, things like that.

30
00:01:35.456 --> 00:01:37.601
So I wanna check for entities.

31
00:01:37.601 --> 00:01:40.801
So I'm gonna make another
metric that's another LLM,

32
00:01:40.801 --> 00:01:45.089
that checks the summary these chat
messages, parse out the entities, and

33
00:01:45.089 --> 00:01:47.758
see if they're in the summary or
not, right?

34
00:01:47.758 --> 00:01:51.611
And then, if so, I think there should
have been four entities in here.

35
00:01:51.611 --> 00:01:53.787
If there are four, that's a one.

36
00:01:53.787 --> 00:01:57.505
If there's only one,
then maybe that's a 025,

37
00:01:57.505 --> 00:02:01.064
whoever you wanna grade,
it's really science.

38
00:02:01.064 --> 00:02:04.449
[LAUGH] So it can get pretty tough.

39
00:02:04.449 --> 00:02:08.348
Luckily for us, smarter people have
created a lot of these metrics and

40
00:02:08.348 --> 00:02:12.312
frameworks that we can use, and
they have really good documentation,

41
00:02:12.312 --> 00:02:14.442
you can kind of play around with them.

42
00:02:14.442 --> 00:02:19.314
I also found that it's very helpful
to use something like Claude or

43
00:02:19.314 --> 00:02:23.664
ChatGPT as a parent partner to
think about these metrics and

44
00:02:23.664 --> 00:02:28.797
to go through and try to figure out
what is the right thing to test here.

45
00:02:28.797 --> 00:02:30.321
So, there are a lot and

46
00:02:30.321 --> 00:02:34.422
there really are no wrong answers
on things you need to test.

47
00:02:34.422 --> 00:02:38.652
But what I can tell you is,
if you do not write evals and

48
00:02:38.652 --> 00:02:43.168
pick the right metrics,
your AI product is gonna be bad.

49
00:02:43.168 --> 00:02:46.142
[LAUGH] It's gonna be really bad and
you're not gonna know why.

50
00:02:46.142 --> 00:02:51.064
And that's the difference between
writing a test in your code and an eval.

51
00:02:51.064 --> 00:02:54.350
Because a test is binary,
it passed or it failed.

52
00:02:54.350 --> 00:02:59.457
An eval is a range, it's like,
in what threshold is this, right?

53
00:02:59.457 --> 00:03:02.595
What is the accuracy of this, right?

54
00:03:02.595 --> 00:03:03.897
Is it 50%?

55
00:03:03.897 --> 00:03:08.683
If your agent is a financial agent, and
it's 50% accurate at doing its job,

56
00:03:08.683 --> 00:03:11.683
I doubt if anyone's gonna
ever use that product,

57
00:03:11.683 --> 00:03:16.061
because it only gets right 50% of
the time when dealing with my money.

58
00:03:16.061 --> 00:03:17.224
I don't think I want that.

59
00:03:17.224 --> 00:03:21.640
I would imagine for something financial,
it has to be be, minimum, 90%, even then,

60
00:03:21.640 --> 00:03:24.155
that's probably too low for
anything financial.

61
00:03:24.155 --> 00:03:25.326
How do you know what that is, though?

62
00:03:25.326 --> 00:03:26.725
You have to measure that.

63
00:03:26.725 --> 00:03:28.397
You have to figure out
what you're measuring.

64
00:03:28.397 --> 00:03:31.823
And you have to know the different, so
that way you can improve those things,

65
00:03:31.823 --> 00:03:32.360
all right?

66
00:03:32.360 --> 00:03:36.514
Cool, here's some practical
implementation strategies.

67
00:03:36.514 --> 00:03:40.594
This stuff doesn't really matter for
the most part.

68
00:03:40.594 --> 00:03:45.568
Everyone kinda does it differently, but
at the end of the day, you basically do

69
00:03:45.568 --> 00:03:50.334
offline evals, and you do evals when
you deploy, that's essentially it.

70
00:03:50.334 --> 00:03:51.836
So we're gonna do the offline evals.

71
00:03:51.836 --> 00:03:54.330
An offline eval is like,
you just run it from your computer.

72
00:03:54.330 --> 00:03:57.274
We're gonna run this eval,
it's gonna give me a report.

73
00:03:57.274 --> 00:03:59.092
I'm gonna make some changes,
and we'll run the eval.

74
00:03:59.092 --> 00:04:02.018
So it give me a report, I'm gonna make
some changes, and you just keep doing

75
00:04:02.018 --> 00:04:04.876
that, and then you deploy your changes
to your code like you normally do.

76
00:04:04.876 --> 00:04:09.589
But you can also do them inside of
a GitHub action or something like that,

77
00:04:09.589 --> 00:04:12.772
just like you would do
any other CI-based test.

78
00:04:12.772 --> 00:04:14.912
You could do evals there as well,

79
00:04:14.912 --> 00:04:19.815
but you probably will just be doing
a lot of offline evals, I would imagine.

80
00:04:19.815 --> 00:04:23.152
But there's no wrong answer,
I know some people that,

81
00:04:23.152 --> 00:04:27.057
they have a chat app,
every X amount of 100 messages they get,

82
00:04:27.057 --> 00:04:31.744
they run the evals again off of those new
data sets just to see what has changed.

83
00:04:31.744 --> 00:04:32.284
Stuff like that.

84
00:04:32.284 --> 00:04:36.130
It's really good to be able to do that,
because if users are annotating

85
00:04:36.130 --> 00:04:40.117
the outputs of your LLM and telling
you whether or not they expected that.

86
00:04:40.117 --> 00:04:44.572
You can have some type of alerting
system when you hit a threshold of, wow,

87
00:04:44.572 --> 00:04:47.630
a lot of people said this
is not what they expected.

88
00:04:47.630 --> 00:04:52.516
So now you know you have a degradation of
your AI system and it's not really good.

89
00:04:52.516 --> 00:04:56.527
So you might get notified about that and
you can just go see, they're all asking

90
00:04:56.527 --> 00:05:00.673
this question, let's just add something
to the system prompt to address that.

91
00:05:00.673 --> 00:05:01.686
Boom, fixed it, right?

92
00:05:01.686 --> 00:05:04.047
And you would not have known
that if you didn't see that.

93
00:05:04.047 --> 00:05:09.353
So you have to have those
evals put in place.

94
00:05:09.353 --> 00:05:13.339
I talked about human feedback,
it's free data, at the end of the day,

95
00:05:13.339 --> 00:05:16.077
you're trying to get a golden data set,
right?

96
00:05:16.077 --> 00:05:19.445
A golden data set is a data set
that's It's already labeled for you.

97
00:05:19.445 --> 00:05:23.275
It says, here's the input,
here's the expected output.

98
00:05:23.275 --> 00:05:24.773
That's a golden data set.

99
00:05:24.773 --> 00:05:29.815
You can synthesize that, but
getting it from a user is really tricky.

100
00:05:29.815 --> 00:05:32.221
So if you can get that,
you're good, right?

101
00:05:32.221 --> 00:05:35.181
And then in some scenarios you don't,
like for

102
00:05:35.181 --> 00:05:38.955
instance, our system,
we were trying to train our system on

103
00:05:38.955 --> 00:05:42.812
how to determine what is an important
email to someone or not.

104
00:05:42.812 --> 00:05:46.024
And we were able to create
that data set to test, for

105
00:05:46.024 --> 00:05:49.088
instance, on my own email
while going to Gmail,

106
00:05:49.088 --> 00:05:53.305
I would put a star on every email
that I thought was important to me.

107
00:05:53.305 --> 00:05:55.427
And then I would export all my emails.

108
00:05:55.427 --> 00:05:59.104
And when I exported them, it had the
Important label on it if it was a starred,

109
00:05:59.104 --> 00:06:01.717
or, I think it had to star
label on it if it was starred.

110
00:06:01.717 --> 00:06:02.675
That's a golden data set.

111
00:06:02.675 --> 00:06:05.057
It was annotated that these
are important emails, and

112
00:06:05.057 --> 00:06:07.192
the ones that don't have
it aren't important.

113
00:06:07.192 --> 00:06:10.256
So I was able to train an LLM on those.

114
00:06:10.256 --> 00:06:13.068
So it was basically free.

115
00:06:13.068 --> 00:06:14.513
But you're not always
gotta be able to do that.

116
00:06:14.513 --> 00:06:17.689
So those are just examples where
it's like, wow, I'm glad I got that.

117
00:06:17.689 --> 00:06:19.927
So, yeah, getting the data is hard.

118
00:06:19.927 --> 00:06:23.177
And this is why it takes time to improve
an AI system, not because it's like,

119
00:06:23.177 --> 00:06:25.577
you need all the time in
the world to implement this code.

120
00:06:25.577 --> 00:06:28.910
It's just because, you need to collect
the data, and collecting the data takes

121
00:06:28.910 --> 00:06:31.459
a lot of time, especially if you
don't have a lot of users, or

122
00:06:31.459 --> 00:06:32.943
a lot of users giving you feedback.

123
00:06:32.943 --> 00:06:37.691
So sometimes you just need to collect
enough data to see what's going wrong so

124
00:06:37.691 --> 00:06:38.942
you can improve it.

125
00:06:38.942 --> 00:06:42.197
And yeah, that's pretty much
the strategy that I see today.

126
00:06:42.197 --> 00:06:46.370
What I see today as far as people getting
things out, they'll prototype something

127
00:06:46.370 --> 00:06:49.715
kinda like what we did in the other
course in a day, maybe two days.

128
00:06:49.715 --> 00:06:52.972
They'll ship it either to
some folks internally or

129
00:06:52.972 --> 00:06:56.844
some beta testers who know that
the thing is gonna be busted.

130
00:06:56.844 --> 00:07:00.882
If they were to eval it right now,
it'll probably be below 50% on everything.

131
00:07:00.882 --> 00:07:02.613
It'll be really bad.

132
00:07:02.613 --> 00:07:05.448
They put it out there, and
then they just collect the feedback.

133
00:07:05.448 --> 00:07:08.262
And in the meantime,
they're writing the evals.

134
00:07:08.262 --> 00:07:11.815
They collect that feedback, they feed
it back into the eval system, and

135
00:07:11.815 --> 00:07:13.337
then they start improving it.

136
00:07:13.337 --> 00:07:17.256
And once they get past some threshold
that they're comfortable with, let's say,

137
00:07:17.256 --> 00:07:20.582
I don't know, 70%, they open the door so
those other people do it.

138
00:07:20.582 --> 00:07:22.611
And then they collect even more feedback.

139
00:07:22.611 --> 00:07:25.112
And then once they feel called,
when they get it to, I don't know, 90%,

140
00:07:25.112 --> 00:07:27.317
then it's like, cool,
let's get this out to everybody, right?

141
00:07:27.317 --> 00:07:31.807
So that's typically how I see things
happening for a lot of people.

142
00:07:31.807 --> 00:07:34.488
And depending on how many users you have
and your feedback loop, that could take

143
00:07:34.488 --> 00:07:36.902
a month, it could take three months,
it could take a year, who knows?

144
00:07:36.902 --> 00:07:42.493
&gt;&gt; Speaker 2: So you said, running them
locally, how do you manage the cost?

145
00:07:42.493 --> 00:07:46.586
I'm assuming,
if you're running tests against real LLMs,

146
00:07:46.586 --> 00:07:51.950
there's costs associated with that,
what's your kind of, strategy around?

147
00:07:51.950 --> 00:07:53.609
&gt;&gt; Scott Moss: There is none,
you just gotta pay for it.

148
00:07:53.609 --> 00:07:57.041
[LAUGH] I think the only time you wanna
manage costs is when users are hitting it,

149
00:07:57.041 --> 00:07:59.289
cuz then it's not in
the controlled environment.

150
00:07:59.289 --> 00:08:01.471
&gt;&gt; Speaker 2: Yeah, sure.
&gt;&gt; Scott Moss: But when it comes to evals,

151
00:08:01.471 --> 00:08:04.913
I mean, there are some, yeah,
when it comes to evals,

152
00:08:04.913 --> 00:08:09.383
the only time you're worried about
cost is when you're trying to reduce

153
00:08:09.383 --> 00:08:11.678
the cost of the user facing systems.

154
00:08:11.678 --> 00:08:15.086
So that is a constraint that you add to,
like for instance,

155
00:08:15.086 --> 00:08:19.585
if I know that when my users use my agent,
they're gonna be using GPT4o mini,

156
00:08:19.585 --> 00:08:23.431
but I'm evaling with GPT 4.0,
it doesn't really make sense.

157
00:08:23.431 --> 00:08:26.665
I should probably eval
with GPT 4.0 mini as well,

158
00:08:26.665 --> 00:08:28.882
which reduces the cost of my evals.

159
00:08:28.882 --> 00:08:32.205
But for the sake of reducing costs for
evals, no,

160
00:08:32.205 --> 00:08:34.987
there's just no way around it right now.

161
00:08:34.987 --> 00:08:40.027
I mean, obviously, you can use Local
stuff, but at the end of the day,

162
00:08:40.027 --> 00:08:44.564
you wanna be evaling the exact
same systems that your users use.

163
00:08:44.564 --> 00:08:47.197
So yeah, there really is no way around it.

164
00:08:47.197 --> 00:08:49.328
But it's really not too bad.

165
00:08:49.328 --> 00:08:55.831
I mean, we run evals, we have hundreds of
thousands of records, and, I don't know,

166
00:08:55.831 --> 00:09:00.840
and maybe cost me a few bucks a month
to run these evals all the time.

167
00:09:00.840 --> 00:09:04.575
So it's not too bad, but then again,
I'm not at scale, [LAUGH] and

168
00:09:04.575 --> 00:09:07.579
maybe some other company
with way more users than me,

169
00:09:07.579 --> 00:09:11.758
maybe they might be paying hundreds
of thousand dollars a month on evals.

170
00:09:11.758 --> 00:09:13.607
&gt;&gt; Speaker 2: [INAUDIBLE] they
can afford it by that point.

171
00:09:13.607 --> 00:09:14.797
&gt;&gt; Scott Moss: Right,
they can afford it at that point.

172
00:09:14.797 --> 00:09:17.084
And they have to write, I think,

173
00:09:17.084 --> 00:09:23.077
the amount of data that you process kinda
somewhat, I mean, it eventually tells off.

174
00:09:23.077 --> 00:09:26.538
But I think there is some type of
correlation, it's not linear, between how

175
00:09:26.538 --> 00:09:30.124
many users you have and how much data
you're processing on your evals, right?

176
00:09:30.124 --> 00:09:33.401
So it does have some type
of correlation there.

177
00:09:33.401 --> 00:09:38.601
But I think at Args who are really
serious about using LLM or production,

178
00:09:38.601 --> 00:09:43.385
they probably spend 20 to 30%
of their time just doing evals.

179
00:09:43.385 --> 00:09:45.926
It's a significant part of their time.

180
00:09:45.926 --> 00:09:48.531
Otherwise, you have people that
are just doing it all fives,

181
00:09:48.531 --> 00:09:50.095
which is the current meme right now.

182
00:09:50.095 --> 00:09:53.995
It's like, yeah,
that looked good when I ran it ten times.

183
00:09:53.995 --> 00:09:54.619
That looked good.

184
00:09:54.619 --> 00:09:57.120
So I'm gonna deploy it and
just be done with it.

