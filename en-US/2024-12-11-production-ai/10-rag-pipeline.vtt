WEBVTT

1
00:00:00.081 --> 00:00:02.292
&gt;&gt; Scott Moss: All right,
let's talk about the RAG pipeline.

2
00:00:02.292 --> 00:00:05.584
So first of all, we need to understand
the different parts of RAG.

3
00:00:05.584 --> 00:00:08.569
RAG involves a couple things.

4
00:00:08.569 --> 00:00:14.015
It involves a vector database,
we'll talk about a vector database,

5
00:00:14.015 --> 00:00:17.154
it involves embeddings in vectors, and

6
00:00:17.154 --> 00:00:21.697
then it involves indexing that,
and then querying that.

7
00:00:21.697 --> 00:00:24.274
So it involves those
four things at minimum.

8
00:00:24.274 --> 00:00:27.867
And then depending on what your process
is, you'll have more than that.

9
00:00:27.867 --> 00:00:32.835
I'll show you a graph right now, it'll
blow your mind, just the type of stuff you

10
00:00:32.835 --> 00:00:36.743
have to do to get something
that's 60% good, it's insane.

11
00:00:36.743 --> 00:00:38.153
So let's talk about the RAG pipeline,

12
00:00:38.153 --> 00:00:39.828
knowing that we have
those different pieces.

13
00:00:39.828 --> 00:00:42.371
So the first thing is,
is you start with your documents.

14
00:00:42.371 --> 00:00:46.855
Your documents are the things that you
want your AI to have access to to answer

15
00:00:46.855 --> 00:00:47.635
questions.

16
00:00:47.635 --> 00:00:51.801
So in my example of these documents
might be books, they might be pages,

17
00:00:51.801 --> 00:00:53.250
they might be chapters.

18
00:00:53.250 --> 00:00:57.671
And that's the first hard part is,
one, I know I have all this data,

19
00:00:57.671 --> 00:01:01.118
what is the best way to put
it in the database, right?

20
00:01:01.118 --> 00:01:05.223
Because if you think about it, first of
all, you don't need to know how a vector

21
00:01:05.223 --> 00:01:09.165
database works right now, but let's
think about the problem that we have.

22
00:01:09.165 --> 00:01:12.879
The problem that we have is,
we can't fit all this data into the LLM.

23
00:01:12.879 --> 00:01:16.581
So if I just take all this data and
put in the vector database as one entity,

24
00:01:16.581 --> 00:01:18.269
that doesn't solve my problem.

25
00:01:18.269 --> 00:01:21.017
Because [LAUGH] when the AI
queries this database to get it,

26
00:01:21.017 --> 00:01:23.330
they're gonna get back
the whole thing anyway.

27
00:01:23.330 --> 00:01:24.693
I might as well just put it in there.

28
00:01:24.693 --> 00:01:28.803
So now you have to think, okay,
I have to break this up so

29
00:01:28.803 --> 00:01:34.148
when the AI finds the appropriate one,
it only gets that piece, right?

30
00:01:34.148 --> 00:01:37.464
So then you have to think of
what's called a chunking strategy.

31
00:01:37.464 --> 00:01:42.185
How do I chunk this data to
a point where it's appropriate so

32
00:01:42.185 --> 00:01:45.397
when my AI finds these pieces of chunks,

33
00:01:45.397 --> 00:01:50.034
I'm confident that it has
the information that it needs.

34
00:01:50.034 --> 00:01:53.315
And then you have to think
about how that works.

35
00:01:53.315 --> 00:01:55.352
Some of your data might be relational.

36
00:01:55.352 --> 00:01:57.346
For us, we might chunk someone's emails.

37
00:01:57.346 --> 00:02:00.928
Emails are highly relational
about the threat that they're on,

38
00:02:00.928 --> 00:02:02.891
whether you sent it or received it.

39
00:02:02.891 --> 00:02:06.361
Did you get BCC or CC?

40
00:02:06.361 --> 00:02:07.401
Who was the sender?

41
00:02:07.401 --> 00:02:11.148
There's so many different angles in
which you have to think about, and

42
00:02:11.148 --> 00:02:14.593
then that's not even considering
the body that's in the email.

43
00:02:14.593 --> 00:02:18.148
And if you think about email body,
it might not just have the email

44
00:02:18.148 --> 00:02:22.737
that you sent, but if it's a reply, it's
got all the other ones underneath it too,

45
00:02:22.737 --> 00:02:25.996
which is doubled,
because those emails have them as well.

46
00:02:25.996 --> 00:02:28.166
So it's, you have to really think about,

47
00:02:28.166 --> 00:02:31.313
how do I separate this to the point
where the AI kinda knows it?

48
00:02:31.313 --> 00:02:33.808
So there really is no right answer, but

49
00:02:33.808 --> 00:02:37.665
what most people do is they pick
a token limit of, all right,

50
00:02:37.665 --> 00:02:42.758
I'm gonna pick a chunk, I'm gonna have
it be at max x amount of tokens, right?

51
00:02:42.758 --> 00:02:46.831
Let's say 100 tokens, so that's,
I don't know, 400 characters or

52
00:02:46.831 --> 00:02:48.914
something like that, think a number.

53
00:02:48.914 --> 00:02:52.299
And then what they'll do
is they will do a overlap.

54
00:02:52.299 --> 00:02:57.502
So if I have a book, and
I'm breaking up every 100 tokens,

55
00:02:57.502 --> 00:03:01.204
what I will do is, for every 100 tokens,

56
00:03:01.204 --> 00:03:06.526
when I go to the next set that
I'm chunking, I'll start back.

57
00:03:06.526 --> 00:03:08.968
So if I've already chunked 100 tokens,

58
00:03:08.968 --> 00:03:13.659
the next one won't start off at the 100th
token, it'll start off at token 80.

59
00:03:13.659 --> 00:03:17.885
So I'll overlap 20 tokens,
and then I'll go from 80,

60
00:03:17.885 --> 00:03:22.546
up to another 100, so that'll be,
whatever that number is.

61
00:03:22.546 --> 00:03:26.242
And then basically,
the chunk after the chunk before it,

62
00:03:26.242 --> 00:03:30.628
there's an overlap of 20 tokens
where they're exactly the same.

63
00:03:30.628 --> 00:03:34.715
That way when I don't miss context,
because if you break up a sentence in

64
00:03:34.715 --> 00:03:38.481
the middle of a word, you might
lose the context of that sentence.

65
00:03:38.481 --> 00:03:44.438
So having that overlap there helps the AI
continue the context of certain words.

66
00:03:44.438 --> 00:03:46.540
So this can get really crazy.

67
00:03:46.540 --> 00:03:51.049
So that's naive rack, that's generic,

68
00:03:51.049 --> 00:03:54.566
very naive implementation V1.

69
00:03:54.566 --> 00:03:56.032
So that's what most people start with.

70
00:03:56.032 --> 00:04:02.308
They do chunking on some token level,
and then they do some type of overlap.

71
00:04:02.308 --> 00:04:04.186
So you'll see a lot of that.

72
00:04:04.186 --> 00:04:07.940
And then that's not to mention text
extraction, I'm talking about things that

73
00:04:07.940 --> 00:04:11.651
are already texts like a book, it's
already a PDF, you already have the text.

74
00:04:11.651 --> 00:04:14.561
Some things aren't text, right,
some things might be in JSON, so

75
00:04:14.561 --> 00:04:16.821
you have to figure out,
how do I format that to text?

76
00:04:16.821 --> 00:04:19.480
Some things might be in XML,
some things might be images,

77
00:04:19.480 --> 00:04:22.106
some things might be video,
some things might be songs.

78
00:04:22.106 --> 00:04:27.400
How do I transform MP3 into
something that I can index, right?

79
00:04:27.400 --> 00:04:29.182
You need a model for that.

80
00:04:29.182 --> 00:04:32.156
So that's usually step zero, but honestly,

81
00:04:32.156 --> 00:04:35.448
I think a lot of that is
solved pretty well already.

82
00:04:35.448 --> 00:04:39.949
There's literally a model to do all
of that, but I think the chunking is

83
00:04:39.949 --> 00:04:44.161
hard because although it's easy to do,
there is no right answer.

84
00:04:44.161 --> 00:04:48.017
And I see a lot of different strategies
every single day about how to do

85
00:04:48.017 --> 00:04:48.694
this best.

86
00:04:48.694 --> 00:04:53.193
And if you ever wanna talk about chunking,
let me know,

87
00:04:53.193 --> 00:04:55.699
I know way too much about that.

88
00:04:55.699 --> 00:04:58.224
The next thing you do is
you have to do embeddings.

89
00:04:58.224 --> 00:05:00.548
So what is an embedding?

90
00:05:00.548 --> 00:05:03.637
Okay, so here's an embedding.

91
00:05:03.637 --> 00:05:08.491
An embedding is a group of vectors,
a vector is a number,

92
00:05:08.491 --> 00:05:13.962
it's an array of vectors,
each number is between zero and one.

93
00:05:13.962 --> 00:05:16.358
It's a couple decimal points.

94
00:05:16.358 --> 00:05:22.697
And they are the numerical representation
of a piece of text, right?

95
00:05:22.697 --> 00:05:26.588
So this piece of text,
when passed into some embedding model, so

96
00:05:26.588 --> 00:05:29.934
right now we have LLMs,
we also have embedding models.

97
00:05:29.934 --> 00:05:34.884
Embedding models are the things
that transform your text into

98
00:05:34.884 --> 00:05:38.320
numbers consistently every single time.

99
00:05:38.320 --> 00:05:39.972
It is the foundation of LLMs.

100
00:05:39.972 --> 00:05:45.085
Without embedding models,
LLMS would not be able to do this

101
00:05:45.085 --> 00:05:49.803
type of prediction in
similarity types of semantics.

102
00:05:49.803 --> 00:05:54.991
So what you have to do is, when you
have the data that you want to ingest,

103
00:05:54.991 --> 00:05:59.833
you then have to convert that data,
those chunks, into vectors,

104
00:05:59.833 --> 00:06:01.918
which is a group of numbers.

105
00:06:01.918 --> 00:06:04.659
And you have to put them in a vector
database cuz the vector database only

106
00:06:04.659 --> 00:06:06.585
takes embedding,
it only takes list of numbers.

107
00:06:06.585 --> 00:06:11.006
You can't put text in a vector database.

108
00:06:11.006 --> 00:06:15.557
Once you have them in the vector database,
you can do math on them, right?

109
00:06:15.557 --> 00:06:17.117
So you could think of,

110
00:06:17.117 --> 00:06:21.804
maybe this represents
a three-dimensional vector database, and

111
00:06:21.804 --> 00:06:27.493
these are pieces of text that are grouped
together by similar semantics or words.

112
00:06:27.493 --> 00:06:29.951
And you can kinda see how far
they are from each other and

113
00:06:29.951 --> 00:06:31.355
their distance between them.

114
00:06:31.355 --> 00:06:36.875
And this is exactly how we calculate the
distance between what a user is saying and

115
00:06:36.875 --> 00:06:40.622
what we want the AI to know
inside of a vector database.

116
00:06:40.622 --> 00:06:44.957
We just put it on a graph,
we do some math, and we figure out,

117
00:06:44.957 --> 00:06:49.548
which are the ones that have
the closest similarities, right?

118
00:06:49.548 --> 00:06:53.189
Although, it's on a 1500 dimension
database, not 3-dimension or

119
00:06:53.189 --> 00:06:57.135
700-dimension database, it really
just depends on your embedding model.

120
00:06:57.135 --> 00:07:01.441
Different embedding models have
different levels of dimensions.

121
00:07:01.441 --> 00:07:05.875
The higher the dimension, the more
detail it knows, but the more it costs,

122
00:07:05.875 --> 00:07:08.524
and the slower it might be,
just like an LLM.

123
00:07:08.524 --> 00:07:11.695
Cool, so once you do that,
then you have to store it,

124
00:07:11.695 --> 00:07:16.424
put it in your vector database, typically
depends on the type of data you have.

125
00:07:16.424 --> 00:07:20.581
You store that information like we're
gonna do today, you just run it once.

126
00:07:20.581 --> 00:07:23.661
We have static data, we have a movie
database that's never updated, so

127
00:07:23.661 --> 00:07:25.841
we're just gonna update it once,
and that's it.

128
00:07:25.841 --> 00:07:29.301
We do that offline, we don't do
that when application's running.

129
00:07:29.301 --> 00:07:33.055
Now if you're indexing data
that is constantly changing,

130
00:07:33.055 --> 00:07:35.389
then it's like a regular database.

131
00:07:35.389 --> 00:07:39.062
When that data comes in, you create it,
you chunk it, you create an embedding,

132
00:07:39.062 --> 00:07:42.376
you save it in your database,
just like you would in a regular database.

133
00:07:42.376 --> 00:07:43.792
So it just depends on your data.

134
00:07:43.792 --> 00:07:47.643
I would say, for most of the examples out
there, usually talk about some static data

135
00:07:47.643 --> 00:07:50.096
or something that doesn't
really change that much.

136
00:07:50.096 --> 00:07:53.443
Take your notion data and turn it into
a vector so you can search on it or

137
00:07:53.443 --> 00:07:56.469
something like that, or
talk to a PDF or something like that.

138
00:07:56.469 --> 00:07:58.559
Those are static,
they almost never change.

139
00:07:58.559 --> 00:08:04.831
So the update frequency is just depending
on how much your data changes, right?

140
00:08:04.831 --> 00:08:07.226
Yeah, it can get pretty expensive [LAUGH].

141
00:08:07.226 --> 00:08:09.944
Then the last part, which is the R of RAG,

142
00:08:09.944 --> 00:08:14.625
retrieval, is, well, not the last part,
the other side of the coin.

143
00:08:14.625 --> 00:08:17.684
What I just talked about was indexing and
ingesting the data.

144
00:08:17.684 --> 00:08:21.501
Now that you have that,
when someone ask a question on your LLM,

145
00:08:21.501 --> 00:08:23.277
now you gotta go retrieve it.

146
00:08:23.277 --> 00:08:24.676
So now here's the other side of it.

147
00:08:24.676 --> 00:08:26.846
We already indexed it,
it's in the database.

148
00:08:26.846 --> 00:08:32.163
The retrieval part is kinda
similar to the indexing part,

149
00:08:32.163 --> 00:08:38.034
we take what the user said,
we also turn that into an embedding.

150
00:08:38.034 --> 00:08:43.724
Why would we turn what the user said
into an embedding, anybody know why?

151
00:08:43.724 --> 00:08:45.257
&gt;&gt; Speaker 2: So we can do math?

152
00:08:45.257 --> 00:08:48.390
&gt;&gt; Scott Moss: So we can do math,
right, yeah, we wanna do number math.

153
00:08:48.390 --> 00:08:52.194
So [LAUGH] well, number math isn't
all math numbers, that's so silly.

154
00:08:52.194 --> 00:08:56.973
We wanna do math, so we have to
convert the user's text into vectors,

155
00:08:56.973 --> 00:09:01.766
numerical representation,
the same way that the rest of our data is.

156
00:09:01.766 --> 00:09:06.269
And then what we can do,
most vector databases have

157
00:09:06.269 --> 00:09:11.097
different search algorithms
on how you wanna search.

158
00:09:11.097 --> 00:09:16.386
There's so many of them most people use,
like cosine similarity.

159
00:09:16.386 --> 00:09:20.113
There's other ones that I'm not smart
enough to even remember or understand, but

160
00:09:20.113 --> 00:09:22.415
for the most part,
that's what a lot of people do.

161
00:09:22.415 --> 00:09:27.110
And it's the type of algorithm
that it runs to create

162
00:09:27.110 --> 00:09:32.136
that list of similar pieces of data,
so you find those.

163
00:09:32.136 --> 00:09:35.853
Some databases have advanced features
like metadata filtering, and

164
00:09:35.853 --> 00:09:39.835
you'll see that we have that as well
in the database that we're gonna use.

165
00:09:39.835 --> 00:09:42.435
Where you can not only
search from a vector, but

166
00:09:42.435 --> 00:09:44.258
you can also filter on metadata.

167
00:09:44.258 --> 00:09:48.884
As far as movies go, this director
equals this, year, between this year and

168
00:09:48.884 --> 00:09:49.605
this year.

169
00:09:49.605 --> 00:09:55.375
So you can narrow it down even further,
so you can get really good with it.

170
00:09:55.375 --> 00:09:58.186
And yeah, so that's the retrieval part.

171
00:09:58.186 --> 00:10:03.011
The augmentation part, the A in RAG,
is taking those results and

172
00:10:03.011 --> 00:10:07.326
adding it back to the original LLM so
it can see it, right?

173
00:10:07.326 --> 00:10:09.555
How do I take these results and
add it back?

174
00:10:09.555 --> 00:10:13.155
That part sounds simple,
but there's so much.

175
00:10:13.155 --> 00:10:17.311
In a production app, you would almost
never come directly from a RAG or

176
00:10:17.311 --> 00:10:20.904
from a vector query with a list
of things and give it to the AI,

177
00:10:20.904 --> 00:10:23.560
you almost never do it for
a lot of reasons.

178
00:10:23.560 --> 00:10:27.480
One, how many tokens are in here?

179
00:10:27.480 --> 00:10:30.259
If there's a lot of tokens in here,
and I just feed it back to AI,

180
00:10:30.259 --> 00:10:33.662
I'm back to where I started, I might break
the LLM cuz I added too many tokens.

181
00:10:33.662 --> 00:10:35.554
So one, how many tokens are in here?

182
00:10:35.554 --> 00:10:40.378
Two, a vector database is really good at
finding things that semantically make

183
00:10:40.378 --> 00:10:45.135
sense, but they actually aren't good
at understanding relevance, right?

184
00:10:45.135 --> 00:10:50.672
So if I had a question that mentioned
something about a date and,

185
00:10:50.672 --> 00:10:55.541
find me movies that were between
this time and this time, and

186
00:10:55.541 --> 00:10:58.036
it wouldn't found me movies.

187
00:10:58.036 --> 00:11:04.975
Let's say I said, find me a movie that's
in between the year 1989 and 2000.

188
00:11:04.975 --> 00:11:09.207
And then it found me two movies,
its title was called 1989, and

189
00:11:09.207 --> 00:11:12.040
the other one's title was called 2000.

190
00:11:12.040 --> 00:11:15.027
Yeah, the vector database is gonna
be like, yeah, here you go, but

191
00:11:15.027 --> 00:11:16.741
that's actually not relevant at all.

192
00:11:16.741 --> 00:11:22.231
So typically, you would have another step,
which is called re-ranking, which would

193
00:11:22.231 --> 00:11:27.440
rank these on how relevant they are to the
original message, and some other thing.

194
00:11:27.440 --> 00:11:30.774
You might even summarize these
because they're still too big,

195
00:11:30.774 --> 00:11:33.130
there's a lot of optimization around that.

196
00:11:33.130 --> 00:11:36.941
And then you would augment your
original AI with the results of that so

197
00:11:36.941 --> 00:11:37.736
we can see it.

198
00:11:37.736 --> 00:11:41.312
So that's the augmentation step.

199
00:11:41.312 --> 00:11:46.545
And then the generation step is, now
that AI can see it, formulate an answer.

200
00:11:46.545 --> 00:11:50.950
You have the information you need,
you have the original user question,

201
00:11:50.950 --> 00:11:54.160
formulate an answer,
that's the generation part.

202
00:11:54.160 --> 00:11:59.550
And that has its own nuances as well,
trying to have a ground,

203
00:11:59.550 --> 00:12:03.383
cuz a lot of apps,
if you go to use Claude or,

204
00:12:03.383 --> 00:12:07.861
anything that, perplexity,
they have sources.

205
00:12:07.861 --> 00:12:11.414
Here are all the sources that I
use to answer your question so

206
00:12:11.414 --> 00:12:14.125
that generation part would have that, yes.

207
00:12:14.125 --> 00:12:17.482
&gt;&gt; Speaker 3: Do we need to
have some kinda description for

208
00:12:17.482 --> 00:12:21.395
each chunk that helps AI
choose the correct chunk?

209
00:12:21.395 --> 00:12:26.126
&gt;&gt; Scott Moss: It's so hard to answer
that question because you don't need to

210
00:12:26.126 --> 00:12:30.423
do any of that,
it might help depending on your use case.

211
00:12:30.423 --> 00:12:37.310
The closest thing I can think of that
reminds me of that is actually how

212
00:12:37.310 --> 00:12:42.549
Claude and
the Anthropic team does their chunking.

213
00:12:42.549 --> 00:12:47.915
It's called contextual retrieval,
and it does exactly that.

214
00:12:47.915 --> 00:12:52.709
It'll take a chunk, and then it'll
run that chunk through an LLM to give

215
00:12:52.709 --> 00:12:56.968
a description of that chunk in
relation to the entire document.

216
00:12:56.968 --> 00:13:02.741
And then that contextualized chunk is
what they put in the vector database.

217
00:13:02.741 --> 00:13:05.204
So the AI not only sees the chunk, but

218
00:13:05.204 --> 00:13:09.269
understands how this chunk
relates to the entire document.

219
00:13:09.269 --> 00:13:10.916
This is actually what I use in production,

220
00:13:10.916 --> 00:13:12.672
this is one of the things
I use in production.

221
00:13:12.672 --> 00:13:14.686
So yeah, you can do it, but

222
00:13:14.686 --> 00:13:19.867
there's other things that you can
do as well that are really helpful.

223
00:13:19.867 --> 00:13:21.422
But no, you don't need to do it.

224
00:13:21.422 --> 00:13:24.660
Because the data itself,
the semantics itself,

225
00:13:24.660 --> 00:13:29.144
those numbers themselves,
are the things that are being compared.

226
00:13:29.144 --> 00:13:35.151
So you have to think to yourself,
there's a process of thinking of,

227
00:13:35.151 --> 00:13:41.690
what would I expect my users to be
searching for versus the data that I have?

228
00:13:41.690 --> 00:13:44.132
And how can I get ahead of that and
add some of that here?

229
00:13:44.132 --> 00:13:46.594
But also the only way to get
around this is just EVALS.

230
00:13:46.594 --> 00:13:50.571
You have to do EVALS on this stuff
to see how good your retrieval is,

231
00:13:50.571 --> 00:13:54.141
your augmentation is,
your generation is, everything.

232
00:13:54.141 --> 00:13:56.441
There's just no way around it.

233
00:13:56.441 --> 00:13:58.440
One of many approaches is that, yes.

