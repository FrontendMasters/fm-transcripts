WEBVTT

1
00:00:00.000 --> 00:00:02.716
Last part is just,
where do we go from here guys?

2
00:00:02.716 --> 00:00:07.410
Beyond just this,
it's just more of the same really,

3
00:00:07.410 --> 00:00:11.198
at this point, it's just more of the same.

4
00:00:11.198 --> 00:00:15.891
Once you get to everything
I just showed you today,

5
00:00:15.891 --> 00:00:21.570
it's more evals, more metrics,
more prompt engineering,

6
00:00:21.570 --> 00:00:26.062
better RAG,
you have the more framework level.

7
00:00:26.062 --> 00:00:30.765
Right now we have one agent, you might
split these off into many agents,

8
00:00:30.765 --> 00:00:36.098
to something called like a swarm, which,
I keep thinking I'm in a browser.

9
00:00:36.098 --> 00:00:41.020
Lemme go here, open AI swarm,
it's an educational framework,

10
00:00:41.020 --> 00:00:46.578
it's written in Python, but
it's actually pretty easy to understand.

11
00:00:46.578 --> 00:00:53.142
Essentially, you have one agent whose job
is to orchestrate all the other agents and

12
00:00:53.142 --> 00:00:58.782
each agent is assigned a set of functions
in which they are only bound to,

13
00:00:58.782 --> 00:01:00.834
and it's called a swarm.

14
00:01:00.834 --> 00:01:04.134
This is really good, this is something
you can use and it's actually so

15
00:01:04.134 --> 00:01:07.435
easy to follow if you look at it, but
there's tons of things like this.

16
00:01:07.435 --> 00:01:10.075
Yeah, it's mostly just more of that,
right,

17
00:01:10.075 --> 00:01:15.028
if there's only one thing you ever learned
from this course today, it's evals matter.

18
00:01:15.028 --> 00:01:18.178
You have to do them, they shouldn't
be treated like unit tests or

19
00:01:18.178 --> 00:01:19.450
testing your other code.

20
00:01:19.450 --> 00:01:23.247
You actually can't build
good products with an AI,

21
00:01:23.247 --> 00:01:26.628
without evals, they literally can't do it.

22
00:01:26.628 --> 00:01:32.382
In fact, one of the creators of GPT, Greg
Brockman, one of the co founders open AI,

23
00:01:32.382 --> 00:01:38.333
tweeted this out December 9,
2023 evals are surprisingly all you need.

24
00:01:38.333 --> 00:01:42.221
[LAUGH] Surprisingly, really,
if you just had some good evals,

25
00:01:42.221 --> 00:01:46.181
you the difference between you
having a product that's great and

26
00:01:46.181 --> 00:01:48.640
a product that only that nobody can use.

27
00:01:48.640 --> 00:01:52.011
So I cannot emphasize how
important evals are, so

28
00:01:52.011 --> 00:01:56.596
please spend time on evals if you
are planning on ever having users.

29
00:01:56.596 --> 00:02:00.147
It should not be an afterthought,
it's not like testing,

30
00:02:00.147 --> 00:02:02.712
it is part of the product,
it is part of it.

31
00:02:02.712 --> 00:02:05.840
Talked about advanced rack
techniques in a little bit but

32
00:02:05.840 --> 00:02:08.197
like hybrid search, so we got rid of that.

33
00:02:08.197 --> 00:02:11.874
That was basically like the filtering,
so being able to do like the metadata

34
00:02:11.874 --> 00:02:15.677
filtering plus the vector searching,
that improves search results so much.

35
00:02:15.677 --> 00:02:20.696
But it also exponentiates the complexity
of how you need to test it, I haven't

36
00:02:20.696 --> 00:02:26.032
found a good way to test it yet, but when
it works, it's great, it is really good.

37
00:02:26.032 --> 00:02:30.233
I think it's hard for a chat app, I think
it's so much easier when the filtering

38
00:02:30.233 --> 00:02:33.804
can be selected by a GUI, where you
have a search bar and a dropdown,

39
00:02:33.804 --> 00:02:35.892
you can do like facet based filtering.

40
00:02:35.892 --> 00:02:39.042
Then it's like you get the best of both
worlds, you get to pick the filters and

41
00:02:39.042 --> 00:02:40.077
you get to do the search.

42
00:02:40.077 --> 00:02:42.082
And it's great every time,

43
00:02:42.082 --> 00:02:46.598
but trying to figure out filters
from someone's chat message.

44
00:02:46.598 --> 00:02:49.335
It's usually always too strong or
not strong enough,

45
00:02:49.335 --> 00:02:52.977
it's just not good enough to actually
filter on, so it's really hard.

46
00:02:52.977 --> 00:02:56.101
So I haven't found a good way for that,
but hybrid search is really great,

47
00:02:56.101 --> 00:02:57.400
highly recommend doing that.

48
00:02:57.400 --> 00:02:59.575
In fact, if you're gonna do RAG,

49
00:02:59.575 --> 00:03:03.179
you should always do hybrid
to some degree in my opinion.

50
00:03:03.179 --> 00:03:07.492
Unless it's just like,
you're chatting with a PDF,

51
00:03:07.492 --> 00:03:11.619
if you're chatting with
multiple pieces of data and

52
00:03:11.619 --> 00:03:15.769
not just like one corpus
of data that's chunked up.

53
00:03:15.769 --> 00:03:19.164
But actual for instance, songs,
movies, emails, things like that,

54
00:03:19.164 --> 00:03:22.016
you probably wanna do metadata filtering,
hybrid search.

55
00:03:22.016 --> 00:03:26.328
But if you're like, I just trunked up this
one gigabyte PDF, and I just wanna chat

56
00:03:26.328 --> 00:03:30.909
with it, you probably don't need metadata
filtering, what are you gonna filter on?

57
00:03:30.909 --> 00:03:32.057
&gt;&gt; Student: It's already filtered for you.

58
00:03:32.057 --> 00:03:35.096
&gt;&gt; Scott Moss: Right, exactly,
so context processing,

59
00:03:35.096 --> 00:03:39.624
I talked a little bit about this
with the example with Anthropic.

60
00:03:39.624 --> 00:03:43.926
Contextual retrieval,
different things like that,

61
00:03:43.926 --> 00:03:48.794
these are all just terms that I
put here that you can go Google.

62
00:03:48.794 --> 00:03:51.755
I was like, I'm just gonna give them
a bunch of terms that they can type into

63
00:03:51.755 --> 00:03:53.826
Google and go search themselves or
ask the AI about.

64
00:03:53.826 --> 00:03:57.497
Because I looked at my notes
over the last six months, and

65
00:03:57.497 --> 00:04:02.943
these are all the things that I have been
researching and implementing and testing.

66
00:04:02.943 --> 00:04:08.454
So I put them here for
people to go look up, they're all insane,

67
00:04:08.454 --> 00:04:13.573
here are some of my favorite
white papers that I read on RAG.

68
00:04:13.573 --> 00:04:18.340
So self-RAG, context-faithful prompting,
hypothetical document embeddings,

69
00:04:18.340 --> 00:04:21.811
RAG-Fusion, there's so
many more you can go check them out.

70
00:04:21.811 --> 00:04:26.104
White papers are just like, if you don't
know what those are scientific papers that

71
00:04:26.104 --> 00:04:29.929
scientists release when they come up
with some new approach to something.

72
00:04:29.929 --> 00:04:32.495
So you check that out, Fine-Tuning,

73
00:04:32.495 --> 00:04:36.086
there's just no way we're
ever gonna do fine tuning.

74
00:04:36.086 --> 00:04:39.236
Let's just real, you're gonna spend 20
bucks, we're gonna be here all day and

75
00:04:39.236 --> 00:04:40.140
we don't have enough.

76
00:04:40.140 --> 00:04:45.181
Data but again Fine-Tuning is like you
got on a plane you took an uber and

77
00:04:45.181 --> 00:04:47.626
then you hopped on a lime scooter.

78
00:04:47.626 --> 00:04:53.349
That's the Fine-Tuning it's
the last mile of tuning and

79
00:04:53.349 --> 00:05:00.138
it's mostly good for changing
the output of the LLM to make it sound.

80
00:05:00.138 --> 00:05:05.009
Like something else, or look like
something else, or, in the case of OpenAI,

81
00:05:05.009 --> 00:05:07.349
structured output things like that.

82
00:05:07.349 --> 00:05:11.016
So if you wanna change the format of how
something is outputted, not because you

83
00:05:11.016 --> 00:05:14.428
need it to be a certain format,
because it might actually save you money.

84
00:05:14.428 --> 00:05:18.760
So let's say you're using GPT4, and it's
actually really good at what it's doing,

85
00:05:18.760 --> 00:05:20.773
but there's a lot of fluff words in there.

86
00:05:20.773 --> 00:05:25.205
Those are a lot of tokens that you're
being charged on, so you can fine tune it,

87
00:05:25.205 --> 00:05:28.665
to not add those fluff words and
still get like good accuracy.

88
00:05:28.665 --> 00:05:32.996
And using the fine tune model, depending
on which model you use is usually cheaper

89
00:05:32.996 --> 00:05:34.895
than using the non fine tune model.

90
00:05:34.895 --> 00:05:39.193
So what people do is eventually as
they get off of like a GPT model,

91
00:05:39.193 --> 00:05:41.394
they go to something like Llama.

92
00:05:41.394 --> 00:05:45.510
Or something that's like really big and
open source and they'll fine-tune it for

93
00:05:45.510 --> 00:05:49.652
their specific use case and it's fractions
cheaper than what open AI is offering.

94
00:05:49.652 --> 00:05:51.921
Plus they can put it on
their own infrastructure or

95
00:05:51.921 --> 00:05:53.496
their own cloud infrastructure.

96
00:05:53.496 --> 00:05:57.081
And they have no limits other than
whatever access to GPUs that they have, so

97
00:05:57.081 --> 00:05:59.729
that's typically the strategy
you'll see people do.

98
00:05:59.729 --> 00:06:03.362
&gt;&gt; Student: So if you wanted to
add domain knowledge to a model,

99
00:06:03.362 --> 00:06:06.160
you wouldn't do fine tuning for that?

100
00:06:06.160 --> 00:06:07.971
&gt;&gt; Scott Moss: No, yeah,
fine tune is not for

101
00:06:07.971 --> 00:06:12.573
adding domain knowledge that the model
is already tuned, It's like you would

102
00:06:12.573 --> 00:06:16.771
have to make a custom foundational
model from scratch, and anything.

103
00:06:16.771 --> 00:06:20.029
Yeah, good luck,
you got a couple 100 million couple Right.

104
00:06:20.029 --> 00:06:22.034
So yeah, not domain knowledge,

105
00:06:22.034 --> 00:06:26.403
it's just like tuning the output at
those layers that control the domain

106
00:06:26.403 --> 00:06:31.076
out have already been tuned, and
the fine tuning don't touch those layers.

107
00:06:31.076 --> 00:06:35.705
It's like touching the last bit of
layers on the end of the neural net, and

108
00:06:35.705 --> 00:06:40.051
not the hidden layers which is what
you would probably need for that.

109
00:06:40.051 --> 00:06:45.677
So no, those weights and balances
are already set, so yeah, it's mostly for

110
00:06:45.677 --> 00:06:50.980
output-related things, like how it looks,
the shape of it, the tone.

111
00:06:50.980 --> 00:06:55.875
But yeah, not the knowledge, knowledge
is RAG, you've got to use RAG for

112
00:06:55.875 --> 00:07:00.248
that, unless you use some crazy
model that has a huge token limit.

113
00:07:00.248 --> 00:07:04.139
And then again, you have of
the problem of needle in the middle,

114
00:07:04.139 --> 00:07:06.961
the haystack can't find information there.

115
00:07:06.961 --> 00:07:08.743
You still spend a lot of money on tokens,

116
00:07:08.743 --> 00:07:12.114
you're setting up your whole corpus of
data, so RAG is the answer for that.

117
00:07:12.114 --> 00:07:15.716
Yeah, talk about generated UI,
Schema-Driven UI generation,

118
00:07:15.716 --> 00:07:16.948
we talked about this.

119
00:07:16.948 --> 00:07:22.058
You can literally teach this thing how to
do grids, flex, boxes, whatever you want,

120
00:07:22.058 --> 00:07:26.341
have it generated for you, so
just think of the possibilities go crazy.

121
00:07:26.341 --> 00:07:32.257
Here's an example of using different
interactions with different triggers and

122
00:07:32.257 --> 00:07:34.962
components and different layouts.

123
00:07:34.962 --> 00:07:38.875
And you can feed this to the to the UI or
to the AI, and

124
00:07:38.875 --> 00:07:42.522
then the AI might have
a system prompt of build.

125
00:07:42.522 --> 00:07:46.112
The appropriate UI, depending on
the answer that you wanna give,

126
00:07:46.112 --> 00:07:50.085
given these things, and send back
an array of that, and it'll do it, and

127
00:07:50.085 --> 00:07:53.313
then you just render it,
you loop over, and you render it.

