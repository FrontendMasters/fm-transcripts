WEBVTT

1
00:00:00.260 --> 00:00:02.650
&gt;&gt; Scott Mos: Understanding and
implementing RAG systems.

2
00:00:02.650 --> 00:00:07.725
Retrieval, augmented generation, it's
kinda like, when Angular One came out and

3
00:00:07.725 --> 00:00:11.580
they had trans exclusion, or
what the hell was it called?

4
00:00:11.580 --> 00:00:13.420
I forgot.
It's like, why are we calling it that?

5
00:00:13.420 --> 00:00:15.060
That's kinda what RAG is.

6
00:00:15.060 --> 00:00:20.925
It's like, I get it, but in reality it's
just like the AI doesn't know something.

7
00:00:20.925 --> 00:00:25.515
So you have to go get it and,
well, let's take it a step.

8
00:00:25.515 --> 00:00:27.835
Let's go before that.

9
00:00:27.835 --> 00:00:32.539
If the, if you asked the AI about,
if you asked GPT 4 mini

10
00:00:32.539 --> 00:00:37.635
about the scores of last night's NBA game,
would it know it?

11
00:00:39.075 --> 00:00:40.717
Why would it not know it?

12
00:00:40.717 --> 00:00:43.131
&gt;&gt; Jamie: It wasn't in the training
&gt;&gt; Scott Mos: it wasn't in the training

13
00:00:43.131 --> 00:00:44.107
data, exactly.

14
00:00:44.107 --> 00:00:46.897
So there's really two things it can do.

15
00:00:46.897 --> 00:00:50.887
It can lie, which, well, I guess not lie,
cuz it doesn't know that it's lying.

16
00:00:50.887 --> 00:00:55.413
It can say something, that you know,
if you just read the sentence,

17
00:00:55.413 --> 00:00:59.410
grammatically makes sense,
but isn't actually factual.

18
00:01:00.421 --> 00:01:03.760
If, you know, the creators of GT40 Money
put some guardrails in there,

19
00:01:03.760 --> 00:01:06.463
it would reply back and say,
sorry, I don't know that, but

20
00:01:06.463 --> 00:01:08.831
those are the only two
things you're gonna get.

21
00:01:08.831 --> 00:01:13.304
It's gonna say, I don't know it, or it's
gonna say something that makes complete

22
00:01:13.304 --> 00:01:17.841
sense in whatever language you ask it to
respond with, but actually isn't factual.

23
00:01:18.881 --> 00:01:20.606
So how do you get past that?

24
00:01:20.606 --> 00:01:25.362
Do you just have to like, I don't know,
is OpenAI updating their model every

25
00:01:25.362 --> 00:01:28.371
single day and
you gotta go get the latest model?

26
00:01:28.371 --> 00:01:29.571
Like how does that work?

27
00:01:30.661 --> 00:01:33.373
No, it doesn't work that way because
training a model costs hundreds of

28
00:01:33.373 --> 00:01:34.221
millions of dollars.

29
00:01:34.221 --> 00:01:36.201
So that is not actually what's happening.

30
00:01:36.201 --> 00:01:40.101
The other thing you might think of is
like, well, maybe I'm fine tuning it.

31
00:01:40.101 --> 00:01:44.372
Which is similar to training, but it's
more like, fine tuning is like, you know,

32
00:01:44.372 --> 00:01:48.281
you can take an airplane somewhere, and
then when you get there, you can take

33
00:01:48.281 --> 00:01:52.321
an uber somewhere, and then for
the last mile, you can hop on a scooter.

34
00:01:52.321 --> 00:01:53.971
That scooter is fine tuning.

35
00:01:53.971 --> 00:01:56.732
It's just that last part,
but everything else

36
00:01:56.732 --> 00:02:01.435
before that was the training that cost
hundreds of millions of dollars, right?

37
00:02:01.435 --> 00:02:05.206
So you still aren't gonna fine
tune every single day either.

38
00:02:05.206 --> 00:02:06.066
That would be annoying.

39
00:02:06.066 --> 00:02:08.616
So you wouldn't wanna do that.

40
00:02:08.616 --> 00:02:11.366
Costs a lot of money,
needs a lot of data, takes a lot of time.

41
00:02:11.366 --> 00:02:13.776
So you wouldn't be doing
that every single day.

42
00:02:13.776 --> 00:02:19.036
So how do you get the AI to know
something that it doesn't know?

43
00:02:19.036 --> 00:02:21.593
Well, you can just tell it, right?

44
00:02:21.593 --> 00:02:26.333
The best way you can tell
the AI is in the system prompt.

45
00:02:26.333 --> 00:02:30.633
Whatever you put in the system prompt,
the AI will now have that reference.

46
00:02:30.633 --> 00:02:34.683
So if I say, here's everything you need to
know about Jamie, here's how old he is,

47
00:02:34.683 --> 00:02:38.677
here's where he works, here's his favorite
color, here's food that he likes,

48
00:02:38.677 --> 00:02:40.083
it now knows that, right?

49
00:02:40.083 --> 00:02:44.589
So, cool, if I want the AI to
know about anything related to,

50
00:02:44.589 --> 00:02:49.202
you know, a game last night,
I can just put that information.

51
00:02:49.202 --> 00:02:51.662
This is problem solved, great.

52
00:02:51.662 --> 00:02:57.332
But what if I wanted to know about
this entire book series that I read?

53
00:02:57.332 --> 00:03:02.234
And it's like, each book is a thousand
pages and there's ten books,

54
00:03:02.234 --> 00:03:06.658
there's 10,000 pages,how
many tokens would that be?

55
00:03:06.658 --> 00:03:11.381
10,000 pages every few
four letters is a token?

56
00:03:12.991 --> 00:03:14.901
Yeah, I don't think it would
fit in the context window.

57
00:03:14.901 --> 00:03:19.401
I would eventually run out and AI would
say, sorry, I have too many tokens.

58
00:03:19.401 --> 00:03:23.075
I can't do this for you, even if you had
something that was like way massive and

59
00:03:23.075 --> 00:03:26.696
it can fit a million tokens,
that would still cost you a lot of money.

60
00:03:26.696 --> 00:03:28.558
Even if it could fit all the tokens,

61
00:03:28.558 --> 00:03:32.746
you would be charged those tokens
every request that you send up.

62
00:03:32.746 --> 00:03:35.806
Which, and
because you have all those tokens,

63
00:03:35.806 --> 00:03:41.270
it's gonna be slower to get a response
back because it has to process all that.

64
00:03:41.270 --> 00:03:44.170
And you have this problem of,
I forgot the actual name of it.

65
00:03:44.170 --> 00:03:45.700
It's like, on the tip of my tongue.

66
00:03:45.700 --> 00:03:49.800
But basically, if you have, like all this
information that you gave the AI, and

67
00:03:49.800 --> 00:03:52.636
you ask it a question, and
the relevant information for

68
00:03:52.636 --> 00:03:56.890
that question is in the middle of that
information, it's never gonna find it.

69
00:03:56.890 --> 00:03:59.427
It's only really good at
finding information at, like,

70
00:03:59.427 --> 00:04:01.436
the top and the end and not in the middle.

71
00:04:01.436 --> 00:04:03.196
It kinda forgets.

72
00:04:03.196 --> 00:04:04.937
So the more information you have,

73
00:04:04.937 --> 00:04:09.026
the less likely it'll be able to give you
an answer over something in the middle.

74
00:04:09.026 --> 00:04:13.144
So it's like, it's great that
we have these new models that

75
00:04:13.144 --> 00:04:17.990
are coming out that are 128K tokens,
all these crazy token limits,

76
00:04:17.990 --> 00:04:22.937
but we still got to pay a lot of money for
that every single time.

77
00:04:22.937 --> 00:04:26.782
It's still gonna be slow, and the AI,
even though it has information, it's still

78
00:04:26.782 --> 00:04:30.377
gonna forget because that's so much
information for it to remember, right?

79
00:04:30.377 --> 00:04:33.777
So, RAG tries to solve that.

80
00:04:33.777 --> 00:04:40.330
What RAG tries to do is it tries to find
the exact information that you need or

81
00:04:40.330 --> 00:04:46.371
the AI needs to answer the question
that you asked and only that.

82
00:04:46.371 --> 00:04:50.482
So it tries to find that, grab that
information, put it in, you know,

83
00:04:50.482 --> 00:04:54.386
the response, the system prompt,
wherever you want to put it, and

84
00:04:54.386 --> 00:04:55.571
then it answers it.

85
00:04:55.571 --> 00:04:59.751
So you only have the bare minimum of
what you need to answer the question.

86
00:04:59.751 --> 00:05:03.869
So one, it's faster as far as like
when it gets to the inference level,

87
00:05:03.869 --> 00:05:07.110
it's cheaper because you're
sending up less tokens.

88
00:05:07.110 --> 00:05:10.450
And because there is less information,
less tokens in the prompt,

89
00:05:10.450 --> 00:05:13.560
it won't forget stuff because
it's not like a big text.

90
00:05:13.560 --> 00:05:14.730
So it tries to solve all of that.

91
00:05:15.770 --> 00:05:17.598
The problem is, RAG is really hard to do.

92
00:05:17.598 --> 00:05:23.515
And I would say Evals might be the number
one problem right now with LLMs,

93
00:05:23.515 --> 00:05:27.866
a very close second,
maybe even a tie is RAG.

94
00:05:27.866 --> 00:05:30.776
Those are the two most challenging
problems people are trying to solve today.

95
00:05:30.776 --> 00:05:34.008
You see a lot of companies trying to solve
this, a lot of people trying to solve

96
00:05:34.008 --> 00:05:37.192
this, a lot of research scientists
trying to figure out a new RAG approach,

97
00:05:37.192 --> 00:05:39.066
a new eval, a new RAG approach,
a new eval.

98
00:05:39.066 --> 00:05:43.233
I would say that's 90% of what's happening
today and then obviously you have

99
00:05:43.233 --> 00:05:46.851
the foundational people trying
to make it better at reasoning.

100
00:05:46.851 --> 00:05:48.931
They're like,
how do we make this thing reason better?

101
00:05:48.931 --> 00:05:51.841
How do we do more tokens
in the context window?

102
00:05:51.841 --> 00:05:53.125
But everyone else is trying
to do these two things.

