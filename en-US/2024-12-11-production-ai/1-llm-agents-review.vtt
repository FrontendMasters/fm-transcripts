WEBVTT

1
00:00:00.120 --> 00:00:01.470
&gt;&gt; Scott Moss: Let's do a quick recap.

2
00:00:01.470 --> 00:00:05.199
So again, if you don't know anything about
LLMs, agents, take the other course,

3
00:00:05.199 --> 00:00:07.420
this is just a little recap,
a little refresher.

4
00:00:07.420 --> 00:00:12.113
I have everything you need to know about
all that stuff in here, but I'm just gonna

5
00:00:12.113 --> 00:00:16.760
go over it in my own words of what an LLM
is, what an agent is, kinda those basics.

6
00:00:16.760 --> 00:00:21.611
So as you may or may not know, an LLM is
just a large language model It's some

7
00:00:21.611 --> 00:00:25.769
form of weights and balances,
which are like probabilities and

8
00:00:25.769 --> 00:00:30.389
statistics on what decisions to make
to help it predict, in this case,

9
00:00:30.389 --> 00:00:34.400
in a large language model,
predict the next word.

10
00:00:34.400 --> 00:00:37.151
And other models like
diffusion-based models,

11
00:00:37.151 --> 00:00:41.865
which are models used to generate images
like we saw, it's very similar, except for

12
00:00:41.865 --> 00:00:43.908
it's not predicting the next word.

13
00:00:43.908 --> 00:00:49.199
It's more about denoising pixels and
coming back to some image that was

14
00:00:49.199 --> 00:00:55.558
very similar to a description of another
image, but still about predicting things.

15
00:00:55.558 --> 00:00:59.017
At the end of the day, there's statistics,
there's weights and balances, and

16
00:00:59.017 --> 00:01:01.139
you have a model with neural
networks that use it.

17
00:01:01.139 --> 00:01:06.535
The thing that's special about LLMS is
they have something called attention,

18
00:01:06.535 --> 00:01:11.205
which basically just allows it to
keep in memory the whole sentence,

19
00:01:11.205 --> 00:01:15.729
the whole set of characters while
predicting the next character.

20
00:01:15.729 --> 00:01:20.767
Whereas previous attempts
of text prediction based

21
00:01:20.767 --> 00:01:27.302
AI models did not have the ability
to keep the entire part in memory.

22
00:01:27.302 --> 00:01:31.387
They only knew what the previous letter
was and not the whole sentence before it,

23
00:01:31.387 --> 00:01:33.162
and every letter before it.

24
00:01:33.162 --> 00:01:35.742
So that is the innovation here.

25
00:01:37.352 --> 00:01:39.803
And then a agents kind of build on LLMS.

26
00:01:39.803 --> 00:01:43.846
So you think of LLMs are just like,
you'll hear these terms a lot.

27
00:01:43.846 --> 00:01:46.076
You'll hear single turn and multi turn.

28
00:01:46.076 --> 00:01:51.476
So single turn it's just a, what I'd
like to call a transactional LLM call.

29
00:01:51.476 --> 00:01:56.091
It's very similar to what you would do
in cursor or VS Code or GitHub Copilot,

30
00:01:56.091 --> 00:02:00.394
where you say, do this thing, and
it does the thing and it's done.

31
00:02:00.394 --> 00:02:01.774
Agent is multi turn.

32
00:02:01.774 --> 00:02:05.294
So, you saw how I was able to do
the approval, generate an image, and

33
00:02:05.294 --> 00:02:06.524
do this stuff.

34
00:02:06.524 --> 00:02:11.004
That is the agent taking,
it's calling an LLM multiple times.

35
00:02:11.004 --> 00:02:13.774
It's getting the results,
it's feeding it back.

36
00:02:13.774 --> 00:02:15.454
It's thinking, it's getting a result.

37
00:02:15.454 --> 00:02:17.924
It's feeding it back until it
doesn't have to think anymore.

38
00:02:17.924 --> 00:02:19.144
So, that's kind of what an agent is.

39
00:02:19.144 --> 00:02:20.566
It's an LLM.

40
00:02:20.566 --> 00:02:23.888
On a loop that has typically
access to tools, and

41
00:02:23.888 --> 00:02:27.356
also in the form of
a chat-based agent memory.

42
00:02:27.356 --> 00:02:29.978
So it can remember all the messages
that you've said before.

