WEBVTT

1
00:00:00.000 --> 00:00:02.713
&gt;&gt; Richard Feldman: Okay, so I did
a little recap as part of our wrap up and

2
00:00:02.713 --> 00:00:06.173
then basically just do a little
bit of further learning if there's

3
00:00:06.173 --> 00:00:09.410
other resources that you're
interested in on these topics.

4
00:00:09.410 --> 00:00:13.418
So at the very beginning, we talked about
how basically the fundamental job of

5
00:00:13.418 --> 00:00:16.820
a compiler, whether it's ahead of time or
runtime interpreter,

6
00:00:16.820 --> 00:00:20.064
is to go from source representation
to machine instructions.

7
00:00:20.064 --> 00:00:23.335
Talked about how there's different
arguments about terminology is

8
00:00:23.335 --> 00:00:26.222
an assembler really a compiler or
is that a different thing?

9
00:00:26.222 --> 00:00:27.972
I'm not gonna take any stances on that,
but

10
00:00:27.972 --> 00:00:30.826
the main thing is just like head of
time compiler runtime interpreter.

11
00:00:30.826 --> 00:00:32.535
Pretty well understood what those mean.

12
00:00:32.535 --> 00:00:35.947
We also talked about just-in-time
compilers, such as WebAssembly, which we

13
00:00:35.947 --> 00:00:39.560
actually got to build something with, as
well as an optimization for interpreters,

14
00:00:39.560 --> 00:00:43.341
which is the most common way that they're
used, such as JavaScript, PHP, Ruby, etc.

15
00:00:43.341 --> 00:00:45.382
We talked about these different terms, and

16
00:00:45.382 --> 00:00:49.315
sort of like assembler is low level to low
level, you have a really low abstraction.

17
00:00:49.315 --> 00:00:53.067
It is human readable, but it is very,
very close to the middle language that's

18
00:00:53.067 --> 00:00:56.875
compiling to something that's definitely
not human readable transpiler being

19
00:00:56.875 --> 00:01:00.571
something that's going from a high level
language that is designed to be human

20
00:01:00.571 --> 00:01:04.391
readable to another high level language
that's designed to be human readable.

21
00:01:04.391 --> 00:01:08.209
I talked about interpreters, which are
generating the machine code at runtime.

22
00:01:08.209 --> 00:01:12.623
Compilers are basically taking the input
and generating the output at compile time.

23
00:01:12.623 --> 00:01:15.782
We talked about some compiler passes and
we saw a variety of them.

24
00:01:15.782 --> 00:01:18.849
We talked about tokenizing,
aka Lexing that feeds into parsing,

25
00:01:18.849 --> 00:01:22.245
followed by name resolution or
canonicalization, type Inference, and

26
00:01:22.245 --> 00:01:23.739
then finally code generation.

27
00:01:23.739 --> 00:01:27.302
We talked about basically if you're
generating code as a string,

28
00:01:27.302 --> 00:01:29.219
that's essentially formatting.

29
00:01:29.219 --> 00:01:33.490
So you're going from source string over to
tokens via Lexing, then tokens to parse

30
00:01:33.490 --> 00:01:37.903
tree, that's parsing, name resolution,
type inference, and finally code gen.

31
00:01:37.903 --> 00:01:41.843
And if what you end up at the end of all
that is a string, we call that formatting.

32
00:01:41.843 --> 00:01:45.343
But if we're not compiling to that and
everything else is exactly the same,

33
00:01:45.343 --> 00:01:49.295
then now we call it, it's not formatting,
it's a compiler or something like that.

34
00:01:49.295 --> 00:01:52.332
And that's where we ended up doing
all of these different stages,

35
00:01:52.332 --> 00:01:55.968
spending the most time on type inference,
granted, but still getting a little

36
00:01:55.968 --> 00:01:59.302
bit of a taste of lexing and
parsing at the beginning, name resolution.

37
00:01:59.302 --> 00:02:01.842
And then at the very end,
some code gen to WebAssembly.

38
00:02:01.842 --> 00:02:05.260
Really the most classic book in all
the history of compilers is this book.

39
00:02:05.260 --> 00:02:06.785
It's known as the dragon book.

40
00:02:06.785 --> 00:02:08.737
I actually don't know what its name is.

41
00:02:08.737 --> 00:02:11.604
I'm looking at it and
I still don't know what its name is.

42
00:02:11.604 --> 00:02:14.532
It's just the dragon book,
that's what that's what it is.

43
00:02:14.532 --> 00:02:17.765
When I wanted to find an image for
this, I just Googled dragon book.

44
00:02:17.765 --> 00:02:20.777
This book is very famous and
also very old and out of date, so

45
00:02:20.777 --> 00:02:24.404
I hesitate to recommend it and
except in the sense that it's a classic.

46
00:02:24.404 --> 00:02:27.305
So I'm not saying you should read this and
then be like,

47
00:02:27.305 --> 00:02:29.419
this is the best way to build compilers.

48
00:02:29.419 --> 00:02:33.504
But it is maybe worth your while to read
it just because of its classic status.

49
00:02:33.504 --> 00:02:36.837
A book that I have not read, but
which I have heard recommended as

50
00:02:36.837 --> 00:02:41.307
an alternative to the dragon book is this
one, it's called Engineering A Compiler.

51
00:02:41.307 --> 00:02:46.383
I can't first, recommend it cuz I haven't
read it, but I have heard people say that

52
00:02:46.383 --> 00:02:51.174
if you want something that's more focused
on, hey, this is a practical modern

53
00:02:51.174 --> 00:02:56.069
thing for people who are application
developers, I'm assuming most of us are.

54
00:02:56.069 --> 00:03:00.269
Then this is like kind of a more
reasonable book to get more things.

55
00:03:00.269 --> 00:03:04.332
This is also not something that I have
personally read, but I have a very strong

56
00:03:04.332 --> 00:03:08.232
recommendation from one of the best
compiler authors I know, Ayaz Hafiz.

57
00:03:08.232 --> 00:03:11.026
He was like, this is the book
if you wanna do get really,

58
00:03:11.026 --> 00:03:12.992
really hardcore into type inference.

59
00:03:12.992 --> 00:03:16.408
I'm embarrassed to say I bought it and
have read two pages of it, so

60
00:03:16.408 --> 00:03:17.691
I can't endorse it yet.

61
00:03:17.691 --> 00:03:20.376
But I mean, so
far the two pages were stellar so.

62
00:03:20.376 --> 00:03:22.446
But yeah, if you wanna get really,

63
00:03:22.446 --> 00:03:25.965
really deep into all the nuances
of different type inference

64
00:03:25.965 --> 00:03:30.255
implementations and ways you can go
about optimizing different things.

65
00:03:30.255 --> 00:03:32.927
This apparently is a really great book for
that.

66
00:03:32.927 --> 00:03:36.906
And finally, if I could only leave you
with one thing to take away from this

67
00:03:36.906 --> 00:03:40.306
is this theme of traverse the input,
generate the output, and

68
00:03:40.306 --> 00:03:44.178
then traverse the thing that the previous
step Gib is really powerful.

69
00:03:44.178 --> 00:03:45.746
It's not just a compiler's thing.

70
00:03:45.746 --> 00:03:48.073
I mean, it is powerful in compilers,
of course, and

71
00:03:48.073 --> 00:03:49.379
we got a lot of value out of it.

72
00:03:49.379 --> 00:03:51.979
But it is something that comes
up even outside compilers,

73
00:03:51.979 --> 00:03:53.314
in all sorts of programming.

74
00:03:53.314 --> 00:03:56.176
And Steve Yegge had a famous blog
post where he's talked about,

75
00:03:56.176 --> 00:03:59.800
once you've done some compiler stuff,
you start to see compilers everywhere.

76
00:03:59.800 --> 00:04:02.440
And I think this is the main
thing that he's talking about.

77
00:04:02.440 --> 00:04:06.171
[LAUGH] There's so many problems in
software that just break down into like,

78
00:04:06.171 --> 00:04:08.633
can I get this into a more
useful representation?

79
00:04:08.633 --> 00:04:11.999
And then can I get from that useful
representation into an even more useful

80
00:04:11.999 --> 00:04:12.874
representation?

81
00:04:12.874 --> 00:04:16.009
Really at the end of the day,
that's what we've been doing all day,

82
00:04:16.009 --> 00:04:18.930
is we've started with a representation
that's nice to read but

83
00:04:18.930 --> 00:04:22.662
doesn't actually run all the way in
through something where we've analyzed it,

84
00:04:22.662 --> 00:04:25.608
we got outputs in the form of
naming errors and type mismatches.

85
00:04:25.608 --> 00:04:26.807
We did type inference and

86
00:04:26.807 --> 00:04:29.891
then finally ended up with something
that could actually run and

87
00:04:29.891 --> 00:04:33.451
actually run faster than the source
code that we put in on the first place.

88
00:04:33.451 --> 00:04:36.475
WebAssembly, generally speaking,
runs faster than Javascript.

89
00:04:36.475 --> 00:04:38.303
So yeah, this idea, powerful idea, and

90
00:04:38.303 --> 00:04:41.875
if there's one thing you can take away
from today, I would hope it would be this.

91
00:04:41.875 --> 00:04:43.352
&gt;&gt; Speaker 2: Some final, Questions here.

92
00:04:43.352 --> 00:04:43.910
&gt;&gt; Richard Feldman: Yeah, sure.

93
00:04:43.910 --> 00:04:46.838
&gt;&gt; Speaker 2: How are recursive
types handled by type inference?

94
00:04:46.838 --> 00:04:50.533
&gt;&gt; Richard Feldman: [LAUGH] How are
recursive types handled by type inference?

95
00:04:50.533 --> 00:04:55.341
That was the first thing I'm like, we're
definitely not doing that in the workshop.

96
00:04:55.341 --> 00:04:57.710
I mean, the short answer is complicatedly.

97
00:04:57.710 --> 00:05:02.039
It absolutely works, but that's where
you get all sorts of gotchas around,

98
00:05:02.039 --> 00:05:04.244
I mentioned the occurs check earlier.

99
00:05:04.244 --> 00:05:06.734
That's really important for
recursive types.

100
00:05:06.734 --> 00:05:10.115
Also, when you get to code generation,
they're a total nightmare.

101
00:05:10.115 --> 00:05:13.542
And a lot of cases, especially if
you're doing monomorphization,

102
00:05:13.542 --> 00:05:16.685
the short answer is,
there's just a lot of complexity there.

103
00:05:16.685 --> 00:05:20.524
I don't have a concise answer the question
of, how do you do other than just,

104
00:05:20.524 --> 00:05:21.656
how much time you got?

105
00:05:21.656 --> 00:05:22.551
[LAUGH] Yeah.

106
00:05:22.551 --> 00:05:26.962
&gt;&gt; Speaker 3: So I don't know a whole lot
about LLVM, but how hard would it be to

107
00:05:26.962 --> 00:05:31.949
take that kind of intermediate
representation we made and plug that into

108
00:05:31.949 --> 00:05:36.873
LLVM, and once you have that is that
support generating WebAssembly.

109
00:05:36.873 --> 00:05:40.042
&gt;&gt; Richard Feldman: It does, yeah,
okay, so LLVM is also a deep topic.

110
00:05:40.042 --> 00:05:42.781
I can give you some
brief points on that and

111
00:05:42.781 --> 00:05:45.682
then also we can talk
more after if you want.

112
00:05:45.682 --> 00:05:49.986
So the question was, so LLVM is
a compiler back end that its job is

113
00:05:49.986 --> 00:05:55.119
essentially you give it an intermediate
representation that it controls.

114
00:05:55.119 --> 00:05:59.306
It says, hey, instead of generating web
assembly, you generate web LLVM IR.

115
00:05:59.306 --> 00:06:02.563
And then from there you can
tell LLVM it's a library.

116
00:06:02.563 --> 00:06:06.325
Basically say, hey,
generate me some WebAssembly, literally,

117
00:06:06.325 --> 00:06:07.783
LLVM will do that for you.

118
00:06:07.783 --> 00:06:10.917
Also you can say,
generate me x86-64 machine code, or

119
00:06:10.917 --> 00:06:13.443
generate me ARM64 machine code, or RISC-V.

120
00:06:13.443 --> 00:06:16.720
And then they have really long
lists of generation targets.

121
00:06:16.720 --> 00:06:19.699
On top of which the thing
that LLVM is most known for,

122
00:06:19.699 --> 00:06:23.163
the second most thing is known for
is running really slowly.

123
00:06:23.163 --> 00:06:26.382
But the thing is most known for
is its optimization passes.

124
00:06:26.382 --> 00:06:30.610
So in addition to giving it the code and
saying, hey, generate me this code, first,

125
00:06:30.610 --> 00:06:33.223
you can say, really,
really optimize this thing.

126
00:06:33.223 --> 00:06:37.675
And LLVM is basically the gold standard in
terms of doing the best job of cranking on

127
00:06:37.675 --> 00:06:41.545
your code and getting it to be so
that what the machine code that comes out

128
00:06:41.545 --> 00:06:45.248
the other side runs as fast as you
can get from pretty much anything.

129
00:06:45.248 --> 00:06:49.046
Unfortunately, the downside is that LLVM
takes a long time to give you that code

130
00:06:49.046 --> 00:06:51.095
even if you're not optimizing anything.

131
00:06:51.095 --> 00:06:54.504
So it's because of architectural
problems in LLVM rather than,

132
00:06:54.504 --> 00:06:57.416
they're just working really
hard on your optimizations.

133
00:06:57.416 --> 00:07:00.249
No, you can say zero optimization and
it's still very slow.

134
00:07:00.249 --> 00:07:02.587
But, yes, you can totally do that for
WebAssembly.

135
00:07:02.587 --> 00:07:04.806
How hard is it is a different question.

136
00:07:04.806 --> 00:07:07.098
So, yeah, the ergonomics are not amazing.

137
00:07:07.098 --> 00:07:10.240
It's a similar level of abstraction,
I would say,

138
00:07:10.240 --> 00:07:14.598
overall to WebAssembly in terms of
they're really not giving you a lot of

139
00:07:14.598 --> 00:07:17.814
conveniences compared to if
you were compiling to C or

140
00:07:17.814 --> 00:07:22.686
compiling to Javascript or something
like that, like a higher level language.

141
00:07:22.686 --> 00:07:27.458
It's quite low level, there are some
conviniences but, yeah, you still have to

142
00:07:27.458 --> 00:07:31.698
do basically pre-similar stuff to
what we just said with WebAssembly.

143
00:07:31.698 --> 00:07:36.312
The difference being that then you have
to bring in the LLVM independency and

144
00:07:36.312 --> 00:07:40.942
we first along compile times and
dealing with its' api and stuff like that.

145
00:07:40.942 --> 00:07:44.714
So my preference would be kind of
just go straight to WebAssembly for

146
00:07:44.714 --> 00:07:45.980
something like this.

147
00:07:45.980 --> 00:07:48.574
But if you wanted to,
you absolutely could in theory.

148
00:07:48.574 --> 00:07:52.893
And also I didn't wanna wire up no js to
LLVM [LAUGH] that doesn't sound fun, but

149
00:07:52.893 --> 00:07:54.934
in theory, yeah, you absolutely.

150
00:07:54.934 --> 00:07:58.328
If you wanted to, you could take
this compiler we just built,

151
00:07:58.328 --> 00:08:00.680
take the exact representations we made and

152
00:08:00.680 --> 00:08:05.202
translate them into LLVM IR instead into
WebAssembly, that would totally work.

153
00:08:05.202 --> 00:08:06.697
Yeah, other questions?

154
00:08:06.697 --> 00:08:08.668
Was there one online?

155
00:08:08.668 --> 00:08:10.541
Do I remember that, right?

156
00:08:10.541 --> 00:08:15.225
&gt;&gt; Speaker 4: Yeah, could you talk about
the limitations of type inference?

157
00:08:15.225 --> 00:08:17.976
&gt;&gt; Richard Feldman: The limitations
of type inference,

158
00:08:17.976 --> 00:08:22.156
I would say it depends on
the type inference algorithm.

159
00:08:22.156 --> 00:08:23.977
Limitations of Hindley-Milner?

160
00:08:23.977 --> 00:08:27.467
I mean, I guess it's mainly opportunity
costs, just in the sense of,

161
00:08:27.467 --> 00:08:31.014
if you choose Henley Miller for
your type system, you get type inference

162
00:08:31.014 --> 00:08:34.213
that I don't know if it would be
hard pressed to think of something

163
00:08:34.213 --> 00:08:38.196
where I could say that Hindley-Milner's
type inference has its limitations.

164
00:08:38.196 --> 00:08:40.540
Well, it's principal
decidable type inference,

165
00:08:40.540 --> 00:08:42.517
I don't know what else
you want [INAUDIBLE].

166
00:08:42.517 --> 00:08:47.466
It doesn't get much better than like it
always correctly infers 100% of your types

167
00:08:47.466 --> 00:08:50.601
and it always correctly
infers the most general type.

168
00:08:50.601 --> 00:08:51.539
I'm at a loss for

169
00:08:51.539 --> 00:08:55.505
what you could do better on the specific
side of type inference there.

170
00:08:55.505 --> 00:08:58.338
Mind reading type inference
might be better, yeah, yeah.

171
00:08:58.338 --> 00:09:01.864
&gt;&gt; Speaker 5: I would personally argue
that resolving into generic types is

172
00:09:01.864 --> 00:09:04.524
a suboptimal idea and
maybe that's an angle.

173
00:09:04.524 --> 00:09:06.457
&gt;&gt; Richard Feldman: Okay, but
what would you do instead?

174
00:09:06.457 --> 00:09:08.390
&gt;&gt; Speaker 5: Yeah,
well you have to design, right?

175
00:09:08.390 --> 00:09:12.466
There's a design choice in there,
you can't fully genericize, but yeah.

176
00:09:12.466 --> 00:09:15.976
&gt;&gt; Richard Feldman: Okay, I mean, yeah,
so if you don't like it being so general,

177
00:09:15.976 --> 00:09:19.300
yes, then I guess that maybe
you would want something else.

178
00:09:19.300 --> 00:09:23.211
But to me, I would think that
the trade-offs are more, at least to me

179
00:09:23.211 --> 00:09:28.001
personally, I would say that the type
inference capabilities of Hindley-Millner

180
00:09:28.001 --> 00:09:32.261
are sort of an almost an unalloyed good,
that's just a big selling point.

181
00:09:32.261 --> 00:09:36.089
The downsides would be more that, well,
yes, but if you're buying into this

182
00:09:36.089 --> 00:09:40.149
system, there are these other things that
are harder, that are outside the remote

183
00:09:40.149 --> 00:09:42.707
type inference,
subtyping being the biggest one.

184
00:09:42.707 --> 00:09:46.553
And there are lots of nice benefits to
subtyping, also some downsides like

185
00:09:46.553 --> 00:09:50.417
covariance and contravariance are become
things you have to worry about.

186
00:09:50.417 --> 00:09:54.937
But yeah, in general, so I don't know if
this is what the comment was referencing,

187
00:09:54.937 --> 00:09:59.267
but anything that I have heard people say
is, okay, I don't like type inference

188
00:09:59.267 --> 00:10:02.514
because if I don't have types anywhere,
then it's hard for

189
00:10:02.514 --> 00:10:06.968
me to understand what things are going on,
or I get spooky action at a distance where

190
00:10:06.968 --> 00:10:11.340
I change one type over here and it affects
some type that's very distant to that.

191
00:10:11.340 --> 00:10:15.561
The problem with that to in my mind is
that it basically is saying, okay, so

192
00:10:15.561 --> 00:10:18.375
if you have the capability
to choose to annotate or

193
00:10:18.375 --> 00:10:22.395
not annotate your types as much or
as little as you want, and you make bad

194
00:10:22.395 --> 00:10:26.294
choices, the thing that allows you
to make those bad choices is bad.

195
00:10:26.294 --> 00:10:30.415
I don't know, in some senses that logic
makes sense to me when I'm trying

196
00:10:30.415 --> 00:10:33.014
to make good choices,
but it's error prone.

197
00:10:33.014 --> 00:10:33.554
So again,

198
00:10:33.554 --> 00:10:37.394
we've taught a workshop on C it's pretty
hard to get right a lot of the time.

199
00:10:37.394 --> 00:10:41.215
But the very, very simple heuristic
of just always annotate your

200
00:10:41.215 --> 00:10:46.146
top level types is something that pretty
much all professionals I've worked with,

201
00:10:46.146 --> 00:10:48.239
do in languages that support this.

202
00:10:48.239 --> 00:10:52.106
But then if you do that, you're not any
worse off when it comes to top level types

203
00:10:52.106 --> 00:10:53.742
than languages that require it.

204
00:10:53.742 --> 00:10:55.779
So I don't see how that's a downside,
really.

205
00:10:55.779 --> 00:10:59.734
But then if you're writing, a small script
where I actually would prefer not to do

206
00:10:59.734 --> 00:11:02.384
it here,
then it's convenient that I don't have to.

207
00:11:02.384 --> 00:11:06.818
So I don't know, I have a tough time
buying the argument that it's a downside,

208
00:11:06.818 --> 00:11:11.186
that it gives you that much flexibility,
because the amount of discipline and

209
00:11:11.186 --> 00:11:15.377
carefulness you need to use to not have
that downside materialize is zero.

210
00:11:15.377 --> 00:11:19.825
So anyway, [LAUGH] I don't know if that
was what the question was getting at,

211
00:11:19.825 --> 00:11:21.506
but that's my answer to it.

212
00:11:22.648 --> 00:11:25.071
&gt;&gt; Speaker 2: What do you
think about the SCIP book.

213
00:11:25.071 --> 00:11:26.611
&gt;&gt; Richard Feldman: SCIP,
so SCIP structure and

214
00:11:26.611 --> 00:11:28.803
interpretation of computer programs.

215
00:11:28.803 --> 00:11:32.790
I mean, it doesn't really have
anything to do with compilers, really.

216
00:11:32.790 --> 00:11:36.776
It's been a long time since I don't
even remember if I read all of it.

217
00:11:36.776 --> 00:11:41.697
I don't know, a lot of people talk about
it as it changed the way they looked at

218
00:11:41.697 --> 00:11:44.140
programming, they really loved it.

219
00:11:44.140 --> 00:11:47.420
It wasn't really for me,
they talk a lot about,

220
00:11:47.420 --> 00:11:51.820
I don't know, the elegance and
beauty of code and I don't know,

221
00:11:51.820 --> 00:11:57.024
when I read stuff like that, I just see
slow programs that I don't wanna use.

222
00:11:57.024 --> 00:11:59.224
[LAUGH] And
that was kind of the vibe for me.

223
00:11:59.224 --> 00:12:04.205
But I appreciate that a lot of other
people have a very different vibe from it.

224
00:12:04.205 --> 00:12:07.531
And I don't think there's any
type checking in that book, but

225
00:12:07.531 --> 00:12:11.171
I assume that there is based on
the interpretation part of the title,

226
00:12:11.171 --> 00:12:12.893
they do get into interpreters.

227
00:12:12.893 --> 00:12:15.251
But there's a number of good,
I don't know,

228
00:12:15.251 --> 00:12:17.798
highly respected interpreters
books right now.

229
00:12:17.798 --> 00:12:19.351
Was it grokking interpreters?

230
00:12:19.351 --> 00:12:21.789
I'm gonna blank on the name, but anyway,

231
00:12:21.789 --> 00:12:26.668
there's some good interpreters book that a
lot of people seem to like by Bob Nystrom,

232
00:12:26.668 --> 00:12:30.429
crafting interpreters,
that's one, yeah, thanks, right?

233
00:12:30.429 --> 00:12:34.322
Grokking simplicity, an unrelated book,
yeah, Eric Normand, yeah.

234
00:12:34.322 --> 00:12:36.077
Also, I hit him on the podcast.

235
00:12:36.077 --> 00:12:38.521
Other questions, I think there was, yeah.

236
00:12:38.521 --> 00:12:42.649
&gt;&gt; Speaker 6: Yeah, I just kind of wanna
go back to the kind of comparison you made

237
00:12:42.649 --> 00:12:44.883
about kind of formatters, right?

238
00:12:44.883 --> 00:12:47.454
And how they're kind of comparable
to compilers in a sense, or

239
00:12:47.454 --> 00:12:48.578
there's a lot of overlap.

240
00:12:48.578 --> 00:12:51.350
And I guess another thing that comes
to mind is like linters, right?

241
00:12:51.350 --> 00:12:55.463
I guess, I'm curious to know what are some
other maybe underutilized kind of maybe

242
00:12:55.463 --> 00:12:59.459
not necessarily, I don't know the right
term for comparisons or comparable use

243
00:12:59.459 --> 00:13:03.573
cases for just parsing and things that are
adjacent to compiling, that you could do

244
00:13:03.573 --> 00:13:05.356
with code more generally,
&gt;&gt; Richard Feldman: Yeah,

245
00:13:05.356 --> 00:13:09.176
it's a good question, I mean,
I haven't sat down and made a list.

246
00:13:09.176 --> 00:13:12.816
I have to think about that,
I don't know, I thought about these,

247
00:13:12.816 --> 00:13:14.972
because I think about compilers a lot.

248
00:13:14.972 --> 00:13:17.930
[LAUGH] But as far as other things, yeah,

249
00:13:17.930 --> 00:13:20.983
I don't have a list off
the top of my head.

250
00:13:20.983 --> 00:13:22.200
I'll think about it.

251
00:13:22.200 --> 00:13:23.202
Thanks very much.

252
00:13:23.202 --> 00:13:26.063
&gt;&gt; [APPLAUSE]

