WEBVTT

1
00:00:00.111 --> 00:00:02.276
&gt;&gt; Todd Gardner: Next,
we're gonna talk about tests and tools.

2
00:00:02.276 --> 00:00:06.164
So how to actually test for these metrics,
how to gather this data and

3
00:00:06.164 --> 00:00:08.175
how to interpret it as part of that,

4
00:00:08.175 --> 00:00:11.408
we're gonna need to talk
a little bit about statistics.

5
00:00:11.408 --> 00:00:14.808
Now, this isn't gonna be like
a seventh grade math course.

6
00:00:14.808 --> 00:00:18.367
We're just gonna do just
enough to get us by so

7
00:00:18.367 --> 00:00:22.313
don't feel too bad you
didn't do your homework.

8
00:00:22.313 --> 00:00:27.824
So we're gonna talk about testing methods
like, how do we go and look at this data?

9
00:00:27.824 --> 00:00:30.847
Then we're gonna look at some
common tools that we use, and

10
00:00:30.847 --> 00:00:34.354
then we're gonna talk briefly about
real user monitoring, which is

11
00:00:34.354 --> 00:00:38.483
a classification of tool, that can help
you gather a bunch of interesting data.

12
00:00:38.483 --> 00:00:40.668
So first, testing performance.

13
00:00:40.668 --> 00:00:44.308
How do we go about thinking
about capturing data?

14
00:00:44.308 --> 00:00:46.437
Where do we get the data from?

15
00:00:46.437 --> 00:00:50.199
So there's a couple of different
approaches to this, and

16
00:00:50.199 --> 00:00:53.052
each has a little bit different drawbacks.

17
00:00:53.052 --> 00:00:56.660
So there's a couple of different
ways we can test performance.

18
00:00:56.660 --> 00:00:59.999
It's important to think about this,
because how we measure performance?

19
00:00:59.999 --> 00:01:05.345
Where we choose to measure from can
give us drastically different results.

20
00:01:05.345 --> 00:01:07.954
Fundamentally, what we're doing
is we have a host somewhere.

21
00:01:07.954 --> 00:01:12.791
This could be a server, it could be your
cloud, it could be the hosting service

22
00:01:12.791 --> 00:01:17.631
you provide, and that has all your
content, and then you're delivering that

23
00:01:17.631 --> 00:01:22.348
content across the Internet to your
users and rendering it on their device.

24
00:01:22.348 --> 00:01:26.223
And so there's a couple of
different places we could test that

25
00:01:26.223 --> 00:01:29.055
web performance from, the first place, and

26
00:01:29.055 --> 00:01:33.844
the most common place that most people
do it is with what's called lab data.

27
00:01:33.844 --> 00:01:36.092
Is they create a test device and

28
00:01:36.092 --> 00:01:40.426
it's located usually somewhere
really close to the host.

29
00:01:40.426 --> 00:01:44.907
Close in terms of like network or
connectivity or whatever for

30
00:01:44.907 --> 00:01:49.042
example, when I'm running
our sticker store locally,

31
00:01:49.042 --> 00:01:53.538
I'm literally testing
the performance from the same place.

32
00:01:53.538 --> 00:01:58.381
It's not needing to cross any networks or
do anything that would be lab data.

33
00:01:58.381 --> 00:02:04.260
So we have a test device, we render it and
we gather some metrics about it.

34
00:02:04.260 --> 00:02:09.171
Another place that's really common place
to test is what's called synthetic data,

35
00:02:09.171 --> 00:02:13.602
as we have some robots some service out
in the internet somewhere that we say,

36
00:02:13.602 --> 00:02:16.218
hey go to that website and
tell me how fast is.

37
00:02:16.218 --> 00:02:20.269
Now that's a little bit better,
it's a little bit more accurate because it

38
00:02:20.269 --> 00:02:22.560
crosses a real network
in order to test it.

39
00:02:22.560 --> 00:02:28.240
But generally, these test devices are just
a server like another high-end machine

40
00:02:28.240 --> 00:02:33.772
on a high-end network somewhere out on
the Internet that is rendering your page.

41
00:02:33.772 --> 00:02:35.279
We're not really,

42
00:02:35.279 --> 00:02:40.610
neither of these two approaches
are testing what the users are seeing.

43
00:02:40.610 --> 00:02:45.351
We're both just taking like we're
cutting out a bunch of steps and

44
00:02:45.351 --> 00:02:50.021
creating a simulation of what we
think the site is gonna be doing.

45
00:02:50.021 --> 00:02:54.008
The third way that we can capture
data is called field data.

46
00:02:54.008 --> 00:02:59.073
And that's what we were talking
about a little bit this morning,

47
00:02:59.073 --> 00:03:03.955
is we can capture those performance
metrics from the real users

48
00:03:03.955 --> 00:03:08.762
using your site and
send that data off to a reporting service.

49
00:03:08.762 --> 00:03:12.839
Now this is the most accurate
way because it is reality.

50
00:03:12.839 --> 00:03:15.905
It's these are the people
that visit your site and

51
00:03:15.905 --> 00:03:18.754
this is the performance
experience they had.

52
00:03:18.754 --> 00:03:21.918
Everything else was a diagnostic,
was a test.

53
00:03:21.918 --> 00:03:25.465
But this sort of field data is real.

54
00:03:25.465 --> 00:03:28.128
It's gathering this real data.

55
00:03:28.128 --> 00:03:31.070
Now there's huge differences in this.

56
00:03:31.070 --> 00:03:33.736
And it's important to note that
there's a sample size involved.

57
00:03:33.736 --> 00:03:40.587
So this is the same, this is performance
data from the exact same website.

58
00:03:40.587 --> 00:03:44.951
On the left is a Chrome Lighthouse report,
which is a tool that we'll talk about.

59
00:03:44.951 --> 00:03:47.514
And a Chrome Lighthouse
report is lab data.

60
00:03:47.514 --> 00:03:52.945
It's, I am running a test from my
laptop against a server that's

61
00:03:52.945 --> 00:03:58.390
probably close to where my laptop
is maybe hosted from my laptop.

62
00:03:58.390 --> 00:04:01.948
And it generated
a performance score of 100,

63
00:04:01.948 --> 00:04:06.480
which means that time,
that one time performance was great.

64
00:04:06.480 --> 00:04:11.456
On the right are the core web vital
data from the real users that visit that

65
00:04:11.456 --> 00:04:14.977
site and
notice that there's not one score there.

66
00:04:14.977 --> 00:04:20.879
There's thousands of scores representing
every single user who visited.

67
00:04:20.879 --> 00:04:25.142
Now this is two different
views of the same website, and

68
00:04:25.142 --> 00:04:30.599
on the left we have lab data saying
you got a perfect performance score.

69
00:04:30.599 --> 00:04:33.416
And on the right we have
field data saying, hey,

70
00:04:33.416 --> 00:04:37.565
there's a lot of people that have
a crappy experience on your website.

71
00:04:37.565 --> 00:04:42.342
Now, it's not saying they
all do because some people.

72
00:04:42.342 --> 00:04:44.796
Some people had a great experience.

73
00:04:44.796 --> 00:04:50.727
It does happen sometimes, but a lot of
people had a really bad experience.

74
00:04:50.727 --> 00:04:55.191
And so we have to think
about how we look at data.

75
00:04:55.191 --> 00:05:00.931
When we're talking about lab data,
we have one sample.

76
00:05:00.931 --> 00:05:05.363
You run a test, you get one performance
score back, and it says something,

77
00:05:05.363 --> 00:05:07.992
it says 100, it says 95, it says 80,

78
00:05:07.992 --> 00:05:12.306
it says some metric that you're tracking
and it gives you one data point.

79
00:05:12.306 --> 00:05:15.412
But when we're looking at field data,
we get lots of data points.

80
00:05:15.412 --> 00:05:17.581
You have one score, one value for

81
00:05:17.581 --> 00:05:23.096
every single user who visits the page over
whatever time period you're looking at.

82
00:05:23.096 --> 00:05:28.964
And we have to interpret this data
because these could be performance

83
00:05:28.964 --> 00:05:34.033
data from the same website, but
how do we think about this?

