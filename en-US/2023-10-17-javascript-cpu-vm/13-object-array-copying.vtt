WEBVTT

1
00:00:00.100 --> 00:00:04.546
JSX ,so JSX the way it is
written is that the JSX

2
00:00:04.546 --> 00:00:08.542
converts into object literals, right?

3
00:00:08.542 --> 00:00:11.908
And these object literals,
it's kind of sorry hard to see,

4
00:00:11.908 --> 00:00:14.689
these object literals have properties and
keys.

5
00:00:14.689 --> 00:00:20.086
And now that you know everything we talked
about about how this means that every

6
00:00:20.086 --> 00:00:25.402
single element inside of JSX really
will have a different set of attributes,

7
00:00:25.402 --> 00:00:29.760
which means it will have
a different hidden shape, right?

8
00:00:29.760 --> 00:00:31.865
So you can see how that's
gonna become a problem.

9
00:00:31.865 --> 00:00:34.315
And so,
what does the system wanna do internally?

10
00:00:34.315 --> 00:00:38.173
Well, the system wants to convert
these into attributes in the DOM,

11
00:00:38.173 --> 00:00:42.098
which means that we're gonna do a lot
of iterations over these keys and

12
00:00:42.098 --> 00:00:44.045
lots of reads of these keys, etc.

13
00:00:44.045 --> 00:00:48.715
And so, all of this together conspires
against us, right, and makes it so

14
00:00:48.715 --> 00:00:52.660
that this thing is not the most
performing way of doing this.

15
00:00:52.660 --> 00:00:54.908
Now, in the grand scheme of
things turns out DOM is so

16
00:00:54.908 --> 00:00:57.320
slow that it doesn't really
matter how fast the JSX is.

17
00:00:59.140 --> 00:01:01.600
But it is things things to keep in mind.

18
00:01:01.600 --> 00:01:02.390
And again,

19
00:01:02.390 --> 00:01:07.683
this is an example where microbenchmarks
will tend to lie to you because you will

20
00:01:07.683 --> 00:01:13.060
not exercise the secondary cache as hard
as you would do it in a real application.

21
00:01:14.160 --> 00:01:19.314
And what the alternative would be is to
instead of putting in an object literals

22
00:01:19.314 --> 00:01:24.389
to put it inside of regular arrays,
where the odd indices contain the keys and

23
00:01:24.389 --> 00:01:29.980
even indices contain the values or vice
versa which way that you want it, right?

24
00:01:29.980 --> 00:01:35.198
&gt;&gt; You said that JSX doesn't add
that much overhead over the DOM

25
00:01:35.198 --> 00:01:40.109
manipulation part but
it would add a lot of extra memory,

26
00:01:40.109 --> 00:01:44.020
right, so garbage collection and stuff.

27
00:01:44.020 --> 00:01:48.799
&gt;&gt; So, memory, yes, in terms of memory
objects and arrays are about the same.

28
00:01:48.799 --> 00:01:54.452
So I don't necessarily think one will
be better necessarily than the other.

29
00:01:54.452 --> 00:01:58.478
I think that the key thing is that
internally the framework has to iterate

30
00:01:58.478 --> 00:02:00.795
and read the values of these properties.

31
00:02:00.795 --> 00:02:04.310
And this iteration is what's expensive,
right?

32
00:02:07.390 --> 00:02:12.163
I don't know exact numbers, but I believe
last time I was chatting with folks from

33
00:02:12.163 --> 00:02:16.935
React, they said that the iteration of the
properties is somewhere in the order of

34
00:02:16.935 --> 00:02:19.490
5% of the application runtime.

35
00:02:19.490 --> 00:02:24.300
So it's not terribly significant, and
maybe from 5% we could make it in 2.5%, so

36
00:02:24.300 --> 00:02:28.840
overall you would just have a couple
of percentage, points difference.

37
00:02:28.840 --> 00:02:33.633
I mean on the real application so
it's not a huge difference but

38
00:02:33.633 --> 00:02:38.433
it is a difference, right,
in some cases and so it would help.

39
00:02:38.433 --> 00:02:40.785
Okay, so
let's look at this particular benchmark.

40
00:02:40.785 --> 00:02:41.593
So what are we doing here?

41
00:02:41.593 --> 00:02:47.631
So we're creating either
an object which has 100 keys or

42
00:02:47.631 --> 00:02:54.755
we creating an array where every other
the even numbers are the keys and

43
00:02:54.755 --> 00:02:58.877
the odd numbers are the values, right?

44
00:02:58.877 --> 00:03:02.630
So this array is actually gonna be twice
the size of the object because it has to

45
00:03:02.630 --> 00:03:04.920
store twice as much information.

46
00:03:04.920 --> 00:03:10.047
Objects essentially cheat because
the values are stored in one location and

47
00:03:10.047 --> 00:03:13.700
the keys are stored in the hidden class,
right?

48
00:03:13.700 --> 00:03:15.860
If sometimes the hidden
classes can be shared,

49
00:03:15.860 --> 00:03:17.810
sometimes they cannot be shared, right?

50
00:03:17.810 --> 00:03:20.574
But if they can be shared,
then you definitely get savings and

51
00:03:20.574 --> 00:03:23.730
you wouldn't get savings in this
particular case with arrays.

52
00:03:23.730 --> 00:03:25.950
So these are trade-offs
to kind of think about.

53
00:03:25.950 --> 00:03:30.190
But let's kind of do and kind of
look at these particular benchmarks.

54
00:03:30.190 --> 00:03:34.130
So the first benchmark is to
just read all the properties.

55
00:03:34.130 --> 00:03:38.168
In other words,
iterate over every single value and

56
00:03:38.168 --> 00:03:41.440
read it, right, and then sum it up.

57
00:03:41.440 --> 00:03:42.683
So let's go through the benchmark, right?

58
00:03:42.683 --> 00:03:47.790
So this benchmark, what it does is it
tries to simply read all the values and

59
00:03:47.790 --> 00:03:49.910
sum them up together, right?

60
00:03:49.910 --> 00:03:51.980
So it just does a sum of all the things.

61
00:03:51.980 --> 00:03:54.200
Now, in order to get this,
there's two things that have to happen.

62
00:03:54.200 --> 00:03:58.440
First, we have to enumerate
all of the properties.

63
00:03:58.440 --> 00:04:01.450
For the VM, that's relatively
straightforward because it can go and

64
00:04:01.450 --> 00:04:02.623
through the hidden class,

65
00:04:02.623 --> 00:04:05.539
it can get to an internal data
structure where it knows all the keys.

66
00:04:07.460 --> 00:04:09.810
So that in itself is not expensive.

67
00:04:09.810 --> 00:04:14.384
This bit right here though is, because
this actually causes a megamorphic read,

68
00:04:14.384 --> 00:04:18.260
because the key we're
reading is always different.

69
00:04:18.260 --> 00:04:23.027
I don't know if the VMs are smart enough
to recognize the fact that like, look,

70
00:04:23.027 --> 00:04:25.645
I am actually doing a key
off over the object and

71
00:04:25.645 --> 00:04:29.260
I'm reading this key in this exact place,
same place.

72
00:04:29.260 --> 00:04:32.852
And so in theory, the VM could be
intelligent enough to be like, actually,

73
00:04:32.852 --> 00:04:34.660
let me just read the values directly.

74
00:04:35.780 --> 00:04:39.611
I don't think it does that
because if I remember correctly,

75
00:04:39.611 --> 00:04:43.670
this will give you the keys on
the prototypes as well, right?

76
00:04:43.670 --> 00:04:44.609
Or am I wrong on this?

77
00:04:46.500 --> 00:04:47.840
I don't know.

78
00:04:47.840 --> 00:04:52.170
I know in some instances,
you might get the parent's ones as well.

79
00:04:52.170 --> 00:04:54.970
And so this is not as easy as
just like reading the values.

80
00:04:54.970 --> 00:04:59.570
You actually have to do some
logic in here as well, yes.

81
00:04:59.570 --> 00:05:04.126
&gt;&gt; What if we were to do like
object.keys or object.entries?

82
00:05:04.126 --> 00:05:06.843
And do the same process, do we know?

83
00:05:06.843 --> 00:05:11.740
&gt;&gt; So object.entries I think
you would have an ease.

84
00:05:11.740 --> 00:05:13.150
I don't know.

85
00:05:13.150 --> 00:05:17.110
I mean I can see how the VM
could optimize different things.

86
00:05:17.110 --> 00:05:20.726
Again, like once you understand how
the thing works on a silicon and

87
00:05:20.726 --> 00:05:23.723
hopefully Apple is able to
kind of convey that, right?

88
00:05:23.723 --> 00:05:26.510
How you can see that you just have
these objects which are arrays, etc.

89
00:05:26.510 --> 00:05:31.640
You can kind of derive in terms of
like what is and isn't possible.

90
00:05:31.640 --> 00:05:36.725
It may be that the VM hasn't chosen to
implement this particular optimization,

91
00:05:36.725 --> 00:05:41.135
but you should have a pretty good
idea of what is and isn't realistic,

92
00:05:41.135 --> 00:05:43.251
right, through all this stuff.

93
00:05:43.251 --> 00:05:46.201
So something like object.keys
collecting the keys should

94
00:05:46.201 --> 00:05:49.977
be relatively straightforward process
because that information is stored

95
00:05:49.977 --> 00:05:52.752
inside of the hidden class for
that particular object.

96
00:05:52.752 --> 00:05:56.698
And so it should be a pretty easy way
to do that for the VM, should be pretty

97
00:05:56.698 --> 00:06:02.961
efficient The reading though would
might be more complicated, right?

98
00:06:02.961 --> 00:06:07.650
Because then you would have to read
the keys, and that's kind of trickier.

99
00:06:07.650 --> 00:06:11.190
Okay, so in here,
we just simply do the copying.

100
00:06:11.190 --> 00:06:13.620
Here, what we're doing is we're
doing the same exact operation.

101
00:06:14.970 --> 00:06:18.770
The only thing we're doing different
is we're copying the object over first.

102
00:06:18.770 --> 00:06:20.733
So obviously, this is gonna be slower,

103
00:06:20.733 --> 00:06:24.520
because the cost of copying that
object is gonna cost us something.

104
00:06:24.520 --> 00:06:28.287
But now we have the problem that we
are essentially enumerating over

105
00:06:28.287 --> 00:06:29.650
the properties twice.

106
00:06:29.650 --> 00:06:32.482
Once we have to enumerate here, and
then we again enumerate here, and

107
00:06:32.482 --> 00:06:33.750
both are megamorphic.

108
00:06:33.750 --> 00:06:35.520
So now we do the same
exact thing with arrays.

109
00:06:35.520 --> 00:06:36.880
So in an array,
we do the same exact thing.

110
00:06:36.880 --> 00:06:40.540
We just say, hey, can I just go and
add up all the numbers?

111
00:06:40.540 --> 00:06:43.450
Obviously, I have to skip, so
I do every other one, right?

112
00:06:44.610 --> 00:06:48.299
And I'm gonna do the same exact thing
where I do a copy first, and then I'm

113
00:06:48.299 --> 00:06:52.830
gonna iterate over the copy of the array,
again, skipping every other thing, right?

114
00:06:52.830 --> 00:06:56.580
So the question is, how much of
a performant difference is this?

115
00:06:56.580 --> 00:06:57.714
How much would you expect?

116
00:06:59.615 --> 00:07:04.586
You would expect about the same,
or way faster?

117
00:07:04.586 --> 00:07:05.940
All right, well, let's try it.

118
00:07:08.474 --> 00:07:10.080
So here we go.

119
00:07:10.080 --> 00:07:14.290
So the fastest one, it was obviously
just iterating over the array.

120
00:07:14.290 --> 00:07:17.720
And the difference is huge,
right, between those two.

121
00:07:17.720 --> 00:07:19.430
That's a pretty significant difference.

122
00:07:19.430 --> 00:07:24.240
And again let's see, how many,
what do we have here?

123
00:07:24.240 --> 00:07:27.746
So in this case, we have a size of 100,

124
00:07:27.746 --> 00:07:33.070
which means that we didn't
overwhelm the second level cache.

125
00:07:34.300 --> 00:07:37.132
So if I go and make this into 10,000,

126
00:07:37.132 --> 00:07:40.995
this should be a significant
difference in performance.

127
00:07:40.995 --> 00:07:45.170
Penalty because we would like
bust the second level cache.

128
00:07:45.170 --> 00:07:46.150
Let's see how we do.

129
00:07:47.420 --> 00:07:50.310
Hopefully it will run, while it's running.

130
00:07:51.760 --> 00:07:53.477
So the array is the fastest.

131
00:07:53.477 --> 00:07:56.190
Notice, making a copy is relatively cheap.

132
00:07:56.190 --> 00:08:02.000
The cost of a copy is next to nothing
which is kind of surprising, right?

133
00:08:03.520 --> 00:08:08.885
But to make a copy of an object,
that's a little more expensive.

134
00:08:11.081 --> 00:08:18.467
So here is 680% versus 6,000%.

135
00:08:18.467 --> 00:08:23.115
It's a huge difference in
performance that you can have.

136
00:08:23.115 --> 00:08:26.810
And so let's see here,
we did with more objects, right?

137
00:08:26.810 --> 00:08:30.620
You see, originally,
the difference in performance was 680%.

138
00:08:30.620 --> 00:08:34.320
Now the difference is 3,000%.

139
00:08:34.320 --> 00:08:37.640
So when we made the number
of shapes bigger,

140
00:08:37.640 --> 00:08:41.550
we really messed with the VM even more.

141
00:08:41.550 --> 00:08:45.920
And again, I'm going to come back because
this is why microbenchmarks lie to you.

142
00:08:45.920 --> 00:08:49.610
Because it's not a real program,
it's a small subset.

143
00:08:49.610 --> 00:08:53.092
And all these secondary caches
can play tricks on you and

144
00:08:53.092 --> 00:08:56.660
make it look like the stuff is
faster than it actually is.

145
00:08:58.740 --> 00:09:00.720
But I find it really kind of fascinating.

146
00:09:00.720 --> 00:09:05.530
I think it's really surprising to
people that a copy of an array,

147
00:09:05.530 --> 00:09:09.120
which is what this is,
is really cheap, right?

148
00:09:10.650 --> 00:09:14.880
Because internally, all you have to do
is copy memory and CPUs usually have

149
00:09:14.880 --> 00:09:19.059
dedicated instructions to be like,
copy this memory over here, right?

150
00:09:19.059 --> 00:09:23.764
And so it's super cheap whereas
copying an object you can't just copy

151
00:09:23.764 --> 00:09:28.707
the values is more like you have to make
sure that in this particular case we

152
00:09:28.707 --> 00:09:32.232
might end up with a different
hidden class, right?

153
00:09:32.232 --> 00:09:36.977
Because it could be additional properties
added to this particular list, right?

154
00:09:36.977 --> 00:09:41.450
And so the copying there, also I believe
the prototypes chains will flatten, right?

155
00:09:41.450 --> 00:09:44.744
So if this object has properties
inherited from a prototype chain,

156
00:09:44.744 --> 00:09:46.890
the child will have it flattened.

157
00:09:46.890 --> 00:09:49.170
So you can't just copy things over.

158
00:09:49.170 --> 00:09:52.960
Things become a lot more
complicated than that.

159
00:09:52.960 --> 00:09:57.944
And so whenever you deal with objects,
you should be really thinking in your head

160
00:09:57.944 --> 00:10:01.260
like, can the VM take
advantage of inline caching?

161
00:10:01.260 --> 00:10:04.820
If the answer is yes, then your code
is good in terms of performance.

162
00:10:04.820 --> 00:10:08.210
If the answer is no, then you might
consider writing it differently.

163
00:10:08.210 --> 00:10:10.480
Now, should you do it all the time, no.

164
00:10:10.480 --> 00:10:13.630
Sometimes it just doesn't matter, right?

165
00:10:13.630 --> 00:10:16.569
But if it's hot code,
then you probably should consider it.

166
00:10:19.020 --> 00:10:20.530
Questions here.

167
00:10:20.530 --> 00:10:25.590
&gt;&gt; Micro benchmarks are somewhat more
reliable on fully compiled languages.

168
00:10:26.890 --> 00:10:28.352
&gt;&gt; I would say on fully
compiled languages,

169
00:10:28.352 --> 00:10:29.870
they'll be a lot more predictable, yes.

170
00:10:31.700 --> 00:10:36.561
I think the place where people will shoot
themselves in the foot with dynamic

171
00:10:36.561 --> 00:10:41.353
languages like JavaScript is that
because they don't present sufficiently

172
00:10:41.353 --> 00:10:45.795
complex problem to the micro benchmark
because it's a micro, right?

173
00:10:45.795 --> 00:10:50.638
The micro benchmarks will tend to
over optimize than what they will

174
00:10:50.638 --> 00:10:52.376
do in reality, right?

175
00:10:52.376 --> 00:10:56.263
They will say you always call me
with the same function, in line.

176
00:10:56.263 --> 00:10:58.043
In reality, that might not happen, right?

177
00:10:58.043 --> 00:10:59.910
Or you might have a deopt.

178
00:10:59.910 --> 00:11:02.520
You always come with the same shape,
inline caches everywhere.

179
00:11:02.520 --> 00:11:05.090
Again, in the reality
that might not happen.

180
00:11:05.090 --> 00:11:09.194
Or even if you don't have the same shape,
I showed you how the second layer cache

181
00:11:09.194 --> 00:11:13.296
might be lying to you, because as long as
the number of objects that you access is

182
00:11:13.296 --> 00:11:17.580
small enough, it's not a big deal,
the second level cache will save you.

183
00:11:17.580 --> 00:11:22.359
And so all of those things, if you
add them up essentially mean that you

184
00:11:22.359 --> 00:11:25.970
have to be careful with micro benchmarks.

185
00:11:25.970 --> 00:11:28.540
They're useful, they show useful stuff.

186
00:11:28.540 --> 00:11:31.496
But a lot of times,
they're kind of tricky.

187
00:11:31.496 --> 00:11:36.279
Actually, one of the things I want
to show you is that oftentimes when

188
00:11:36.279 --> 00:11:40.073
I write my benchmark,
I am always sure to do an effect on

189
00:11:40.073 --> 00:11:43.389
a variable that's outside
of a loop like this.

190
00:11:44.670 --> 00:11:48.901
Because sometimes, under certain
conditions, the VM can look at this code

191
00:11:48.901 --> 00:11:52.420
and be like, that has no side effect,
I'm just gonna skip it.

192
00:11:54.510 --> 00:11:57.785
And by doing something like this,
the VM can't skip it,

193
00:11:57.785 --> 00:12:02.360
because there's a code path by which
information is leaking out, right?

194
00:12:02.360 --> 00:12:06.410
So it cannot be smart
enough to do these tricks.

195
00:12:06.410 --> 00:12:10.732
But in theory, right, in theory,
if the compiler was intelligent enough,

196
00:12:10.732 --> 00:12:15.270
it could look at all this thing and say,
none of this stuff is necessary, right?

197
00:12:15.270 --> 00:12:16.390
I could just skip it.

198
00:12:16.390 --> 00:12:19.799
Cuz it says, you're reading sum,
you're computing the sum value, but

199
00:12:19.799 --> 00:12:22.550
then on the end of the day,
you're not doing anything with it.

200
00:12:22.550 --> 00:12:25.820
So it's unnecessary,
delete, delete, delete.

201
00:12:25.820 --> 00:12:28.780
All right, and prune the code.

202
00:12:28.780 --> 00:12:33.712
Luckily we're not that far yet but I mean,
if we got that far, you would have really

203
00:12:33.712 --> 00:12:38.157
hard time writing micro benchmarks
because it would be relatively easy for

204
00:12:38.157 --> 00:12:42.410
the compiler to prove that something
is uneeded and just throw it away.

205
00:12:42.410 --> 00:12:47.086
And there have been incidences of this is
all this in the past where the benchmarks

206
00:12:47.086 --> 00:12:51.835
just look ridiculously better because
it's just omitting something that cannot

207
00:12:51.835 --> 00:12:54.401
be eliminated in the real case, yeah Mark.

208
00:12:54.401 --> 00:12:57.837
&gt;&gt; Are there any options to turn off
some of these optimizations that you'd

209
00:12:57.837 --> 00:13:00.340
be confident about your benchmarks?

210
00:13:00.340 --> 00:13:02.420
&gt;&gt; Yes, there are actually.

211
00:13:02.420 --> 00:13:03.982
So when you execute node,

212
00:13:03.982 --> 00:13:08.460
you can pass in a whole bunch of options
to tell it don't do certain things.

213
00:13:08.460 --> 00:13:12.605
So for example, you can tell
the system do not do inlining.

214
00:13:12.605 --> 00:13:14.387
And it will not inline any functions.

215
00:13:14.387 --> 00:13:16.620
Of course,
your code is gonna run slower, but

216
00:13:16.620 --> 00:13:19.155
it might be easier to
collect certain information.

217
00:13:19.155 --> 00:13:22.480
So if you really wanna know what
the cost of certain things are,

218
00:13:22.480 --> 00:13:24.375
turning off inliner is important.

219
00:13:25.685 --> 00:13:30.515
The way this manifests itself, and again,
the tools are changing all the time.

220
00:13:30.515 --> 00:13:33.699
So I might be telling you information
that might have been true a while ago and

221
00:13:33.699 --> 00:13:34.560
no longer the case.

222
00:13:34.560 --> 00:13:37.773
But I have noticed that when I
try to benchmark things, I will

223
00:13:37.773 --> 00:13:42.257
see that a particular function is gonna be
expensive, and then it just disappears.

224
00:13:42.257 --> 00:13:44.360
It's no longer there.

225
00:13:44.360 --> 00:13:48.607
And what's happening is it's showing like,
look, it's expensive before I inlined it,

226
00:13:48.607 --> 00:13:50.440
and then I inlined it, and it's gone.

227
00:13:50.440 --> 00:13:51.500
It no longer exists.

228
00:13:51.500 --> 00:13:55.840
So as far as the benchmark is concerned,
that function disappeared, and

229
00:13:55.840 --> 00:13:59.650
now this other function
just became more expensive.

230
00:13:59.650 --> 00:14:01.170
But it's not obvious what's happening.

231
00:14:01.170 --> 00:14:06.800
And so sometimes it's difficult to unwind
what's actually happening in your system.

232
00:14:08.190 --> 00:14:12.449
And so it's nice to run your code
when you disable inlining because you

233
00:14:12.449 --> 00:14:17.570
have a better view of the information
that you get out of the profiler, right?

234
00:14:17.570 --> 00:14:22.108
But of course, your code then runs slower
because inlining is a useful technique

235
00:14:22.108 --> 00:14:25.910
which gives you benefits,
especially for small functions.

236
00:14:25.910 --> 00:14:29.705
And we tend to write like tiny little
getters that return the value or

237
00:14:29.705 --> 00:14:31.640
add things together, etc.

238
00:14:31.640 --> 00:14:33.259
And in situations like that,

239
00:14:33.259 --> 00:14:37.407
inlining actually makes a huge difference
because the cost of the operation

240
00:14:37.407 --> 00:14:42.860
that you're doing typically is less than
the actual overhead of making that call.

241
00:14:42.860 --> 00:14:46.260
And so in cases like that inlining
is a huge benefit, right?

242
00:14:47.730 --> 00:14:50.180
&gt;&gt; There's someone asking for
a clarification.

243
00:14:51.440 --> 00:14:53.413
A deep copy copies all fields and

244
00:14:53.413 --> 00:14:58.630
makes copies of dynamically allocated
memory pointed to by the fields.

245
00:14:58.630 --> 00:15:05.305
Deep copy occurs when an object is copied
along with the objects to which it refers.

246
00:15:05.305 --> 00:15:09.084
&gt;&gt; Yeah, but
JavaScript does not have a deep copy.

247
00:15:09.084 --> 00:15:11.684
Well there's, I think there's
a helper function or something, but

248
00:15:11.684 --> 00:15:13.676
when you do dot, dot, dot,
that's not a deep copy.

249
00:15:13.676 --> 00:15:15.332
That's a shallow copy, right?

250
00:15:15.332 --> 00:15:17.232
&gt;&gt; Yeah, there's structured clone.

251
00:15:17.232 --> 00:15:19.152
&gt;&gt; There's a structured clone,
yes for deep copy, but

252
00:15:19.152 --> 00:15:20.310
we're not talking about that.

