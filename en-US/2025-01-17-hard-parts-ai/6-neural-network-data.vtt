WEBVTT

1
00:00:00.000 --> 00:00:04.478
&gt;&gt; Will Sentance: All right, so,
if we were able to predict fraud or

2
00:00:04.478 --> 00:00:10.452
not because we had background
information on the refund request,

3
00:00:10.452 --> 00:00:13.455
I want my money back on DoorDash.

4
00:00:13.455 --> 00:00:17.225
Then we used it on
the population as a whole.

5
00:00:17.225 --> 00:00:20.523
That means all refunds, and
we were able to say, hey,

6
00:00:20.523 --> 00:00:24.397
this new refund request,
looking at the background data on it,

7
00:00:24.397 --> 00:00:29.012
because I learned the pattern of
the background data of the refund request.

8
00:00:29.012 --> 00:00:33.071
And whether or
not to give a refund for the sample,

9
00:00:33.071 --> 00:00:37.049
I can apply that same
pattern to the population.

10
00:00:37.049 --> 00:00:44.575
If we have labeled data, some information
in that case it was details of the refund,

11
00:00:44.575 --> 00:00:49.669
represented as numbers plus
labels of what that info is.

12
00:00:49.669 --> 00:00:54.216
And remember, whether or not a refund was
given was essentially a label, Refund 1,

13
00:00:54.216 --> 00:00:54.987
Refund -1.

14
00:00:54.987 --> 00:00:57.948
This data associated with Refund 1.

15
00:00:57.948 --> 00:01:02.830
This data associated with Refund -1,
for the sample.

16
00:01:02.830 --> 00:01:07.839
If we have labeled data, some numbers
plus a label for it, and we then created

17
00:01:07.839 --> 00:01:12.697
a converter for that sample, converting
the input values into the label,

18
00:01:12.697 --> 00:01:17.340
so the input numbers into the label, 1 or
&gt;&gt; Will Sentance: -1,

19
00:01:17.340 --> 00:01:22.527
then assuming the sample will effect
the population, that is to say what there

20
00:01:22.527 --> 00:01:27.491
was like in the sample was reflective
of the patterns in the distribution.

21
00:01:27.491 --> 00:01:32.325
Which is a fancy way of saying the
patterns of the data in the broader set of

22
00:01:32.325 --> 00:01:35.668
refund requests,
it's completely different.

23
00:01:35.668 --> 00:01:40.533
Then we could use that convert on the rest
of the population that isn't labeled

24
00:01:40.533 --> 00:01:43.788
to predict those labels,
refund 1 or refund -1.

25
00:01:43.788 --> 00:01:47.799
What other info represents numbers and
labels of what that info is?

26
00:01:47.799 --> 00:01:52.113
Can you think of, people, what other
stuff that you might not think of as

27
00:01:52.113 --> 00:01:57.098
actually being a collection of numbers
might actually be a collection of numbers?

28
00:01:57.098 --> 00:02:01.056
Maria, what other things
can we capture as data.

29
00:02:01.056 --> 00:02:01.584
&gt;&gt; Maria: Pixels.

30
00:02:01.584 --> 00:02:06.110
&gt;&gt; Will Sentance: Pixels, images,
images can be collections of numbers.

31
00:02:06.110 --> 00:02:11.667
Let's say, I don't know, higher numbers
for brighter, or higher numbers for

32
00:02:11.667 --> 00:02:16.504
darker, or lower numbers for dimmer,
or lower numbers for lighter.

33
00:02:16.504 --> 00:02:17.910
But we can also do it with words.

34
00:02:17.910 --> 00:02:19.204
Let's just take a quick look at words.

35
00:02:19.204 --> 00:02:24.373
Just to get a rough sense of it,
I am gonna show this for a second here.

36
00:02:24.373 --> 00:02:30.906
So, if we could build
a sample of human language,

37
00:02:30.906 --> 00:02:37.291
let's say, I don't know,
some text strings.

38
00:02:37.291 --> 00:02:41.557
Let's say, I don't know,
I [LAUGH] always throw down these.

39
00:02:41.557 --> 00:02:46.391
I love my, and then our label could

40
00:02:46.391 --> 00:02:51.239
be how likely the next word is dog.

41
00:02:51.239 --> 00:02:56.964
Let's say very often in, I don't know,
text off the Internet, it's dog.

42
00:02:56.964 --> 00:03:02.213
Let's say it's cat is,
I don't know, I walk my,

43
00:03:02.213 --> 00:03:06.374
there we go, that's a bit more likely.

44
00:03:06.374 --> 00:03:10.687
Walk my dog, very likely in
the history of text on the Internet.

45
00:03:10.687 --> 00:03:12.624
Cat, very likely?

46
00:03:12.624 --> 00:03:16.289
Not impossible, but pretty unlikely.

47
00:03:16.289 --> 00:03:20.804
Refrigerator, [LAUGH] Never been used.

48
00:03:20.804 --> 00:03:25.157
Completely unused in
the history of the Internet.

49
00:03:25.157 --> 00:03:31.308
And then we can have, I love,
now this works better, doesn't it?

50
00:03:31.308 --> 00:03:35.963
Dogs, maybe that's being
used like quite often.

51
00:03:35.963 --> 00:03:42.683
Cats also, I don't know, pretty often but
less than right because, who likes cats?

52
00:03:42.683 --> 00:03:43.644
No, sorry.

53
00:03:43.644 --> 00:03:45.260
[LAUGH] Why pick a fight?

54
00:03:45.260 --> 00:03:50.845
If we can, though, work out the pattern to
convert I walk my into the distribution,

55
00:03:50.845 --> 00:03:56.037
the number of dog, cat, refrigerator
in the history of text on the internet,

56
00:03:56.037 --> 00:03:57.941
and if I love into dogs cats.

57
00:03:57.941 --> 00:04:02.386
Then we could create a a converter for,

58
00:04:02.386 --> 00:04:07.908
I walk my into these strings,
I love into these,

59
00:04:07.908 --> 00:04:12.892
and then use it on
the larger population of,

60
00:04:12.892 --> 00:04:17.897
and this is wow,
where it gets really wild.

61
00:04:17.897 --> 00:04:21.212
Of all human language, or

62
00:04:21.212 --> 00:04:25.588
let's think, all text strings.

63
00:04:25.588 --> 00:04:30.405
And perhaps we could
make predictions on how

64
00:04:30.405 --> 00:04:35.089
likely I love my, so
not I walk my or I love,

65
00:04:35.089 --> 00:04:40.554
which have these, but
I love my is going to be dog,

66
00:04:40.554 --> 00:04:45.393
we don't know, or cat, or refrigerator.

67
00:04:45.393 --> 00:04:49.221
We can apply the pattern we found for
converting I walk my into the next

68
00:04:49.221 --> 00:04:52.595
word in the string,
I love into the next word in that string.

69
00:04:52.595 --> 00:04:58.041
We can apply that same conversion or
set of rules for

70
00:04:58.041 --> 00:05:04.120
I love my, because we can give
each word a numeric value,

71
00:05:04.120 --> 00:05:07.046
or a set of numeric values.

72
00:05:07.046 --> 00:05:11.456
And match up those numeric values
with the numeric values of dog, cat,

73
00:05:11.456 --> 00:05:16.473
refrigerator, and their associated
frequency in the history of language use.

74
00:05:16.473 --> 00:05:20.985
And from that generalize to
the underlying population of all

75
00:05:20.985 --> 00:05:23.290
possible human language use.

76
00:05:23.290 --> 00:05:27.059
The term,
the equivalent of pixels in language,

77
00:05:27.059 --> 00:05:31.727
anyone know the equivalent of
pixels in numeric values that we

78
00:05:31.727 --> 00:05:35.786
give to bits of words or
sentences in human language.

79
00:05:35.786 --> 00:05:37.616
The term is embeddings.

80
00:05:37.616 --> 00:05:41.620
These are the text,
sort of numbers that we give.

81
00:05:41.620 --> 00:05:47.160
But we are, though,
going to focus on pixels,

82
00:05:47.160 --> 00:05:52.434
because we want to build
a smile identifier.

83
00:05:52.434 --> 00:05:55.906
We want to be able to identify,

84
00:05:55.906 --> 00:06:01.116
not patterns of numbers
that represent text,

85
00:06:01.116 --> 00:06:06.604
but patterns of numbers
that represent images.

86
00:06:06.604 --> 00:06:11.592
And again, remind us Maria,
what numeric values, or what's the,

87
00:06:11.592 --> 00:06:16.838
I don't know, what the images comprise
that we can then help comprise,

88
00:06:16.838 --> 00:06:19.251
that we can then give numbers to?

89
00:06:19.251 --> 00:06:20.607
What's the word you said earlier?

90
00:06:20.607 --> 00:06:21.107
Just say it again.
&gt;&gt; Maria: Pixels.

91
00:06:21.107 --> 00:06:23.414
&gt;&gt; Will Sentance: Pixels, well done,

92
00:06:23.414 --> 00:06:25.647
Maria, thank you.
Exactly, pixels.

93
00:06:25.647 --> 00:06:28.997
So we're gonna do this not for

94
00:06:28.997 --> 00:06:33.654
words, but for images.
Not with embeddings,

95
00:06:33.654 --> 00:06:34.821
but with?
&gt;&gt; Maria: Pixels.

96
00:06:34.821 --> 00:06:35.924
&gt;&gt; Will Sentance: Pixels, it's gonna be

97
00:06:35.924 --> 00:06:38.328
the right answer forever.
Excellent, thank you.

98
00:06:38.328 --> 00:06:41.676
So, we can see an image here of a happy

99
00:06:41.676 --> 00:06:46.782
face being reduced in its dimensionality.
That's another

100
00:06:46.782 --> 00:06:51.901
term we'll see again and again,
gone from 1,000 by 1,000 maybe,

101
00:06:51.901 --> 00:06:55.899
something like that, down to 3 by 4.
All right,

102
00:06:55.899 --> 00:07:01.166
we're gonna have a sample
of 3 by 4 images,

103
00:07:01.166 --> 00:07:05.186
3 by 4 images of 0s and1s only,

104
00:07:05.186 --> 00:07:10.548
keeping it super, super simple.
And we're going to,

105
00:07:10.548 --> 00:07:15.191
so 1s being the dark, raise your hand if
you buy that that is a smiley face in

106
00:07:15.191 --> 00:07:18.971
the bottom right hand corner there.
Raise your hand high and

107
00:07:18.971 --> 00:07:23.349
proud if you buy it.
That's three people.

108
00:07:23.349 --> 00:07:25.819
Maria, do you buy that's a smiley face?

109
00:07:25.819 --> 00:07:27.106
No, okay, good.

110
00:07:27.106 --> 00:07:29.208
No, that's a very obvious,

111
00:07:29.208 --> 00:07:32.794
that's a smiley face.
That's an obvious smiley face,

112
00:07:32.794 --> 00:07:38.357
reduced to 3 by 4 dimension.
We are going to build a sample of faces,

113
00:07:38.357 --> 00:07:43.992
3 by 4 images, in order to hopefully,
if we label them,

114
00:07:43.992 --> 00:07:48.822
have associated label of smile,
1 for a smile and

115
00:07:48.822 --> 00:07:53.660
-1, I guess, like refund for not.
That we might be able

116
00:07:53.660 --> 00:07:58.205
to look at new 3 by 4 images, and if we
can identify how to, well, let's see.

117
00:07:58.205 --> 00:08:01.655
Let's get our little sample of, and

118
00:08:01.655 --> 00:08:06.887
you're gonna all be very
convinced in a moment that this

119
00:08:06.887 --> 00:08:13.552
looks exactly like a smile.
Here it is.

120
00:08:13.552 --> 00:08:20.361
&gt;&gt; Will Sentance: Indisputably,
and that is labelled Smile 1.

121
00:08:20.361 --> 00:08:23.619
Just like our refund,
someone has come and looked at this,

122
00:08:23.619 --> 00:08:28.200
and by the way, there's a lot of business
opportunity and a lot of people doing it.

123
00:08:28.200 --> 00:08:32.199
I think it's his name, Alex Alexander
Wang, I think I'm getting it wrong,

124
00:08:32.199 --> 00:08:35.215
probably I'm getting it wrong,
the creator of Scale AI,

125
00:08:35.215 --> 00:08:39.233
you don't get this wrong in an actual
public talk, but Scale AI's founder.

126
00:08:39.233 --> 00:08:44.236
Their company's job is to
build out sample data sets,

127
00:08:44.236 --> 00:08:47.499
labeled images, for example, for

128
00:08:47.499 --> 00:08:51.978
example, of, I don't know, physical space.

129
00:08:51.978 --> 00:08:56.602
For autonomous vehicles that
need to be able to recognize,

130
00:08:56.602 --> 00:08:59.843
is that a person or
a shadow in that image.

131
00:08:59.843 --> 00:09:03.350
Those being seen in that rendered space
that's being seen by the autonomous

132
00:09:03.350 --> 00:09:03.852
vehicle.

133
00:09:03.852 --> 00:09:07.150
If you haven't been in a Waymo yet,
it's very fun.

134
00:09:07.150 --> 00:09:11.421
It's capturing and
identifying things all the time based,

135
00:09:11.421 --> 00:09:13.856
on we'll see, some sample data.

136
00:09:13.856 --> 00:09:15.812
Who does the labeling of that sample data?

137
00:09:15.812 --> 00:09:19.555
People, where do they work or
who do they work for?

138
00:09:19.555 --> 00:09:24.727
Companies like Scale AI, which is
worth $10 to $20 billion as a result.

139
00:09:24.727 --> 00:09:28.297
There's also public data sets available,
labeled images, like ImageNet,

140
00:09:28.297 --> 00:09:29.387
one of the most famous.

141
00:09:29.387 --> 00:09:32.335
And a historic one that was
super popular called MNIST,

142
00:09:32.335 --> 00:09:35.421
we'll talk about it in a second,
of hand-drawn digits.

143
00:09:35.421 --> 00:09:38.000
So there you go,
there's one of our images in our sample.

144
00:09:38.000 --> 00:09:41.426
Let's make another one.

145
00:09:41.426 --> 00:09:47.818
It's going to be a absolutely
obviously sad face.

146
00:09:47.818 --> 00:09:49.310
No one's gonna debate that.

147
00:09:49.310 --> 00:09:51.498
And that is Smile -1.

148
00:09:51.498 --> 00:09:53.413
No smile in this picture.

149
00:09:53.413 --> 00:09:56.524
If we can work out,
people, it is really cool.

150
00:09:56.524 --> 00:10:03.090
If we can work out how to
convert these 12 pixels,

151
00:10:03.090 --> 00:10:09.193
1 0 1 0 0 0 1 0 1 1 1 1 into output of 1,

152
00:10:09.193 --> 00:10:14.078
and these 12 pixels, 1 0 1 0 0 1

153
00:10:14.078 --> 00:10:19.116
1 1 1 0 1 into output of Smile -1,

154
00:10:19.116 --> 00:10:23.270
if we can work out how to do that.

155
00:10:23.270 --> 00:10:29.584
Then maybe,
assuming that our 3 by 4 images

156
00:10:29.584 --> 00:10:34.734
that we have in our sample are similar

157
00:10:34.734 --> 00:10:41.047
in their nature to
the population as a whole,

158
00:10:41.047 --> 00:10:45.699
then we can take, I don't know,

159
00:10:45.699 --> 00:10:51.865
let's say, 1 0 0 0 1 0 0 1 0 1 0 1.

160
00:10:51.865 --> 00:10:54.126
Look at that expression.

161
00:10:54.126 --> 00:10:58.489
That looks like the sorting art,
and nothing else.

162
00:10:58.489 --> 00:11:03.676
And that, we don't know,
is that a smile or not?

163
00:11:03.676 --> 00:11:05.552
We don't know.

164
00:11:05.552 --> 00:11:07.020
We don't know.

165
00:11:07.020 --> 00:11:12.837
But if we can apply whatever
rules to convert for

166
00:11:12.837 --> 00:11:18.801
our sample,
our broader population of images,

167
00:11:18.801 --> 00:11:26.226
if we can apply that conversion,
can we get a 1 or a -1 out?

168
00:11:26.226 --> 00:11:31.671
Based on our sample, based on
the fundamental notion that a smile is not

169
00:11:31.671 --> 00:11:37.869
something you can per se define, except
by, this is some smiles and this is not.

170
00:11:37.869 --> 00:11:41.785
Now, use that to tell me,
is this a smile or not?

171
00:11:41.785 --> 00:11:44.761
We're taking information that we have,

172
00:11:44.761 --> 00:11:50.459
where we know from a human it's labeled
small, but no different to information

173
00:11:50.459 --> 00:11:55.325
we had that we knew as a fraud, or
sorry, we knew got a refund or not.

174
00:11:55.325 --> 00:12:02.177
And generalizing that to the population

175
00:12:02.177 --> 00:12:07.626
of all 3 by 4, 0s 1s images.

176
00:12:07.626 --> 00:12:09.478
There it is.

177
00:12:09.478 --> 00:12:12.754
Okay, quick question.

178
00:12:12.754 --> 00:12:17.532
How many possible 3 by 4 images
are there with 12 pixels

179
00:12:17.532 --> 00:12:20.174
that can have either 0s or 1s?

180
00:12:22.581 --> 00:12:24.205
&gt;&gt; Will Sentance: [LAUGH] It's a lot.

181
00:12:24.205 --> 00:12:26.719
I tell you, people,
it's very quickly a lot.

182
00:12:26.719 --> 00:12:31.435
This one is two for each pixel,
there's either a value of 1 or

183
00:12:31.435 --> 00:12:34.260
0, and then there's 12 pixels.

184
00:12:34.260 --> 00:12:40.041
So it's 2 to the power of 12, what?

185
00:12:40.041 --> 00:12:45.215
So it's 4,096 possible 3 by 4 images.

186
00:12:45.215 --> 00:12:47.878
Don't underestimate how
many images there are.

187
00:12:47.878 --> 00:12:53.958
So this, by the way, can quickly tell
you how vast the possible data sets for

188
00:12:53.958 --> 00:12:57.188
any serious number of images, I know for

189
00:12:57.188 --> 00:13:01.096
a fact Ben is looking at this,
it can't be true.

