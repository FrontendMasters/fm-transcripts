WEBVTT

1
00:00:00.000 --> 00:00:03.045
&gt;&gt; Will Sentance: So, if you're
finding this all very trivial so far,

2
00:00:03.045 --> 00:00:07.651
we are building out here the foundations
that will allow us to implement gradient

3
00:00:07.651 --> 00:00:12.005
descent in a neural network to make
predictions about classifying images.

4
00:00:12.005 --> 00:00:17.314
We'll be able to, by the end of
the session today, be able to

5
00:00:17.314 --> 00:00:24.605
speak to pre-processing our data to ensure
it has appropriate dimensionality for

6
00:00:24.605 --> 00:00:29.312
our production model to
ensure accurate inference.

7
00:00:29.312 --> 00:00:31.051
Okay, that sounds pretty fancy.

8
00:00:31.051 --> 00:00:33.604
Thumbs if that sounds fancy.

9
00:00:33.604 --> 00:00:37.122
That's gonna be so, okay,
Joe doesn't think that sounds fancy.

10
00:00:37.122 --> 00:00:40.315
Maybe it doesn't, maybe you're all so
experienced with this.

11
00:00:40.315 --> 00:00:43.773
But that should hopefully be the kinda
language you're able to speak to by

12
00:00:43.773 --> 00:00:44.955
the end of this session.

13
00:00:44.955 --> 00:00:49.019
So if you're finding this rather trite,
or maybe you're not,

14
00:00:49.019 --> 00:00:51.504
certainly I could imagine either way,

15
00:00:51.504 --> 00:00:56.190
know that these are the foundations for
all of those fancy things to come.

16
00:00:56.190 --> 00:00:59.988
So let's get some names on
some of these things so

17
00:00:59.988 --> 00:01:05.826
that before we introduce how this links
to the domain of machine learning,

18
00:01:05.826 --> 00:01:08.899
let's get some names on these things.

19
00:01:08.899 --> 00:01:13.773
Maria, no, well, people,
raise your hand if you have

20
00:01:13.773 --> 00:01:19.188
an idea what this converter might
be called as a general term.

21
00:01:19.188 --> 00:01:23.716
Joe, Ben, raise your hand if you know
what a converter's fancy name is.

22
00:01:23.716 --> 00:01:24.241
&gt;&gt; Kayla: Model.

23
00:01:24.241 --> 00:01:25.477
&gt;&gt; Will Sentance: Model, who said that?

24
00:01:25.477 --> 00:01:26.382
Well done, Kayla.

25
00:01:26.382 --> 00:01:30.942
Exactly, model, that's it,
the set of rules to convert

26
00:01:30.942 --> 00:01:35.794
our data into our background
data into the associated label.

27
00:01:35.794 --> 00:01:38.271
This is a model, this is it.

28
00:01:38.271 --> 00:01:41.564
What type of model is this
with a bunch of boundaries,

29
00:01:41.564 --> 00:01:44.354
maximums.and minimums, above and below?

30
00:01:44.354 --> 00:01:45.925
Anyone know what type
of model that's called?

31
00:01:45.925 --> 00:01:48.355
It could be called a classifier,
a prediction.

32
00:01:48.355 --> 00:01:54.134
It's actually a particular type
of model called a decision tree,

33
00:01:54.134 --> 00:01:58.993
and you could say, wow,
it's a fairly trivial model.

34
00:01:58.993 --> 00:02:01.344
I say trivial in that it's quite like,
what is it?

35
00:02:01.344 --> 00:02:04.751
Bunch of if else statements, right?

36
00:02:04.751 --> 00:02:07.815
And essentially, that is what you
can think of a decision tree,

37
00:02:07.815 --> 00:02:10.481
except that by being so,
quote, trivial, and again,

38
00:02:10.481 --> 00:02:13.346
I'm not saying trivial in a bad way,
that's a good thing.

39
00:02:13.346 --> 00:02:16.736
It's incredibly good for
interpretation, and

40
00:02:16.736 --> 00:02:22.608
especially when you're deciding whether
to give somebody their door dash refund,

41
00:02:22.608 --> 00:02:25.680
or if their insurance claim is fraudulent.

42
00:02:25.680 --> 00:02:27.377
Even more high sensitive,

43
00:02:27.377 --> 00:02:32.856
especially when you're deciding whether
someone's insurance claim is fraudulent.

44
00:02:32.856 --> 00:02:37.322
You better have the ability
to explain to the compliance

45
00:02:37.322 --> 00:02:42.168
division in your company why
you gave one person the refund,

46
00:02:42.168 --> 00:02:45.699
gave one person the payout and
another not.

47
00:02:45.699 --> 00:02:51.556
And we're gonna see that many models would
dream of having the interpretability.

48
00:02:51.556 --> 00:02:55.516
That is to say we could look at each rule
and go, okay, so the reason this person

49
00:02:55.516 --> 00:02:58.816
did is they got years longer than that,
account spend less than or

50
00:02:58.816 --> 00:03:02.259
dollar spend less than that,
a number of accounts in the AP, that.

51
00:03:02.259 --> 00:03:04.137
So that when head of compliance goes,

52
00:03:04.137 --> 00:03:07.665
excuse me, why have we got this
pattern of people not getting refunds?

53
00:03:07.665 --> 00:03:12.986
It:s possible to say, well, let me just
show you, for each individual person, why.

54
00:03:12.986 --> 00:03:16.921
So for each model,
there are gonna be trade-offs,

55
00:03:16.921 --> 00:03:19.784
back to Ben's point, of benefits and

56
00:03:19.784 --> 00:03:25.337
costs associated with a given model,
but this is a model a decision tree.

57
00:03:25.337 --> 00:03:29.829
What are the names of the numbers
that are part that go into a model?

58
00:03:29.829 --> 00:03:34.039
The numbers or the rules that we may or
may not have different values for,

59
00:03:34.039 --> 00:03:37.915
which is not a million miles away
from the term we use in functions?

60
00:03:37.915 --> 00:03:42.965
What are the names of the values,
Kayla, that go into a models.

61
00:03:42.965 --> 00:03:44.329
&gt;&gt; Kayla: Arguments?

62
00:03:44.329 --> 00:03:46.477
&gt;&gt; Will Sentance: Or the one,
I guess, yeah, it's not.

63
00:03:46.477 --> 00:03:48.836
What's the other one we used
a bit before you put the data in?

64
00:03:48.836 --> 00:03:52.369
Parameters, exactly, so how many
parameters, Kayla, do we have here?

65
00:03:52.369 --> 00:03:54.174
Would you say?

66
00:03:54.174 --> 00:03:56.232
&gt;&gt; Kayla: Four.

67
00:03:56.232 --> 00:03:59.356
&gt;&gt; Will Sentance: Spot on,
got four parameters,

68
00:03:59.356 --> 00:04:04.332
four things that we are considering,
four numbers

69
00:04:04.332 --> 00:04:09.205
associated with our converter,
with our model.

70
00:04:09.205 --> 00:04:15.035
Okay, how many input data
values do we have, Paul?

71
00:04:19.230 --> 00:04:23.757
&gt;&gt; Speaker 3: Three or four.
&gt;&gt; Will Sentance: Three, I'd say three.

72
00:04:23.757 --> 00:04:28.665
I guess there's a meta one,
right, of how many we have,

73
00:04:28.665 --> 00:04:32.053
but that's interesting, actually.

74
00:04:32.053 --> 00:04:36.108
But we have here 3, and
what do we call these things, people?

75
00:04:36.108 --> 00:04:38.698
What do we call these input values?

76
00:04:38.698 --> 00:04:44.799
Features, soot on,
three features, spot on from Ben.

77
00:04:44.799 --> 00:04:45.912
Was it Ben or Joe?

78
00:04:45.912 --> 00:04:48.840
Yes, three features.

79
00:04:48.840 --> 00:04:52.970
Now, by the way, in practice,
in a decision tree, very rarely would we

80
00:04:52.970 --> 00:04:57.191
have a multi-input parameter,
one that's checking multiple things.

81
00:04:57.191 --> 00:05:01.534
Typically, a decision tree will go,
do this, left or right.

82
00:05:01.534 --> 00:05:06.598
And decision tree is if years
greater than 0.9 go towards refund,

83
00:05:06.598 --> 00:05:11.312
if dollars less than 200 go
towards refund, if accounts less

84
00:05:11.312 --> 00:05:16.312
than 3 go towards refund,
it really does split them out one by one.

85
00:05:16.312 --> 00:05:20.631
But how do we go about
identifying these boundaries?

86
00:05:20.631 --> 00:05:25.031
Well, in machine learning,

87
00:05:25.031 --> 00:05:29.431
the machine, the computer,

88
00:05:29.431 --> 00:05:34.015
tries different values here.

89
00:05:34.015 --> 00:05:38.136
Our boundaries or
combinations of our boundaries tries

90
00:05:38.136 --> 00:05:43.047
different combination of our
boundaries and assesses how many for

91
00:05:43.047 --> 00:05:48.133
different combinations of our sample
convert their background info

92
00:05:48.133 --> 00:05:53.500
to the label given by a person,
the customer service rep, as possible.

93
00:05:53.500 --> 00:05:58.347
There are so
many combinations of parameter values.

94
00:05:58.347 --> 00:06:00.880
Again, how many parameters we have here,
Kayla?

95
00:06:00.880 --> 00:06:01.978
&gt;&gt; Kayla: Four.
&gt;&gt; Will Sentance: Four, so

96
00:06:01.978 --> 00:06:07.469
many combination proud values to try
before the machine has learned the best

97
00:06:07.469 --> 00:06:13.239
set, the set that maximizes the number
of correct conversions for the sample.

98
00:06:13.239 --> 00:06:18.523
In a decision tree, the technique that's
used actually is check the first one,

99
00:06:18.523 --> 00:06:22.191
try different values for it,
and see how many convert.

100
00:06:22.191 --> 00:06:26.152
Check the next one, and
we'll see in neural networks,

101
00:06:26.152 --> 00:06:32.063
the technique is using some equal grading
percent, but we need back propagation.

102
00:06:32.063 --> 00:06:35.043
We need a bunch of techniques.

103
00:06:35.043 --> 00:06:41.972
Okay, here's the fun thing,
people, here's some math.

104
00:06:41.972 --> 00:06:46.204
Okay, say, we just take the first three
parameters, let's say we just have three

105
00:06:46.204 --> 00:06:50.201
parameters, okay, how many possible
values would we want to check for each?

106
00:06:50.201 --> 00:06:53.691
Give me, I don't know, we tried one.

107
00:06:53.691 --> 00:06:55.786
Let's say our sum was 100,000.

108
00:06:55.786 --> 00:06:57.567
Let's say we're not gonna
try every dollar value.

109
00:06:57.567 --> 00:07:02.087
We don't think it makes much
difference whether it's 199,198, 197.

110
00:07:02.087 --> 00:07:03.620
We're just gonna do tens maybe.

111
00:07:03.620 --> 00:07:06.722
And then for these, maybe do this in 0.1,
that's about a month, right?

112
00:07:06.722 --> 00:07:12.317
So in months, and then accounts on IP,
maybe we check 1 through 30,

113
00:07:12.317 --> 00:07:16.142
and we know that high ones
are really important,

114
00:07:16.142 --> 00:07:20.651
how many possible values might
we wanna check for years?

115
00:07:20.651 --> 00:07:22.573
Just give me a rough number,
how many possible values.

116
00:07:22.573 --> 00:07:26.757
If not number of years, but possible
numbers to check if we're gonna go 0.1,

117
00:07:26.757 --> 00:07:27.257
0.2.

118
00:07:27.257 --> 00:07:29.892
How many might wanna check, Maria?

119
00:07:29.892 --> 00:07:30.493
&gt;&gt; Maria: Ten.

120
00:07:30.493 --> 00:07:33.749
&gt;&gt; Will Sentance: Ten, we don't
have to go between 0 and 1, right?

121
00:07:33.749 --> 00:07:38.389
0.1, 0.2, 0.3, 0.4, and
suppose we do it in 0.1 increments,

122
00:07:38.389 --> 00:07:39.832
what do you think maybe?

123
00:07:39.832 --> 00:07:42.614
Let's say 50,
because that's the number I learned.

124
00:07:42.614 --> 00:07:45.787
50, and then for this,
let's say 50 as well.

125
00:07:45.787 --> 00:07:49.796
So we go in like, I don't know, 10, 20,
30, 40, up to 500 increments of 10.

126
00:07:49.796 --> 00:07:52.683
That'd be 50 values right to check,
so that'd be 50.

127
00:07:52.683 --> 00:07:55.349
And let's just say we go for 50 on this.

128
00:07:55.349 --> 00:07:58.985
Does anyone know the math we
do to calculate what the total

129
00:07:58.985 --> 00:08:03.685
number of different combinations of
parameter values we need to check for

130
00:08:03.685 --> 00:08:08.307
those three, where we have 50 possible
values would be knowing that we

131
00:08:08.307 --> 00:08:12.371
already tweaked one by this to
make it work for a sample of four?

132
00:08:12.371 --> 00:08:16.111
So we could have any possible combination
of those 50 values be the ones that

133
00:08:16.111 --> 00:08:18.193
convert the maximum number of our sample.

134
00:08:18.193 --> 00:08:23.375
There's 100,000 inputs to our sample,
it could be that it's, I don't know,

135
00:08:23.375 --> 00:08:27.858
1.4 years, $120, and accounts on
IP 17 that converts the maximum

136
00:08:27.858 --> 00:08:32.429
background info to the historic decision,
and that maximizes conversion.

137
00:08:32.429 --> 00:08:37.178
And assuming the sample represents
a population, there's your best converter.

138
00:08:37.178 --> 00:08:40.352
Okay, let's say we had 10
things you wanna check, and

139
00:08:40.352 --> 00:08:42.414
it would be 50 to the power of 10.

140
00:08:42.414 --> 00:08:45.529
That's big, how big is that?

141
00:08:45.529 --> 00:08:46.554
Very big, how big is that?

142
00:08:46.554 --> 00:08:50.803
[LAUGH] I don't know this now,
anyone know?

143
00:08:50.803 --> 00:08:54.719
So it's 9 with 16 0s, and

144
00:08:54.719 --> 00:09:00.367
that is 9 quadrillion, is that correct?

145
00:09:00.367 --> 00:09:04.815
I think that's 9 quadrillion maybe,
or is that 0 trillion?

146
00:09:04.815 --> 00:09:07.220
That's 9 quadrillion,
that's a lot to check.

147
00:09:07.220 --> 00:09:09.741
Okay, thank goodness, it's a lot to check.

148
00:09:09.741 --> 00:09:10.858
That's a lot to check.

149
00:09:10.858 --> 00:09:13.963
If we had 10 different parameters,
each we wanna check 50 and

150
00:09:13.963 --> 00:09:15.324
all the combinations of it.

151
00:09:15.324 --> 00:09:19.045
The number we'd have to check, and
then for every single one of them, run our

152
00:09:19.045 --> 00:09:22.724
entire sample through and see which
converts the most, how many converts.

153
00:09:22.724 --> 00:09:27.803
Then try slightly tweaking for the other.

154
00:09:27.803 --> 00:09:35.278
Okay, let's write it so everyone gets
to see, 8, no, should I write it?

155
00:09:35.278 --> 00:09:36.776
No, I'm not writing 9 quadrillion.

156
00:09:36.776 --> 00:09:38.396
9, here we go.

157
00:09:40.437 --> 00:09:45.939
That's six, is this good content,
people, everyone together?

158
00:09:45.939 --> 00:09:51.081
Yes, I mean, there's one more than that,
that's a lot.

159
00:09:51.081 --> 00:09:55.172
We don't have to check all of those,
right, people, not a chance.

160
00:09:55.172 --> 00:09:57.443
Well, so let's have a look,
there's a lot to check.

161
00:09:57.443 --> 00:10:02.685
The discipline of machine learning and
artificial intelligence has three major

162
00:10:02.685 --> 00:10:08.094
buckets, one, techniques and tools for
finding the best converter, the model.

163
00:10:08.094 --> 00:10:11.814
Here, it's a decision tree, but
there are so many to choose from,

164
00:10:11.814 --> 00:10:15.408
support vector machines,
different types of regressions, and

165
00:10:15.408 --> 00:10:18.763
the one that's really changed the game,
neural networks.

166
00:10:18.763 --> 00:10:21.922
There are so
many approaches to neural networks.

167
00:10:21.922 --> 00:10:28.240
This all comes down to finding the best
converter that converts our sample,

168
00:10:28.240 --> 00:10:33.581
captures the pattern in our sample,
converts our sample data.

169
00:10:33.581 --> 00:10:37.479
Ben's background info, Joe's, Kayla's,
Maria's, have I said that enough times,

170
00:10:37.479 --> 00:10:40.168
into the respective decision
whether to give a refund or not.

171
00:10:40.168 --> 00:10:45.142
And there might be a ton of data what
is gonna convert that technical tool we

172
00:10:45.142 --> 00:10:47.402
use to find that best converter.

173
00:10:47.402 --> 00:10:53.392
Two, ways, techniques and
tools, to minimize the time and

174
00:10:53.392 --> 00:10:58.090
effort it takes to get
the converters details,

175
00:10:58.090 --> 00:11:03.852
the model parameters,
the ones that work for our sample.

176
00:11:03.852 --> 00:11:09.925
It is computationally profoundly
demanding to check them all.

177
00:11:11.953 --> 00:11:15.587
&gt;&gt; Will Sentance: So
we have to find shortcuts, optimizations.

178
00:11:15.587 --> 00:11:18.686
In a decision tree, the optimization is,

179
00:11:18.686 --> 00:11:22.400
do not try them all in
combination with each other.

180
00:11:22.400 --> 00:11:26.425
Just try the first one and
see which boundary for

181
00:11:26.425 --> 00:11:29.853
the first one works best for the sample.

182
00:11:29.853 --> 00:11:32.742
Then if you're happy with it,
it's called a greedy algorithm,

183
00:11:32.742 --> 00:11:34.061
gets it right at every stage.

184
00:11:34.061 --> 00:11:37.177
If you're happy with it, move to the next
one, find the best boundary there for

185
00:11:37.177 --> 00:11:37.746
your sample.

186
00:11:37.746 --> 00:11:41.855
If you're happy, move it to the next one,
and you use what's called a purity metric,

187
00:11:41.855 --> 00:11:42.660
a genie purity.

188
00:11:42.660 --> 00:11:46.380
We're not here to learn
decision trees inside out, but

189
00:11:46.380 --> 00:11:50.654
rather that there are techniques for
every single model to get to

190
00:11:50.654 --> 00:11:54.633
the optimal conversion set of
parameters for your sample.

191
00:11:54.633 --> 00:11:59.735
And that's a huge part of machine
learning of artificial intelligence,

192
00:11:59.735 --> 00:12:05.581
because even with incredible optimization
techniques like gradient descendant,

193
00:12:05.581 --> 00:12:10.438
backpropagation, it is still
unprecedented amount of compute power

194
00:12:10.438 --> 00:12:15.151
that's needed to do so for, for
example, a large language model.

195
00:12:15.151 --> 00:12:20.603
Final bucket of work,
all of this conversion, magic or

196
00:12:20.603 --> 00:12:27.563
not magic, set of rules that converts
our sample is only interesting,

197
00:12:27.563 --> 00:12:32.435
cuz if we capture how to
convert our sample from its

198
00:12:32.435 --> 00:12:37.307
background info to its respective label,
well,

199
00:12:37.307 --> 00:12:43.242
we can use that on our population,
future refund requests.

200
00:12:43.242 --> 00:12:47.177
Where we do not have the decision,
we do not have the label,

201
00:12:47.177 --> 00:12:50.422
we have missing data, and
we can try to predict.

202
00:12:50.422 --> 00:12:54.595
We can only do that if the sample is

203
00:12:54.595 --> 00:12:59.227
representative of the population.

204
00:12:59.227 --> 00:13:05.880
Ensuring that is the case
is absolutely vital.

205
00:13:05.880 --> 00:13:09.638
And that ties to being aligned
with our goals and values,

206
00:13:09.638 --> 00:13:13.800
determining what our population even is,
what our sample is.

207
00:13:13.800 --> 00:13:19.331
Suppose that the customer service
reps were not very good at labeling,

208
00:13:19.331 --> 00:13:24.031
refund or not, they maybe even
had implicit or explicit, or

209
00:13:24.031 --> 00:13:29.667
no knowledge at all of it biases in
whether they decided to refund or not.

210
00:13:29.667 --> 00:13:34.089
They base it on things that maybe
the company doesn't wanna be incorporated,

211
00:13:34.089 --> 00:13:36.970
zip code,
some things that might even be illegal.

212
00:13:36.970 --> 00:13:42.640
You can look at that very much as, well,
wait, that means our sample is not

213
00:13:42.640 --> 00:13:48.855
actually truly representative of our true
patterns of the population as a whole.

214
00:13:48.855 --> 00:13:53.697
If we're getting people being
determined no refund for

215
00:13:53.697 --> 00:13:59.994
reasons that are specific to the person
who labeled the refund request.

216
00:13:59.994 --> 00:14:03.877
In other words, by labeled here,
I mean, said no refund,

217
00:14:03.877 --> 00:14:07.620
sorry, then that's not
reflective of the population.

218
00:14:07.620 --> 00:14:10.864
That pattern there is
associating the wrong background

219
00:14:10.864 --> 00:14:13.217
information with don't give a refund.

220
00:14:13.217 --> 00:14:18.023
So the absolutely crucial, as important
as finding the right converter,

221
00:14:18.023 --> 00:14:22.675
as important as working out how to
minimize the time it gets to takes to get

222
00:14:22.675 --> 00:14:25.060
the details of it or the right model.

223
00:14:25.060 --> 00:14:28.700
Working out the parameters is
ensuring that the sample and

224
00:14:28.700 --> 00:14:30.786
population reflect each other.

225
00:14:30.786 --> 00:14:33.608
That the sample data is accurate and

226
00:14:33.608 --> 00:14:38.228
is aligned with the values and
goals of the organization.

227
00:14:38.228 --> 00:14:42.329
And in practice, we'll see everything we
do in prediction is an extension of this.

228
00:14:42.329 --> 00:14:47.393
A sample of data with its labels or
patterns, and a population from which

229
00:14:47.393 --> 00:14:52.303
we wanna make a prediction based on
the insights we have on the sample.

230
00:14:52.303 --> 00:14:59.828
So when you have models that developed
their parameters to make predictions for,

231
00:14:59.828 --> 00:15:04.639
let's say, we'll see later,
image generation.

232
00:15:04.639 --> 00:15:09.097
Gemini from Google's image
generation tool made

233
00:15:09.097 --> 00:15:14.298
predictions about soldiers
from the 1940s in Germany

234
00:15:14.298 --> 00:15:19.195
that did not mirror historical
accuracy by any means.

235
00:15:19.195 --> 00:15:20.919
What is the failure there?

236
00:15:20.919 --> 00:15:25.400
Well, the failure is to
define population and sample.

237
00:15:25.400 --> 00:15:29.385
If you just want to be able to generate,
as we'll see later on,

238
00:15:29.385 --> 00:15:34.294
any possible set of pixels, maybe those
are pixels that could be generated.

239
00:15:34.294 --> 00:15:39.062
In this case, the goal was generate and
the population you

240
00:15:39.062 --> 00:15:43.646
wanna capture is all possible
historically accurate.

241
00:15:43.646 --> 00:15:49.836
Pixel combinations, images, then it's
a failure of that model development.

242
00:15:49.836 --> 00:15:54.851
And that final piece of alignment,
fighting bias or misalignment,

243
00:15:54.851 --> 00:16:00.555
and of course, overfitting, we saw here
we might have got a bit too perfect for

244
00:16:00.555 --> 00:16:03.677
our sample, we then tried to validate it.

245
00:16:03.677 --> 00:16:08.787
If our sample does not reflect our
population as a whole, we have a problem.

