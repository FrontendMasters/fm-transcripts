WEBVTT

1
00:00:00.051 --> 00:00:01.353
&gt;&gt; Will Sentance: Let's
get it to production.

2
00:00:01.353 --> 00:00:06.141
And actually, we don't need to dwell too
long on this cuz it's much like what we

3
00:00:06.141 --> 00:00:10.080
did with our, well, same extension
we did with our fraud model.

4
00:00:10.080 --> 00:00:13.706
So step one is what we spent
much of our time doing here.

5
00:00:13.706 --> 00:00:16.904
Data exploration and
developing a prototype model, there it is.

6
00:00:16.904 --> 00:00:18.616
We played with a small sample.

7
00:00:18.616 --> 00:00:26.705
In practice,
we might have 100,000 images labeled,

8
00:00:26.705 --> 00:00:32.770
and they might be actually
not 3 by 4 images,

9
00:00:32.770 --> 00:00:36.359
but 1000 by 1000.

10
00:00:36.359 --> 00:00:40.573
I'm gonna squeeze it in here,
1000 x 1000 pixel images.

11
00:00:40.573 --> 00:00:44.449
Ooh, that's a lot of data.

12
00:00:44.449 --> 00:00:49.388
They might be RGB with each red, green,

13
00:00:49.388 --> 00:00:54.197
blue having a value 1 through 256.

14
00:00:54.197 --> 00:00:56.318
That's a lot of data.

15
00:00:56.318 --> 00:01:02.963
They might, as we just heard,
be pictures with,

16
00:01:06.283 --> 00:01:09.658
&gt;&gt; Will Sentance: Funny orientations.

17
00:01:09.658 --> 00:01:14.138
Step one,
understand the nature of the data and

18
00:01:14.138 --> 00:01:17.389
get making it easier to work with.

19
00:01:17.389 --> 00:01:19.257
Convert our 1000 x 1000.

20
00:01:19.257 --> 00:01:20.885
Maybe we don't need all that data.

21
00:01:20.885 --> 00:01:25.379
We saw we got beautiful
smiles just from our 3 by 4.

22
00:01:25.379 --> 00:01:30.399
Convert this 1000 x 1000 into,
I don't know, 100 x 100.

23
00:01:30.399 --> 00:01:35.982
Maybe we don't need three colors, maybe
we get much of the shapes by grayscale.

24
00:01:35.982 --> 00:01:40.662
And maybe we only need a 100 different

25
00:01:40.662 --> 00:01:44.936
values from dark or shadow to light.

26
00:01:44.936 --> 00:01:50.021
And then certainly,
we could run an initial

27
00:01:50.021 --> 00:01:55.793
stage where we orient all
of these faces to all be,

28
00:01:55.793 --> 00:01:59.092
I didn't do that very well,

29
00:01:59.092 --> 00:02:03.780
[LAUGH] to all be,
that really bad [LAUGH].

30
00:02:03.780 --> 00:02:06.305
That's really bad,
neither of them went to their exact spot.

31
00:02:06.305 --> 00:02:10.240
Well, they got better, but
to get them all being in the same,

32
00:02:10.240 --> 00:02:14.250
that is horrible, sorry, no,
all being, it got worse,

33
00:02:14.250 --> 00:02:19.041
all being in their same orientation and
size, just as Maria highlighted.

34
00:02:19.041 --> 00:02:21.749
That's our pre-processing.

35
00:02:21.749 --> 00:02:23.704
We did dimensionality reduction.

36
00:02:23.704 --> 00:02:25.337
There are techniques to do that.

37
00:02:25.337 --> 00:02:29.682
That is reducing the dimensions of
the image to make it easier to work with.

38
00:02:29.682 --> 00:02:36.698
We did normalizing or augmenting
our data to align it each input.

39
00:02:36.698 --> 00:02:41.286
Now what that's gonna require is that
when we're using this converter,

40
00:02:41.286 --> 00:02:44.700
this model in inference for
our population as a whole.

41
00:02:44.700 --> 00:02:49.260
For when we want to recognize an image
in production from a new picture,

42
00:02:49.260 --> 00:02:52.003
it better come in at the same orientation.

43
00:02:52.003 --> 00:02:54.541
We better as engineers be saying,

44
00:02:54.541 --> 00:02:59.454
hey, excuse me, how have you
pre-processed the training data?

45
00:02:59.454 --> 00:03:04.432
I wanna make sure that the images
that I'm passing to the model

46
00:03:04.432 --> 00:03:09.517
in production are of the same
dimensions as the training data.

47
00:03:09.517 --> 00:03:14.087
Does that sound less intimidating
a phrase now than it did before?

48
00:03:14.087 --> 00:03:20.809
I think that's hopefully more
approachable at this point.

49
00:03:20.809 --> 00:03:25.618
There's a huge amount of creativity
that goes into developing

50
00:03:25.618 --> 00:03:30.528
what this neural network or
set of multipliers might look like.

51
00:03:30.528 --> 00:03:32.550
We can add layers.

52
00:03:32.550 --> 00:03:37.140
Adding layers would be the output
of these, which was 2,

53
00:03:37.140 --> 00:03:40.805
2.4, or -2.4, -2.3 and 5.2.

54
00:03:40.805 --> 00:03:46.237
Those outputs can actually be inserted
into a new set of multipliers.

55
00:03:46.237 --> 00:03:49.533
In fact,
we could have this set of pixels or

56
00:03:49.533 --> 00:03:55.314
12 go through 12 multipliers, and
then 12 different multipliers.

57
00:03:55.314 --> 00:03:58.484
And then we could take
the output of both of those and

58
00:03:58.484 --> 00:04:01.158
put them through two multiplying values.

59
00:04:01.158 --> 00:04:05.436
And then we can take the output of those
and feed them all the way back in again.

60
00:04:05.436 --> 00:04:09.644
That's called a recurrent
neural network where the output

61
00:04:09.644 --> 00:04:13.355
of one layer gets fed back
in as the input of another.

62
00:04:13.355 --> 00:04:18.123
That's a feedforward neural network
where the values only go forward.

63
00:04:18.123 --> 00:04:22.661
A recurrent one, particularly useful for
what's called sequential data,

64
00:04:22.661 --> 00:04:25.400
where the order of input matters,
like text.

65
00:04:25.400 --> 00:04:30.402
Very well may want to have the output
of multiplying all of these values,

66
00:04:30.402 --> 00:04:34.507
sorry, multiplying all our
pixels by these multipliers.

67
00:04:34.507 --> 00:04:37.957
The output,
we may wanna feed it back in in some way.

68
00:04:37.957 --> 00:04:42.179
Now obviously,
we couldn't feed it back indirectly here,

69
00:04:42.179 --> 00:04:46.667
because one number can't be
fed into a 12-multiplier grid.

70
00:04:46.667 --> 00:04:50.919
But suppose, as an example,

71
00:04:50.919 --> 00:04:56.540
we had 12 grids of 12 multipliers.

72
00:04:56.540 --> 00:05:00.147
So 12, one, another one, another one,
another one, another one.

73
00:05:00.147 --> 00:05:03.413
And had all 12 of these pixels
go through one, output number,

74
00:05:03.413 --> 00:05:07.183
go through the next one, different
set of multipliers, output number.

75
00:05:07.183 --> 00:05:10.083
Next one, different set of multipliers,
output number.

76
00:05:10.083 --> 00:05:12.595
We've now got 12 numbers as our output.

77
00:05:12.595 --> 00:05:15.286
We could have them all go back through or

78
00:05:15.286 --> 00:05:18.559
go through another layer
of 12 multipliers.

79
00:05:18.559 --> 00:05:23.843
This is adding layers and
layers of complexity.

80
00:05:23.843 --> 00:05:27.262
That's what's behind
convolutional neural networks,

81
00:05:27.262 --> 00:05:31.387
CNNs there, that are particularly
tailored for image recognition.

82
00:05:31.387 --> 00:05:37.493
And also transformers that we're gonna see
in a moment are profoundly important for

83
00:05:37.493 --> 00:05:39.304
large language models.

84
00:05:39.304 --> 00:05:43.886
We could also refine as we go through
this process our model design via

85
00:05:43.886 --> 00:05:47.848
metrics on how they're
performing via hyperparameters.

86
00:05:47.848 --> 00:05:51.626
Hyperparameters are the numbers that
go into the making of the model.

87
00:05:51.626 --> 00:05:57.486
Not the actual model itself it gets used
to make predictions on new pictures,

88
00:05:57.486 --> 00:06:02.909
but rather to come up with these
multipliers, like the learning rate.

89
00:06:02.909 --> 00:06:05.235
We took a big old step of 1.

90
00:06:05.235 --> 00:06:09.596
Do you remember we went,
make this one 1, -1, 1, 1, -1, -1?

91
00:06:09.596 --> 00:06:11.037
Big old step in one go.

92
00:06:11.037 --> 00:06:16.076
That was a huge step towards
finding the right conversion,

93
00:06:16.076 --> 00:06:23.255
the right weights that maximize converting
our pixels into the associated labels.

94
00:06:23.255 --> 00:06:27.861
But the learning rate is how
big a step we take each time,

95
00:06:27.861 --> 00:06:30.605
cuz we don't wanna overshoot and

96
00:06:30.605 --> 00:06:36.098
end up missing the best possible
set of multiplies for our sample.

97
00:06:36.098 --> 00:06:40.127
Then we go into deploying
the model in production.

98
00:06:40.127 --> 00:06:43.856
And here is where we have
to scale up to handle,

99
00:06:45.993 --> 00:06:48.116
&gt;&gt; Will Sentance: Every edge case in our,

100
00:06:48.116 --> 00:06:55.136
in this case, smile recognition model or
smile recognition neural network.

101
00:06:55.136 --> 00:06:59.762
We might have not 100,000 images, but

102
00:06:59.762 --> 00:07:03.683
we might have a million or more images.

103
00:07:03.683 --> 00:07:08.514
To do this,
we have to distribute the steps we took.

104
00:07:08.514 --> 00:07:13.425
And remember those key steps,
try up and down, up and down, up and

105
00:07:13.425 --> 00:07:18.101
down, up and down, up and down,
each weight independently.

106
00:07:18.101 --> 00:07:21.809
See whether for each of our sample images,

107
00:07:21.809 --> 00:07:27.379
does it lead to it being converted
better or worse to the target?

108
00:07:27.379 --> 00:07:31.968
If it does, then go up for
that, go down for

109
00:07:31.968 --> 00:07:35.158
this, go up for our weights.

110
00:07:35.158 --> 00:07:40.287
Do that all in one go, and
then do it all over again for

111
00:07:40.287 --> 00:07:43.203
our entire sample again, and

112
00:07:43.203 --> 00:07:48.461
then do it all over again for
our entire sample again.

113
00:07:48.461 --> 00:07:53.153
Maybe we'll be dealing with trillions of
different checks that we have to make,

114
00:07:53.153 --> 00:07:57.431
trillions of calculations,
quadrillions, beyond even calculation,

115
00:07:57.431 --> 00:07:59.367
of calculations have to be made.

116
00:07:59.367 --> 00:08:04.465
The chips that are perfectly tailored
towards doing that are our GPUs.

117
00:08:04.465 --> 00:08:08.302
Our originally graphical processing units,

118
00:08:08.302 --> 00:08:14.409
they're chips that are tailored to
doing lots of tiny, very basic math.

119
00:08:14.409 --> 00:08:19.256
Multiply this by this, this by this,
this by this, this by this.

120
00:08:19.256 --> 00:08:21.160
Then make a little tweak to this one.

121
00:08:21.160 --> 00:08:25.005
Try this by this, all of these the same,
this by this, this by this.

122
00:08:25.005 --> 00:08:30.550
That's very basic math, but so
many times you have to do it [LAUGH].

123
00:08:30.550 --> 00:08:35.437
GPUs are perfectly tailored to that
because they were designed for

124
00:08:35.437 --> 00:08:39.720
building accurate 3D environments,
make it feel real.

125
00:08:39.720 --> 00:08:43.697
The key thing for that is,
where's the light bouncing off???

126
00:08:43.697 --> 00:08:48.156
The physical space,
something called ray tracing,

127
00:08:48.156 --> 00:08:53.504
where you look at, in modeling
a 3D space in a game, whatever,

128
00:08:53.504 --> 00:09:00.061
where's all those individual beams of
light composed together as a light on?

129
00:09:00.061 --> 00:09:04.672
But each of those can be considered
an individual photon of light.

130
00:09:04.672 --> 00:09:06.958
And if you're modeling a space,

131
00:09:06.958 --> 00:09:11.368
you're tracking each of those
lights photons in a 3D game, and

132
00:09:11.368 --> 00:09:16.364
you're going, hey, it lands here,
then it's gonna bounce off here.

133
00:09:16.364 --> 00:09:20.359
Do a little bit of small calculations,
work out which direction it bounces next.

134
00:09:20.359 --> 00:09:23.520
Bounces here, bounces here, bounces here.

135
00:09:23.520 --> 00:09:26.213
And you do that for
all of those rays of light, and

136
00:09:26.213 --> 00:09:28.521
you get a 3D space that looks realistic.

137
00:09:28.521 --> 00:09:34.665
Little equations, multiply two numbers
together, but trillions of them.

138
00:09:34.665 --> 00:09:39.890
How lucky was NVIDIA that
the same type of math was

139
00:09:39.890 --> 00:09:44.997
needed at scale for
training neural networks.

140
00:09:44.997 --> 00:09:51.113
Little math calculations,
but infinite number of them.

141
00:09:51.113 --> 00:09:53.038
They use GPUs and tensor processing units,

142
00:09:53.038 --> 00:09:55.230
which are Google's kinda
proprietary version.

143
00:09:55.230 --> 00:09:59.589
It's even more tailored,
particularly for working with matrix or

144
00:09:59.589 --> 00:10:01.350
similar data structures.

145
00:10:01.350 --> 00:10:02.947
Get it production-ready.

146
00:10:02.947 --> 00:10:05.876
As before, get this set of multipliers.

147
00:10:05.876 --> 00:10:12.359
A trained neural network will be
a set of values, 12 in this case.

148
00:10:12.359 --> 00:10:15.137
There's your trained neural network,

149
00:10:15.137 --> 00:10:20.049
maybe stored as a transferable Python
object, a pickle out of Python.

150
00:10:20.049 --> 00:10:25.581
And then have the API deployed
that's going to take in the pixels

151
00:10:25.581 --> 00:10:30.917
that you are ensuring fit the same
shape as the training data.

152
00:10:30.917 --> 00:10:36.531
So the model's expecting them and
return out the prediction, smart or not.

153
00:10:36.531 --> 00:10:43.925
And crucially, monitoring and periodically
retraining this neural network,

154
00:10:43.925 --> 00:10:48.382
because that same idea of
the drift that we had for

155
00:10:48.382 --> 00:10:55.356
our fraud detection model as our sample
and our population fall out of sync.

156
00:10:55.356 --> 00:10:59.934
And there are techniques like SHAP,
S-H-A-P,

157
00:10:59.934 --> 00:11:03.777
Shapley, who is a famous mathematician.

158
00:11:03.777 --> 00:11:07.670
Additive explanations that
are designed to try to,

159
00:11:07.670 --> 00:11:11.386
even though here,
even with these 12 weights,

160
00:11:11.386 --> 00:11:16.699
it can be a bit of a pain in the ass
to try and work out what they're doing.

161
00:11:16.699 --> 00:11:18.531
I guess you can see the shape, right?

162
00:11:18.531 --> 00:11:22.616
The happy shape has a higher
positive numbers, sad shape has not.

163
00:11:22.616 --> 00:11:25.659
But for anything more complex than this,
not a chance.

164
00:11:25.659 --> 00:11:29.168
And to be honest, even here, why do
the eyes have like a negative weight?

165
00:11:29.168 --> 00:11:30.533
It's hard to know.

166
00:11:30.533 --> 00:11:34.962
And it works,
imagine any degree of complexity.

167
00:11:34.962 --> 00:11:40.872
Shap or S-H-A-P is an effort to improve
interpretability of neural networks.

168
00:11:40.872 --> 00:11:45.295
If you wanna use a neural network,
for example, deciding whether to

169
00:11:45.295 --> 00:11:49.351
give someone an insurance claim or
not, or claim it's fraud.

170
00:11:49.351 --> 00:11:54.151
You will need to be able to track at
least some sense what are the inputs,

171
00:11:54.151 --> 00:11:57.763
what are the features that
mattered in the decision?

172
00:11:57.763 --> 00:12:01.750
And it's trying to,
even in a varsity complex set of weights,

173
00:12:01.750 --> 00:12:04.017
cuz one layer you could work it out.

174
00:12:04.017 --> 00:12:05.945
Ten, you can't.

175
00:12:05.945 --> 00:12:07.738
There are techniques,

176
00:12:07.738 --> 00:12:12.670
and it's a massive part of machine
learning research right now,

177
00:12:12.670 --> 00:12:17.613
to improve opening up the black
box of how the prediction is made.

