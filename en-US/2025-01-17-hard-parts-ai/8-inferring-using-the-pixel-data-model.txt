[00:00:00]
>> Will Sentance: Let's use it. Assuming the sample reflects the population and think about that for a second. We're saying that we can't define a smile except for by giving a label saying this is a smile and this is not. Now, in a very small sample, you might be, well, that's not very sort of true.

[00:00:18]
It's not capturing the essence of a smile. In a larger sample, one would hope that you've labeled enough that you are capturing in some sense the essence of a smile. But that's vital because that sample better be reflective of the overall population. And the population is all possible images, 4,096 of them, smile or not.

[00:00:35]
But also in some senses, hopefully the essence of a smile. Some labels smile or not that there is an underlying I'm sorry. Again, getting a little bit sort of philosophical in the sense of what is a data, what is a smile? Well, it is a collection of [LAUGH] we've all agreed enough of us have agreed.

[00:00:54]
That's roughly what a smile looks like. There is no perfect definition of a smile. It is only a sample of labeled that we've agreed that's a smile and that's not. That is why, by the way, we mentioned scale AI. There's also an amazing research project out of many universities, but I know about it out of Oxford, that is about ensuring that the people doing the labelling of what is increasingly becoming the truth of our society.

[00:01:20]
That is judgments on, from models, the labelers are also representative of a range of cultures, experiences, and definitions of what a smile is. It's a really phenomenal project led by some researchers at Oxford Internet Institute or at least among others. But if really very important because it's literally, that's what's determining.

[00:01:42]
It's only the definition of the smile as so far as the sample says 1 and minus 1. Therefore, okay, so let's go in. We can use this converter on new images of population that don't have a label 1 or minus 1 and run the converter on it to predict their label.

[00:01:57]
Running the converter people, remember this term, inference. Training is coming up with the multipliers to do the conversion. Inference, using those multipliers on a new set of numbers. Folk, let's do it.
>> Will Sentance: I think I only say folk in this room I don't know I've sort of stopped saying anyway.

[00:02:19]
So we don't know if it's a smile or not. Smile, question mark. Let's apply our multipliers, people. Let's apply them, let's apply them, let's apply them. So multiplied by 0. I'm gonna put it here just so we get really clear that we're taking just those set of multipliers.

[00:02:38]
I know it's a bit, do I have to write out again? But, just so we know we're using these now and then we're gonna multiply and then sum all.
>> Will Sentance: No, Maria, you're up. You've already done it. Sorry Kayla, you're up. Give me everyone together, 0 plus.
>> Ben: 0.

[00:03:01]

>> Will Sentance: Plus.
>> Ben: 0.
>> Will Sentance: Plus.
>> Ben: 0.
>> Will Sentance: Who's the only one doing this? Thank you, Ben, amazing. If it's not, Ben, you're not allowed to speak now, so everyone else.
>> Speaker 3: Just like grade school.
>> Will Sentance: [LAUGH] that's good. All right, 0 + 0 + 0 + 0 plus and not allowed to talk everyone else.

[00:03:22]

>> Speaker 4: Plus 0, 0.
>> Will Sentance: Yeah.
>> Speaker 4: Plus 0.
>> Will Sentance: Yeah.
>> Speaker 4: Plus unknown.
>> Will Sentance: I don't konw plus unknown, that's why it's so important, everybody. Ben was carrying us, plus what, everyone?
>> Speaker 5: Minus 1.
>> Will Sentance: Negative 1, 1 multiplied by negative 1 and multiplier is negative 1 here plus.

[00:03:41]

>> Speaker 4: 0.
>> Will Sentance: Plus.
>> Ben: 0.
>> Will Sentance: Plus 0 by 1.
>> Speaker 4: 0.
>> Will Sentance: 0 Plus 1 by 0.
>> Speaker 4: 0.
>> Will Sentance: Sum is 2.
>> Speaker 4: Negative 1.
>> Speaker 5: Negative 1.
>> Will Sentance: Negative 1, so is this a smile? According to our samples, definition of a smile. Is this a smile?

[00:04:01]
No, well done everybody. No smile, well done folk. All right, so really good. Let's get some names of this stuff down. We've trained our set of multipliers to work 100% accuracy for our sample. Our training stage is 100% accurate. We've then used that set of multipliers to run on our new image of whatever this is, and said smile, and we've evaluated it, and we've inferred, inference, -1, no smile.

[00:04:40]
So let's get some names of this stuff. What are these numbers known as? Who knows the name of these numbers? These people, 1,2,3,4,5, these 12 are, what?
>> Speaker 4: The vectors?
>> Will Sentance: No.
>> Speaker 4: Parameters?
>> Will Sentance: The weights.
>> Speaker 5: The weights.
>> Will Sentance: That's all they are people.
>> Speaker 4: Gotcha.
>> Will Sentance: That's all they are.

[00:05:01]
The multipliers are the weights. That's all they are. That profound thing of neural networks. It's these 12 numbers multiplying our input pixels that when multiplied, we get, and sum up an output, or at least a converted output, and compare it to our target, if it matches. Our model's weights are good.

[00:05:22]
Okay, our weights. This is a layer of weights. The word layer is used so flexibly in this space that you could also say this is a layer of pixels, an input layer. Input layer, layer weights. We could also call this output value, the summed output value, the output layer.

[00:05:46]
So in a sense, there's 1, 2, 3, layers here. But we'd never call this a three layer network. We'd call this a single layer network, neural network because it has a single layer of weights. So this here, people, is a single layer neural network. Single layer of weights.

[00:06:08]
And it's successfully trained on our sample, on our training data, to successfully trained, achieve a fit. For our sample, for our training data, that allows us to then make inferences about new images, okay? Association with brain structure more metaphorical. The term neural network, obviously, suggests patterns similar to the brain.

[00:06:43]
It's more metaphorical, just to be specific, first the nodes here, that's the pixels, but are equivalent to neurons, if you know biology or neuroscience better than me and the connections. Things that take those input values and connect them through to an output value are similar to what you would say in America, the synapses that we say in the UK synapses, but it's more metaphorical.

[00:07:18]
The math that goes on in identifying what the right weights or multipliers are, at least until recently, does not appear. And in fact, is it Crick? The person who identified DNA, when these ideas first started to emerge, wrote a big piece saying it is absolutely in no way similar to a neural Network in the brain.

[00:07:41]
But there is indication that some of the ways in which multipliers are identified in an artificial, that's what we do and we're doing today. Neural network might actually look a lot more like the brain's way of developing these patterns. I think if you think about we've identified the algorithm and your network that at scale best captures complexity in our world, complexity data distributions.

[00:08:06]
That is so similar or at least analogous potentially to the brain, which is the most brilliant tool for understanding complexity of data to distributions obviously ever. Well, I said that, depending on one's belief system, but is pretty profound, it's not unlikely that there might be some serious analogy.

[00:08:28]
And the research right now looks at are there ways in which the artificial, the ones that are in the brain, neural networks might identify these multipliers, we're gonna see how to do that in a moment, that is more like the brain does. And does the brain, actually, sometimes do things that are more like the way that artificial neural networks go about identifying these multipliers as well?

[00:08:53]
And if it is, maybe there is greater similarity than purely analogy, okay? Before you do thumbs or I think we just keep going for a second and do thumbs so do these multipliers have any meaning. They must do something it must be that they mean something if they're that good.

[00:09:19]
Converting, do they mean something? The different numbers multiply each pixel, have some potential meaning, but it's very contingent on how we set this problem up, how we label the data, and what we say the pixel numbers are. So in this case, where pixels are dark when they are a feature of the face.

[00:09:42]
So, it's not actually, literally the color of the feature. It's more the shadows. If you look at someone's eyes, they're darker because of a shadow. There's an edge in a three dimensional space. There's a going from my hands all over my face, going from here to in here.

[00:10:00]
Where the features are darker, meaning shadow particularly. So mouth, again, lips, shadow eyes. If we've given numbers of positive number for that and stuff where not much is going on in the face. A 0 and we've said a positive number of 1 when there's a smile and a negative number of negative 1, sorry, when there's not then.

[00:10:29]
The multipliers will be positive for those pixels, there's a positive 1, for those, pixels that are most pixels being stuff, pixels which are most distinctive stuffs going on in the smile images where the output is positive. Because then the one multiplied by the one is a one which is positive, which gives us our target.

[00:10:53]
And the multipliers will be negative, where the pixels with stuff going on are most distinctive to the not smile. Because then we get a thing going on, negative, negative, and that's associated with a not smile. So in this setup, the multipliers act a little bit like correlations. You'll see high positive for pixels, strongly correlated or associated with pixels that are important in the smile or to pixels that it's important that they're dark in the smile.

[00:11:26]
So think about a smile, the lower bit is where the dark is. And when you smile the lower bit is where the light is or it's the north shadow Everyone got that? So in this case, there'll be correlations. High positive multipliers for pixels strongly associated, correlated with a smile 1, and negative with those strongly associated with smile 0.

[00:11:52]
But it's only because we defined smile 1 and minus 1 as the labels and therefore the numbers match. And we define not much going on as 0 and lots of stuff going on as 1. So they look like correlations here, but it's not intrinsic. It's how we label the stuff.

[00:12:10]
We gotta label these other, we gotta label these flipped and now they'd be reversed [LAUGH] it doesn't matter. I think what is deeper true is that the multipliers have more stuff going on when there's more important stuff going on in those pixels. So there's some important stuff going on expect bigger numbers, because we wanna generate out some numbers.

[00:12:32]
But even that's contingent on me wanting the output of 1 and minus 1 and not an output of 0 for stuff going on. But at very least, whatever you do, there's some association between stuff going on in the pixels and the underlying data. Stuff happening and weights or multipliers being more non zero, thing happening with them.

[00:12:54]
For those pixels that have stuff going on either towards an image of a smile or not. And that does capture, even at a scale neural network, there's some sense of that. Stuff going on has greater, weight or importance to it. But it's very specifically looks a bit like correlation only because of how we set the problem up.

