WEBVTT

1
00:00:00.000 --> 00:00:04.002
&gt;&gt; Will Sentance: Let's do so, and
just note people you might be well, hey,

2
00:00:04.002 --> 00:00:08.564
what do we look for in the population
when we run this new converter,

3
00:00:08.564 --> 00:00:10.984
followed by the sigmoid function?

4
00:00:10.984 --> 00:00:15.387
We look for a percentage that hopefully
is close to 100% and it's a smile and

5
00:00:15.387 --> 00:00:16.978
close to 0% and it's not.

6
00:00:16.978 --> 00:00:21.493
Okay, so, expanding our sample and
applying gradient descent,

7
00:00:21.493 --> 00:00:26.171
small adjustments to weights improve
the conversion accuracy, but

8
00:00:26.171 --> 00:00:29.973
actually we didn't manage
to find a perfect matching.

9
00:00:29.973 --> 00:00:34.507
So instead,
we're running the output of those weights,

10
00:00:34.507 --> 00:00:38.494
which was respectively 1, -1, -1, and 3.

11
00:00:38.494 --> 00:00:43.870
Through our sigmoid function,
through our sigmoid function,

12
00:00:43.870 --> 00:00:49.246
through our sigmoid function,
through our sigmoid function,

13
00:00:49.246 --> 00:00:53.534
and
&gt;&gt; Will Sentance: Getting probably not

14
00:00:53.534 --> 00:00:59.838
immediately a great outcome,
at least not for all of them.

15
00:00:59.838 --> 00:01:04.100
But we're on a better path, because
now there's gonna be a value here and

16
00:01:04.100 --> 00:01:06.849
a value here that will
get us close to 100%,

17
00:01:06.849 --> 00:01:10.524
there was no 3 was never gonna be 1 and
1 was never gonna be 1.

18
00:01:10.524 --> 00:01:15.313
So what do we think sigma 1 was roughly,
anyone remember, Jamie,

19
00:01:15.313 --> 00:01:18.517
do you remember what it
roughly was sigma 1?

20
00:01:18.517 --> 00:01:19.531
&gt;&gt; Jamie: 73.
&gt;&gt; Will Sentance: 73%,

21
00:01:19.531 --> 00:01:23.914
it climbs fast, doesn't it, right,
which is quite good, is that 100%?

22
00:01:23.914 --> 00:01:28.986
Nah, too far away, but we'll get there,
we'll get closer with our

23
00:01:28.986 --> 00:01:34.427
next round of tweaks -1, anyone
remember this 1, it was the reverse?

24
00:01:34.427 --> 00:01:35.413
&gt;&gt; Jamie: 27.
&gt;&gt; Will Sentance: 27%,

25
00:01:35.413 --> 00:01:40.124
spot on, it's not close enough,
too far away, -1 again 27%,

26
00:01:40.124 --> 00:01:44.592
too far away, this was quite good,
what's the output of 3 Joe,

27
00:01:44.592 --> 00:01:47.948
do you roughly remember
what the output of 3 is?

28
00:01:47.948 --> 00:01:49.212
&gt;&gt; Jamie: Is it 95?

29
00:01:49.212 --> 00:01:52.213
&gt;&gt; Will Sentance: Yea, spot on 1 under
Joe exactly, so that 1 actually is

30
00:01:52.213 --> 00:01:56.463
the only one that's roughly right, let's
just say these ones are also still not

31
00:01:56.463 --> 00:02:00.173
close enough, that's the only one
that's actually roughly right.

32
00:02:00.173 --> 00:02:06.068
So we've actually got accuracy
already of roughly 1 out of 4.

33
00:02:06.068 --> 00:02:09.038
After one iteration of weight tweaks,

34
00:02:09.038 --> 00:02:14.987
we get this output known as one iteration
of training using gradient descent.

35
00:02:14.987 --> 00:02:19.461
Gradient descent is as we say, the ability
to tweak weights independently due

36
00:02:19.461 --> 00:02:22.260
to partial differentiation
through calculus.

37
00:02:22.260 --> 00:02:27.254
And then do it all at once and know that
that was the right direction to go towards

38
00:02:27.254 --> 00:02:31.125
more accurate conversion for
this model, for this sample.

39
00:02:31.125 --> 00:02:35.516
After one iteration,
even with our converter it's all right,

40
00:02:35.516 --> 00:02:38.930
after 1 before we use
sigmoid it was really good,

41
00:02:38.930 --> 00:02:43.097
it was 3 out of 4, but
we were never gonna get to 4 out of 4.

42
00:02:43.097 --> 00:02:48.061
So we switch to applying sigmoid,
which took us back down to 1 out of 4, but

43
00:02:48.061 --> 00:02:52.650
I promise you there's a route now to
get through this number going up.

44
00:02:52.650 --> 00:02:55.310
This one going up, and
these two going down,

45
00:02:55.310 --> 00:02:57.554
there's a route to get to 4 out of 4.

46
00:02:57.554 --> 00:02:58.929
Joe, go ahead.

47
00:02:58.929 --> 00:03:05.482
&gt;&gt; Joe: I was just curious, why did we
choose 2, is there a specific reason?

48
00:03:05.482 --> 00:03:09.533
&gt;&gt; Will Sentance: Why did I
say 2 would be happy with?

49
00:03:09.533 --> 00:03:14.460
Just because we haven't got 2 yet,
we haven't got those,

50
00:03:14.460 --> 00:03:21.606
that's just I'm saying if this reached
less than -2, and this reach less than -2,

51
00:03:21.606 --> 00:03:25.162
then the output will be somewhere below
&gt;&gt; Joe: 10%.

52
00:03:25.162 --> 00:03:28.946
&gt;&gt; Will Sentance: 10%, and at that point-
&gt;&gt; Joe: And that's a [INAUDIBLE] sigma.

53
00:03:28.946 --> 00:03:32.471
&gt;&gt; Will Sentance: Yeah, exactly,
it's does at that point,

54
00:03:32.471 --> 00:03:37.525
that's roughly good enough
within industry, you are roughly

55
00:03:37.525 --> 00:03:42.688
happy getting a 90% conversion,
you might wanna go higher.

56
00:03:42.688 --> 00:03:49.597
If you know it's not a smile,
then your output of your converter to be,

57
00:03:49.597 --> 00:03:54.908
I don't know, 8%,
doesn't need to be 0 but 8%.

58
00:03:54.908 --> 00:03:59.196
In fact of course in practice, you may
wanna have more like a -5 in there,

59
00:03:59.196 --> 00:04:02.277
which would give you an output
of do you remember Joe?

60
00:04:02.277 --> 00:04:05.320
-5 would be an output of about-
&gt;&gt; Joe: 1%

61
00:04:05.320 --> 00:04:06.408
&gt;&gt; Will Sentance: About 1%, right,

62
00:04:06.408 --> 00:04:10.217
that would be even better probably,
although maybe it gets too perfect for

63
00:04:10.217 --> 00:04:14.397
the sample, and might be too suited to
the sample but not the overall population.

64
00:04:14.397 --> 00:04:19.516
I'd be over fitted, but yeah,
I just know that by the time

65
00:04:19.516 --> 00:04:24.219
the sum weight of these
pixels with these multipliers

66
00:04:24.219 --> 00:04:29.253
comes to below -2,
right now it comes to, what was it?

67
00:04:29.253 --> 00:04:33.139
Right now it comes to -1, right,
yeah, -1, right now comes -1,

68
00:04:33.139 --> 00:04:37.027
by the time these weights have been
tweaked enough for it to come to -2,

69
00:04:37.027 --> 00:04:39.989
then we know the output of
running it through sigmoid.

70
00:04:39.989 --> 00:04:44.973
Will be less than 10% and I will be
roughly happy with the conversion through

71
00:04:44.973 --> 00:04:48.154
that set of weights of this
element of the sample?

72
00:04:48.154 --> 00:04:53.810
After 100, I get to look at my numbers,
these weights,

73
00:04:53.810 --> 00:04:58.901
these multipliers give me
an output of not 1, -1,

74
00:04:58.901 --> 00:05:04.010
-1 and 3 but 2,- 2.4, -2.3 and 5.2.

75
00:05:04.010 --> 00:05:07.046
And that gives me sigmoid
adjusted really damn good,

76
00:05:07.046 --> 00:05:09.010
we're gonna use them in a second.

77
00:05:09.010 --> 00:05:15.680
But Joe, I could keep iterating and
steadily, steadily, steadily,

78
00:05:15.680 --> 00:05:21.016
I'll be getting closer and
closer to 99.9% sorry,

79
00:05:21.016 --> 00:05:26.149
99.9%, 0.01%, that's the trade-off.

80
00:05:26.149 --> 00:05:32.329
I could keep tweaking, keep checking,
is it converting the sample a bit better?

81
00:05:32.329 --> 00:05:35.936
And for a sample of 4 we could probably
try and get there more quickly, right?

82
00:05:35.936 --> 00:05:38.342
We could work out,
just keep increasing these three,

83
00:05:38.342 --> 00:05:41.379
keep decreasing these three until
they're really big numbers, and

84
00:05:41.379 --> 00:05:43.440
we would end up with
a really good conversion.

85
00:05:43.440 --> 00:05:47.930
But for more complex examples,
there's gonna be some cases where it's

86
00:05:47.930 --> 00:05:52.507
tweaking this one keeps going up, and
it's always gonna make it better.

87
00:05:52.507 --> 00:05:57.560
But this one if you tweak it down too far,
it starts to make it all worse.

88
00:05:57.560 --> 00:06:01.608
And so for millions of these weights
which is maybe what we'd have sometimes,

89
00:06:01.608 --> 00:06:05.799
you don't know which way to go for all of
them, does it continue that direction?

90
00:06:05.799 --> 00:06:09.817
No, you need to each time
check all of them, okay, good,

91
00:06:09.817 --> 00:06:12.777
that was the right step then, then again.

92
00:06:12.777 --> 00:06:16.737
And you've got to run your whole
sample through all those weights and

93
00:06:16.737 --> 00:06:21.240
make sure that it's better than it was
the last time, and then do it again, and

94
00:06:21.240 --> 00:06:23.512
then do it again, and then do it again.

95
00:06:23.512 --> 00:06:27.516
That is very compute costly,
very compute costly,

96
00:06:27.516 --> 00:06:31.793
meaning part of the job of
machine learning engineer and

97
00:06:31.793 --> 00:06:37.812
in this space is to go, do I want to keep
the iterations of this training going?

98
00:06:37.812 --> 00:06:42.999
I might get closer but it's costing
me a lot of compute time and money.

99
00:06:42.999 --> 00:06:47.423
But let's see these ones here, after 100
iterations our weights get close enough,

100
00:06:47.423 --> 00:06:50.368
the average error now is only 7%,
that's pretty good.

101
00:06:50.368 --> 00:06:54.156
We've got 1% off on one,
we're 9% off on that one,

102
00:06:54.156 --> 00:06:59.647
we're 8% off on the second image,
we want 0%, 0% for our two respective.

103
00:06:59.647 --> 00:07:02.896
If not smiles you want 100% for

104
00:07:02.896 --> 00:07:08.057
our two smiles 89% and
99% that's pretty good.

