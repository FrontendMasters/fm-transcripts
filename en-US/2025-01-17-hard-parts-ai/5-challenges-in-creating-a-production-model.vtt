WEBVTT

1
00:00:00.000 --> 00:00:02.780
&gt;&gt; Will Sentance: Getting
our fraud detection model.

2
00:00:02.780 --> 00:00:04.055
This is the fraud detection model.

3
00:00:04.055 --> 00:00:08.883
A set of FL essentially,
to production, live on the Internet.

4
00:00:08.883 --> 00:00:10.407
We have two major parts to this.

5
00:00:10.407 --> 00:00:12.435
One, we've been doing essentially
here on the whiteboard,

6
00:00:12.435 --> 00:00:13.451
on the blackboard for a while.

7
00:00:13.451 --> 00:00:18.595
And maybe a data scientist would actually
get on the blackboard and do it like this.

8
00:00:18.595 --> 00:00:23.218
Data exploration and
prototype our model and model pro, okay.

9
00:00:23.218 --> 00:00:24.820
That's a good timing,
and prototype our model.

10
00:00:24.820 --> 00:00:26.970
Often, data scientist-led.

11
00:00:26.970 --> 00:00:29.749
Smaller sample, here we had 100,000,

12
00:00:29.749 --> 00:00:34.472
well, actually here we had 4, would be
about 10 to 20% of the final data set

13
00:00:34.472 --> 00:00:38.994
that we ultimately use to develop our
production version of this converter.

14
00:00:38.994 --> 00:00:42.649
That we can then use on all
future refund requests.

15
00:00:42.649 --> 00:00:46.425
Maybe it takes only
a couple of weeks of data.

16
00:00:46.425 --> 00:00:50.866
Because a data scientist is going
to want to, as Ben's saying,

17
00:00:50.866 --> 00:00:55.079
play with a number of different
approaches, explore them.

18
00:00:55.079 --> 00:00:58.944
They don't wanna get into
perfection every edge-case handle,

19
00:00:58.944 --> 00:01:02.256
they wanna play with a number
of different approaches.

20
00:01:02.256 --> 00:01:09.370
The goal of this piece, understand
the problem and the nature of the data.

21
00:01:09.370 --> 00:01:11.210
So what are some of
the pieces involved in that?

22
00:01:11.210 --> 00:01:12.404
Well, a lot of pre-processing.

23
00:01:12.404 --> 00:01:17.063
Pre-processing might be,
for example, I don't know,

24
00:01:17.063 --> 00:01:21.627
it could be rounding the dollar
amount to a whole number,

25
00:01:21.627 --> 00:01:24.778
rather than carrying about the cents.

26
00:01:24.778 --> 00:01:31.225
It could be categorizing the types of
items, we heard Kayla say, type of items.

27
00:01:31.225 --> 00:01:35.822
It could be categorizing
the types of items into,

28
00:01:35.822 --> 00:01:42.390
not just every single individual title,
but categorizing them into,

29
00:01:42.390 --> 00:01:46.887
I don't know, like,
perishables versus not.

30
00:01:46.887 --> 00:01:52.192
Cuz I'm a lot happier to get a refund
on something I get to keep forever than

31
00:01:52.192 --> 00:01:57.584
just on a, so categorizing, lots of
things you could do to pre-process,

32
00:01:57.584 --> 00:02:00.426
including, filling in missing data.

33
00:02:00.426 --> 00:02:06.407
Maybe for some, older data, or
we don't have restaurant or we don't have,

34
00:02:06.407 --> 00:02:11.192
I don't know, we don't have
information on the council IP,

35
00:02:11.192 --> 00:02:14.522
the IP is masked, or whatever it might be.

36
00:02:14.522 --> 00:02:16.037
There's lots of,

37
00:02:16.037 --> 00:02:21.631
that will prevent us running our
converter if we don't have any data.

38
00:02:21.631 --> 00:02:26.414
Huge part, feature selection,
pruning, engineering.

39
00:02:26.414 --> 00:02:31.429
So, feature selection was, well,
which features did we select?

40
00:02:31.429 --> 00:02:35.069
I'm gonna call on someone
who's not looking right now.

41
00:02:35.069 --> 00:02:38.166
You're all looking.
Kayla, which features did we select?

42
00:02:38.166 --> 00:02:39.666
Well, you selected what?

43
00:02:39.666 --> 00:02:41.527
&gt;&gt; Kayla: The account links.

44
00:02:41.527 --> 00:02:42.573
&gt;&gt; Will Sentance: Account links?

45
00:02:42.573 --> 00:02:46.239
&gt;&gt; Kayla: And the amount of the refund,

46
00:02:46.239 --> 00:02:48.677
and the number of-
&gt;&gt; Will Sentance: Accounting IPs.

47
00:02:48.677 --> 00:02:50.144
Accounting IP, yeah, exactly.

48
00:02:50.144 --> 00:02:51.319
And then which one do we prune?

49
00:02:51.319 --> 00:02:54.380
Well, I guess we didn't
really use any of these, but

50
00:02:54.380 --> 00:02:57.180
pruning would be as we
developed our converter.

51
00:02:57.180 --> 00:03:00.384
Maybe we learned that one of
these was not really important.

52
00:03:00.384 --> 00:03:02.262
It wasn't a very useful
thing to take into account.

53
00:03:02.262 --> 00:03:05.205
But I could also say we pruned out our,
they were expecting to use our date,

54
00:03:05.205 --> 00:03:06.624
we got rid of our date, didn't we?

55
00:03:06.624 --> 00:03:09.878
We didn't use our date in the end.

56
00:03:09.878 --> 00:03:11.439
And feature engineering.

57
00:03:11.439 --> 00:03:14.459
This work, kind of,
to be fair, by the way,

58
00:03:14.459 --> 00:03:18.988
categorizing the types of items
might be feature engineering, but

59
00:03:18.988 --> 00:03:23.453
also amount, let's say,
amount in relation to account length.

60
00:03:23.453 --> 00:03:24.994
Maybe that's an important factor.

61
00:03:24.994 --> 00:03:27.818
Or maybe account length
versus accounts on IP.

62
00:03:27.818 --> 00:03:31.988
Maybe if you've got a lot of accounts
on an IP right at the same time as, and

63
00:03:31.988 --> 00:03:35.916
that might be captured by the conversion
rules, but it might not be.

64
00:03:35.916 --> 00:03:41.924
Maybe you're interested in accounts
on IP per year of account length.

65
00:03:41.924 --> 00:03:46.030
Maybe you're much more worried
about a 150 accounts on a very,

66
00:03:46.030 --> 00:03:50.500
very new account, a very, so
a 150,000 IP on a very new account,

67
00:03:50.500 --> 00:03:53.609
then you would be on one that's 6,
7 years old.

68
00:03:53.609 --> 00:03:57.835
Or maybe it's the change in the number
of accounts on IP in the past week.

69
00:03:57.835 --> 00:04:01.956
Maybe that's a way of capturing
whether it's, I don't know,

70
00:04:01.956 --> 00:04:06.379
a building that has many people in it,
a dorm or a co-working space,

71
00:04:06.379 --> 00:04:09.989
versus someone who's just
suddenly spun up a bot farm.

72
00:04:09.989 --> 00:04:13.364
These are the kind of factors
that you would engineer.

73
00:04:13.364 --> 00:04:21.014
You would turn accounts on
IP into accounts over time.

74
00:04:21.014 --> 00:04:23.928
You would do some calculations,
some engineering,

75
00:04:23.928 --> 00:04:25.944
some playing with the input values.

76
00:04:25.944 --> 00:04:29.982
That's our feature, engineering.

77
00:04:29.982 --> 00:04:35.132
Fixing imbalanced data, a big challenge
for fraud detection models in general,

78
00:04:35.132 --> 00:04:39.235
and we're speaking here to more broadly,
models like this, but

79
00:04:39.235 --> 00:04:42.983
this one particularly has
a problem with imbalanced data.

80
00:04:42.983 --> 00:04:46.619
That is to say that 99% of
the refund requests are legit, and

81
00:04:46.619 --> 00:04:48.964
were given a refund, and 1% wasn't.

82
00:04:48.964 --> 00:04:53.772
Meaning in 100,000 refund requests,
actually 99,000 of them were in

83
00:04:53.772 --> 00:04:57.442
one category, and
only 1,000 were in the other category.

84
00:04:57.442 --> 00:05:02.563
You actually don't have much data on
those that were not given refunds.

85
00:05:02.563 --> 00:05:08.087
You actually got a little bit, it makes
the techniques that are used to identify

86
00:05:08.087 --> 00:05:13.461
the right boundaries less effective
when you've got such imbalanced data.

87
00:05:13.461 --> 00:05:18.567
It's quite an extreme,
99,000 of these were not fraud,

88
00:05:18.567 --> 00:05:22.546
were given a refund, only 1,000 weren't.

89
00:05:22.546 --> 00:05:26.680
The techniques to find the best
conversion boundaries,

90
00:05:26.680 --> 00:05:32.031
like Gini impurity metrics do not work so
easily on such imbalanced data.

91
00:05:32.031 --> 00:05:37.975
So it is not to go into the details of it
but rather to say, finding techniques to

92
00:05:37.975 --> 00:05:43.485
overcome these is a crucial part of
this stage of the model development.

93
00:05:43.485 --> 00:05:51.051
Things like, synthetic minority
over sampling, or SMOTE, S-M-O-T-E.

94
00:05:51.051 --> 00:05:55.443
So, synthetic minority
over sampling technique,

95
00:05:55.443 --> 00:06:00.334
SMOTE, would create some
refund-not-given sample for

96
00:06:00.334 --> 00:06:04.640
helping develop our
converter more effectively.

97
00:06:04.640 --> 00:06:07.829
And then interpretability,
we already talked about this.

98
00:06:07.829 --> 00:06:12.051
Do we need to prioritize understanding
how the decisions are made?

99
00:06:12.051 --> 00:06:18.270
In this case,
our decision tree gives us boundaries.

100
00:06:18.270 --> 00:06:22.865
And then,
experiment with different approaches.

101
00:06:22.865 --> 00:06:26.745
We used a single converter here,
in practice,

102
00:06:26.745 --> 00:06:31.891
we very much might experiment
with ensemble learning method.

103
00:06:31.891 --> 00:06:39.731
So that's combining up multiple models in
this case to see which one works better.

104
00:06:39.731 --> 00:06:44.548
Note that, particularly for
a decision tree, if we start by first

105
00:06:44.548 --> 00:06:49.278
making sure we classify as many
as possible based on their years,

106
00:06:49.278 --> 00:06:54.026
then dollars, then accounts on IP,
and then 10 more factors.

107
00:06:54.026 --> 00:06:56.243
It's what's called a greedy algorithm.

108
00:06:56.243 --> 00:07:01.203
It might well be, it maximizes the class,
putting them in the right bucket 1 or

109
00:07:01.203 --> 00:07:04.325
-1 for years, and
then maximizes for dollars.

110
00:07:04.325 --> 00:07:06.605
But what if there's some
interplay between these?

111
00:07:06.605 --> 00:07:10.348
You might actually not get to
the very best set of boundaries for

112
00:07:10.348 --> 00:07:11.625
your entire sample.

113
00:07:11.625 --> 00:07:15.967
If you'd started with accounts on IP and
then moved back and got each of

114
00:07:15.967 --> 00:07:20.898
them optimally the other direction,
you might have ended up with a different,

115
00:07:20.898 --> 00:07:24.603
better set of boundaries that
convert more of your sample.

116
00:07:24.603 --> 00:07:28.969
Ensemble learning techniques will allow
you to, and this, again, just many, many,

117
00:07:28.969 --> 00:07:29.914
many approaches.

118
00:07:29.914 --> 00:07:33.380
My point here is, wow, there's a heck
of a lot of the stuff you can do here,

119
00:07:33.380 --> 00:07:35.099
it's obviously a huge discipline.

120
00:07:35.099 --> 00:07:39.362
But an ensemble learning technique,
for example, random forests,

121
00:07:39.362 --> 00:07:44.275
might allow us to try different decision
trees, follow different directions,

122
00:07:44.275 --> 00:07:47.975
and maybe get to a globally,
maximally, best conversion.

123
00:07:47.975 --> 00:07:53.257
As opposed to only the one that,
if you follow this route, start here,

124
00:07:53.257 --> 00:07:58.645
start here, then here, gets you to
only a locally optimal conversion.

125
00:07:58.645 --> 00:08:00.165
And then evaluation metrics.

126
00:08:00.165 --> 00:08:01.543
So, assess how well it did.

127
00:08:01.543 --> 00:08:03.520
Well, we got 4/4 and 1/1.

128
00:08:03.520 --> 00:08:08.506
But actually in fraud detection models,
that may not be very good.

129
00:08:08.506 --> 00:08:13.784
You could get 99%, I love this,
you can get 99% conversion for

130
00:08:13.784 --> 00:08:18.880
your sample of fraud detection
that has 99,000 not fraud and

131
00:08:18.880 --> 00:08:22.157
1,000 fraud, didn't give refund.

132
00:08:22.157 --> 00:08:25.203
You could give 99% conversion accuracy,
and

133
00:08:25.203 --> 00:08:29.893
actually you could do that by just saying,
every single case is not fraud.

134
00:08:29.893 --> 00:08:33.209
Every single case gets a refund.

135
00:08:33.209 --> 00:08:38.447
You'd be 99% right for
the 99,000 that got a refund.

136
00:08:38.447 --> 00:08:42.378
It just happens that the 1,000 that didn't
were the very ones that did the fraud.

137
00:08:42.378 --> 00:08:45.530
So for fraud detection algorithms,
or fraud detection models,

138
00:08:45.530 --> 00:08:48.177
you care also about things
like precision and recall.

139
00:08:48.177 --> 00:08:52.417
Which is, of the ones that were actually
fraud, how many were converted?

140
00:08:52.417 --> 00:08:55.267
Of this one, we could get 3 out of 4.

141
00:08:55.267 --> 00:08:58.822
Let's say we had 99 out of 100 and
this was the one that we got wrong,

142
00:08:58.822 --> 00:09:00.818
that's the bloody one that's a fraud.

143
00:09:00.818 --> 00:09:02.360
That's the one you need to get right.

144
00:09:02.360 --> 00:09:07.330
So metrics like evaluation, metrics like
precision, which is of the ones that were

145
00:09:07.330 --> 00:09:10.977
actually fraud, or labeled so,
how many did they get right?

146
00:09:10.977 --> 00:09:14.319
How many did the model get right of those?

147
00:09:14.319 --> 00:09:16.078
And there's many metrics like this.

148
00:09:16.078 --> 00:09:20.414
And all of it can then be maintained and
tracked and the experiments,

149
00:09:20.414 --> 00:09:23.931
very exploratory,
trying different approaches, but

150
00:09:23.931 --> 00:09:27.392
you want to keep track of it,
using a tool like MLflow.

151
00:09:27.392 --> 00:09:32.410
Which actually, shout out to,
which is the default framework for

152
00:09:32.410 --> 00:09:38.066
tracking and maintaining your
experiments and your evaluation metrics.

153
00:09:38.066 --> 00:09:43.859
And shout out to Codesmith team,
who just announced MLflow JS,

154
00:09:43.859 --> 00:09:51.278
a JavaScript, at least it hooks into
MLflow, JavaScript interface for ML Flow.

155
00:09:51.278 --> 00:09:54.164
Next part, produce that for production.

156
00:09:54.164 --> 00:09:57.842
You gotta get this bloody
converter online so

157
00:09:57.842 --> 00:10:01.130
you can get about using it to infer live,

158
00:10:01.130 --> 00:10:07.058
shall we give Paul this refund that
they just clicked the Ask button for?

159
00:10:07.058 --> 00:10:07.889
How do we do it?

160
00:10:07.889 --> 00:10:10.600
This is often ML engineer-led.

161
00:10:10.600 --> 00:10:13.534
None of this is exclusive,

162
00:10:13.534 --> 00:10:19.410
these terms are kind of very,
very overly expansive.

163
00:10:19.410 --> 00:10:24.002
And that was, by the way, similar to
software engineering for many years,

164
00:10:24.002 --> 00:10:25.810
not very well defined terms.

165
00:10:25.810 --> 00:10:27.998
You had system engineers,
you had technical,

166
00:10:27.998 --> 00:10:31.457
you had a lot of terms that all
essentially have become software engineer.

167
00:10:31.457 --> 00:10:36.710
Same thing here, they're a bit vague and
it's evolving so fast, but

168
00:10:36.710 --> 00:10:42.250
the bit of getting it live online at
scale, typically movie ML engineer.

169
00:10:42.250 --> 00:10:46.501
Here you're now caring about
expanding your sample from maybe

170
00:10:46.501 --> 00:10:49.558
100,000 to get the best conversion.

171
00:10:49.558 --> 00:10:53.331
That the prototyping data scientist said,
look,

172
00:10:53.331 --> 00:10:57.116
we roughly there,
to working on the full data set.

173
00:10:57.116 --> 00:11:00.613
Because, a good example would be,
suppose your data scientist is

174
00:11:00.613 --> 00:11:05.111
working with 100,000 to be able to
experiment rapidly, rapidly prototype,

175
00:11:05.111 --> 00:11:06.930
they may only take a month of data.

176
00:11:06.930 --> 00:11:12.140
Great, most of the patterns are gonna
be independent of the time of the year.

177
00:11:12.140 --> 00:11:15.269
But if you wanna take
that model to production,

178
00:11:15.269 --> 00:11:19.586
there may be subtle differences
between December and February.

179
00:11:19.586 --> 00:11:24.151
In terms of, you need to be able to
make predictions for every edge case.

180
00:11:24.151 --> 00:11:27.681
It doesn't change the fundamental
model implementation,

181
00:11:27.681 --> 00:11:31.348
but you do need to then ensure
you find conversions that capture

182
00:11:31.348 --> 00:11:35.448
every damn piece across a larger data set,
a larger set of edge cases.

183
00:11:35.448 --> 00:11:37.758
So, maybe a million refunds.

184
00:11:37.758 --> 00:11:39.123
So we'd expand this to,

185
00:11:39.123 --> 00:11:42.379
not a good sign when I'm not
being right on the whiteboard.

186
00:11:42.379 --> 00:11:45.212
Expand this to make maybe a million.

187
00:11:45.212 --> 00:11:49.738
We'd use techniques to come up with
the converter at scale, like extreme

188
00:11:49.738 --> 00:11:54.272
gradient boosting within decision trees,
developed both a principle.

189
00:11:54.272 --> 00:12:00.101
Only again, this stuff is so new,
it was developed by somebody who I think,

190
00:12:00.101 --> 00:12:02.248
Chen, I think that's right.

191
00:12:02.248 --> 00:12:06.647
An engineer or a researcher then at,
I wanna say Washington,

192
00:12:06.647 --> 00:12:10.464
U Washington, and now,
I think at Carnegie Mellon.

193
00:12:10.464 --> 00:12:14.963
This is one of the most popular,
both techniques and

194
00:12:14.963 --> 00:12:20.486
then implemented libraries for
handling, developing the best

195
00:12:20.486 --> 00:12:26.124
decision tree at scale,
the optimal set of boundaries at scale.

196
00:12:26.124 --> 00:12:32.760
Introducing a paper in, I think 2016,
this stuff is always evolving like now.

197
00:12:32.760 --> 00:12:34.824
And yet now it's the standard for

198
00:12:34.824 --> 00:12:39.258
working on large data sets where you
wanna get decision trees at scale.

199
00:12:39.258 --> 00:12:41.963
Where you're dealing with
a million refund requests.

200
00:12:41.963 --> 00:12:46.775
Where you wanna find across many,
many parameter values, the optimal,

201
00:12:46.775 --> 00:12:49.038
the best set of parameter values.

202
00:12:49.038 --> 00:12:52.228
So we could use things, I'd say,

203
00:12:52.228 --> 00:12:57.189
like XGBoost to come up with
our our best boundaries.

204
00:12:57.189 --> 00:13:01.432
End-to-end deployment, all of the stuff
we did here needs to be put on

205
00:13:01.432 --> 00:13:05.337
the Internet so that it can be
part of your company's workflows.

206
00:13:05.337 --> 00:13:06.605
So for example,

207
00:13:06.605 --> 00:13:12.501
our converter here that we're gonna use
on pool will build an API likely for.

208
00:13:12.501 --> 00:13:16.774
And I can't squeeze it in there, but
we would have, let's put it here.

209
00:13:16.774 --> 00:13:21.504
We would have some, I don't know, API,

210
00:13:21.504 --> 00:13:26.913
let's say doordash.com/models/refund,

211
00:13:26.913 --> 00:13:32.725
that would expect years,
it would expect dollar,

212
00:13:32.725 --> 00:13:36.645
look at this object, it's very,

213
00:13:36.645 --> 00:13:40.721
it's JSON, it's very accurate.

214
00:13:40.721 --> 00:13:43.330
And it would expect accounts on IP.

215
00:13:43.330 --> 00:13:49.394
And then the return might be 1,
give a refund, or -1, don't.

216
00:13:49.394 --> 00:13:54.450
We'll deploy this model, and
the deployed trained, shit,

217
00:13:54.450 --> 00:13:57.277
we didn't use the term training.

218
00:13:57.277 --> 00:14:00.610
Dear.
All right, bonus, the stage that we

219
00:14:00.610 --> 00:14:05.412
used our sample to come up with our model,
our converter,

220
00:14:05.412 --> 00:14:11.391
the right boundaries for our sample,
what's that stage called, the?

221
00:14:11.391 --> 00:14:12.212
&gt;&gt; Speaker 3: Training.

222
00:14:12.212 --> 00:14:16.335
&gt;&gt; Will Sentance: Yeah, okay, good.
Yeah, training, that fancy word.

223
00:14:16.335 --> 00:14:19.596
Raise your hand if you've heard that
word in, yeah, that's all it is,

224
00:14:19.596 --> 00:14:21.796
using our sample to come
up with our conversion.

225
00:14:21.796 --> 00:14:22.993
That's our training stage.

226
00:14:22.993 --> 00:14:27.919
When we were training our model and
then validating it

227
00:14:27.919 --> 00:14:33.064
with our sample data, yeah,
we came up with our model,

228
00:14:33.064 --> 00:14:38.880
which we're then using to make
a prediction inference stage.

229
00:14:38.880 --> 00:14:41.723
Yeah, the model we trained needs to,
yeah, yeah, sorry,

230
00:14:41.723 --> 00:14:43.707
I was wondering why I
kept the word training.

231
00:14:43.707 --> 00:14:49.102
The trained model is this.

232
00:14:49.102 --> 00:14:51.968
The best boundaries for that sample.

233
00:14:51.968 --> 00:14:54.181
That's the trained model.

234
00:14:54.181 --> 00:14:56.364
That's it.
The deployed trained model is,

235
00:14:56.364 --> 00:14:59.603
[LAUGH] take in, and
that's why a trained model,

236
00:14:59.603 --> 00:15:04.511
by the way, can often be stored as
a Python object, a Python dictionary.

237
00:15:04.511 --> 00:15:07.585
Because it's just numbers.

238
00:15:07.585 --> 00:15:11.728
A trained model, yeah,

239
00:15:11.728 --> 00:15:19.433
it's wonderfully touchable set of numbers.

240
00:15:19.433 --> 00:15:21.253
Set of boundaries in
the case of decision tree.

241
00:15:21.253 --> 00:15:27.929
So when we deploy it, we're saying,
take in an input and return out an output.

242
00:15:27.929 --> 00:15:30.542
That's our deployed model.

243
00:15:30.542 --> 00:15:34.613
And then finally, a crucial part,
we already touched on it,

244
00:15:34.613 --> 00:15:36.975
ongoing performance of the model.

245
00:15:36.975 --> 00:15:41.891
Checking when Paul comes along and

246
00:15:41.891 --> 00:15:47.152
tests the limits of the prediction.

247
00:15:47.152 --> 00:15:51.555
Not only bad actor, but he's clever,

248
00:15:51.555 --> 00:15:56.230
smart, brilliant, boundary testing.

249
00:15:56.230 --> 00:16:03.517
Paul is great information to refine
the model, to improve the sample,

250
00:16:03.517 --> 00:16:09.839
which has now got out of kilter
with the population as a whole.

251
00:16:09.839 --> 00:16:14.235
The population now has people like Paul
that are not captured in our sample, and

252
00:16:14.235 --> 00:16:15.949
in therefor, our converter.

253
00:16:15.949 --> 00:16:22.033
So to remove and reduce that disconnect or
drift, we can update the sample with,

254
00:16:22.033 --> 00:16:26.596
let's say, somebody is regularly
checking for outliers,

255
00:16:26.596 --> 00:16:31.442
like the number of IPs,
number of counts on the IP that Pull has.

256
00:16:31.442 --> 00:16:33.980
And are correcting.

257
00:16:33.980 --> 00:16:39.990
So the refund was given,
the correction was, do not give.

258
00:16:39.990 --> 00:16:43.739
And that is then reintegrated
back into the sample.

259
00:16:43.739 --> 00:16:49.375
And now we have 1 million plus,
let's say 1,000 of those a week.

260
00:16:49.375 --> 00:16:53.243
Are we going to then take 1 million and
1,000 and redevelop our conversion

261
00:16:53.243 --> 00:16:56.660
boundaries from scratch trying out
all of the different boundaries?

262
00:16:56.660 --> 00:17:01.456
Well, actually, for decision tree,
there are fewer techniques to tune, but

263
00:17:01.456 --> 00:17:05.609
there are still techniques to do
what's called tune our converter,

264
00:17:05.609 --> 00:17:09.633
adjust our model without a full
retraining with all of our sample.

265
00:17:09.633 --> 00:17:14.786
That tuning stage of adding in
new data that tweaks the model,

266
00:17:14.786 --> 00:17:20.037
that makes the converter values work for
not just this sample,

267
00:17:20.037 --> 00:17:26.301
not just our million sample, but
also the updated corrected pool refund.

268
00:17:26.301 --> 00:17:30.997
That's called tuning or
avoiding retraining, or

269
00:17:30.997 --> 00:17:36.785
optimizing the retraining of our
model to prevent that drift,

270
00:17:36.785 --> 00:17:42.044
that disconnect between the sample and
the population.

