WEBVTT

1
00:00:00.070 --> 00:00:01.420
So let's just hop into it.

2
00:00:01.420 --> 00:00:04.290
So what I'm gonna do is I'm gonna
make a new file in the utils folder.

3
00:00:04.290 --> 00:00:06.378
I'm gonna call it AI.

4
00:00:09.525 --> 00:00:12.120
So there's a lot I can talk
about with the AI stuff.

5
00:00:12.120 --> 00:00:18.930
So I'm gonna give a primer on some of
the technologies that we'll be using.

6
00:00:18.930 --> 00:00:23.944
I'll give a demo of how to do a very basic
example of some of these technologies and

7
00:00:23.944 --> 00:00:27.634
then we'll actually implement
what we just talked about,

8
00:00:27.634 --> 00:00:31.060
that way you have some context
of what what is going on.

9
00:00:31.060 --> 00:00:33.449
Okay, anyone here knows what an LLM is?

10
00:00:34.490 --> 00:00:35.730
All right, anyone wanna explain that?

11
00:00:37.100 --> 00:00:40.035
&gt;&gt; Just like a basic level
language learning model.

12
00:00:40.035 --> 00:00:40.604
It's like a.

13
00:00:40.604 --> 00:00:41.680
It's like an algorithm.

14
00:00:41.680 --> 00:00:48.290
It's heuristics, trains itself on datasets
and learns, selects best responses.

15
00:00:48.290 --> 00:00:52.483
So it needs some sort of input
to actually work on, and

16
00:00:52.483 --> 00:00:57.440
then it also needs input to shape
how it weights its responses.

17
00:00:58.550 --> 00:00:59.880
&gt;&gt; Yeah, that's pretty accurate.

18
00:00:59.880 --> 00:01:02.900
So LLM stands for large language model.

19
00:01:02.900 --> 00:01:06.570
But what you just said
also makes sense too.

20
00:01:06.570 --> 00:01:09.180
But yeah, everything else,
pretty accurate.

21
00:01:09.180 --> 00:01:14.390
So there was this whitepaper that came
out not too long ago called Transformers.

22
00:01:14.390 --> 00:01:17.950
Well, it was on a topic of
something called Transformers.

23
00:01:17.950 --> 00:01:22.339
The best way I can describe Transformers
is it's a different architecture for

24
00:01:22.339 --> 00:01:23.412
neural networks.

25
00:01:23.412 --> 00:01:25.950
If you don't know what a neural
network is, that's totally fine.

26
00:01:25.950 --> 00:01:28.550
It's actually a really great exercise
to go through and build one yourself.

27
00:01:28.550 --> 00:01:30.130
It sounds scary, but it's actually not.

28
00:01:30.130 --> 00:01:33.598
You can go watch YouTube video on building
your own neural network from scratch in

29
00:01:33.598 --> 00:01:35.650
JavaScript, and
it's a really good example.

30
00:01:35.650 --> 00:01:41.127
But basically, a neural network is like
our best guess of how our brains work and

31
00:01:41.127 --> 00:01:43.190
then converted that to code.

32
00:01:43.190 --> 00:01:46.977
It's something that can take in inputs and
then, through training,

33
00:01:46.977 --> 00:01:49.880
can do some type of prediction on and
output, right?

34
00:01:49.880 --> 00:01:54.753
In this case, these neural networks
that are inside these LLMs can

35
00:01:54.753 --> 00:01:59.713
take in something like a prompt,
like an input, like you said, and

36
00:01:59.713 --> 00:02:03.716
then output something that sounds,
not even truthy,

37
00:02:03.716 --> 00:02:07.310
something that sounds like
a human would say it.

38
00:02:07.310 --> 00:02:11.183
And that's because these neural networks
are trained on the entire Internet,

39
00:02:11.183 --> 00:02:13.590
at least the one that
we're gonna be using.

40
00:02:13.590 --> 00:02:15.620
It's literally trained
on the entire Internet.

41
00:02:15.620 --> 00:02:20.748
So everything that is publicly
available on the Internet, it has seen,

42
00:02:20.748 --> 00:02:26.970
so it can make plausible predictions based
off what it thinks the next word might be.

43
00:02:26.970 --> 00:02:32.169
So Transformers, that is a different
type of neural network architecture that

44
00:02:32.169 --> 00:02:36.982
allows the neural network to basically
observe an entire sentence versus

45
00:02:36.982 --> 00:02:42.100
before they could only look at the
previous word to predict the next word.

46
00:02:42.100 --> 00:02:45.940
Now you can look at the entire
input to predict the next word, and

47
00:02:45.940 --> 00:02:51.080
it's somewhat recursive, and that allows
it to be way better at predicting things.

48
00:02:51.080 --> 00:02:55.232
And that's kinda the new advancement that
was created that's enabled all this stuff

49
00:02:55.232 --> 00:02:56.680
with generative AI, right?

50
00:02:56.680 --> 00:02:59.506
It's really just that paper
called Transformers, and

51
00:02:59.506 --> 00:03:02.220
I think the paper is called
Attention Is All You Need.

52
00:03:03.950 --> 00:03:07.080
Attention is All, or something like that.

53
00:03:07.080 --> 00:03:07.580
&gt;&gt; That's it, yeah.

54
00:03:08.780 --> 00:03:10.430
&gt;&gt; Yeah, Attention Is All You Need, yep.

55
00:03:10.430 --> 00:03:14.847
So if you read this, yeah,
if you read that whitepaper,

56
00:03:14.847 --> 00:03:21.300
you'll either probably be really confused,
which I was a million times.

57
00:03:21.300 --> 00:03:24.891
I actually fed the whitepaper to
the AI and asked it questions like,

58
00:03:24.891 --> 00:03:26.340
can you explain this to me?

59
00:03:26.340 --> 00:03:28.110
So, yeah,
highly recommend checking that out.

60
00:03:28.110 --> 00:03:29.940
But anyway,
I wanted to give you that primer.

61
00:03:29.940 --> 00:03:34.044
I think it's just giving you enough
that if you wanna look that up and

62
00:03:34.044 --> 00:03:36.500
learn more about it, you can look it up.

63
00:03:36.500 --> 00:03:38.665
Okay, so now that you know LLM is,

64
00:03:38.665 --> 00:03:42.177
some of the bigger ones out
there are the one created by,

65
00:03:42.177 --> 00:03:47.650
you probably heard of this one,
created by OpenAI, and it's called GPT.

66
00:03:47.650 --> 00:03:52.913
So GPT stands for, The T in GPT stands for

67
00:03:52.913 --> 00:03:56.310
transformer, so
it's a generative pre-trained transformer.

68
00:03:56.310 --> 00:04:00.256
That's literally all it is,
it's actually quite powerful.

69
00:04:00.256 --> 00:04:02.610
I don't know why I'm just
putting it down like that.

70
00:04:02.610 --> 00:04:07.740
But yeah, we're gonna be using
GPT which is a hosted LLM.

71
00:04:07.740 --> 00:04:11.085
We don't have to,
bringing that down to our computer,

72
00:04:11.085 --> 00:04:15.720
if you wanted to run an LLM on your
machine, you would need a GPU, basically.

73
00:04:17.390 --> 00:04:21.350
Most Macs could probably run a decent LLM,
the problem is you're not gonna get

74
00:04:21.350 --> 00:04:24.600
something as big and
powerful as GPT on your machine.

75
00:04:24.600 --> 00:04:28.428
But there are tons of other
open source LLMs out there that

76
00:04:28.428 --> 00:04:32.030
are single-purposely built
to do specific things.

77
00:04:32.030 --> 00:04:37.210
GPT is kinda like the Walmart of LLM,
so it kinda just does everything.

78
00:04:38.620 --> 00:04:40.510
And it's massive, it's huge.

79
00:04:40.510 --> 00:04:44.104
But if you want arts and crafts, you could
go to Walmart, but you could go to arts

80
00:04:44.104 --> 00:04:47.880
and crafts store that's a little smaller
and has the thing you specifically need.

81
00:04:47.880 --> 00:04:49.620
So, there's tons of these coming out.

82
00:04:49.620 --> 00:04:50.830
This is the one that we're gonna use.

83
00:04:50.830 --> 00:04:51.760
It's just an API.

84
00:04:51.760 --> 00:04:54.860
You don't need to know anything about
it other than how to make an API call.

85
00:04:56.060 --> 00:05:02.377
Okay, now the other thing is, I don't
know if you had any experience with AI,

86
00:05:02.377 --> 00:05:06.850
but you have to interface with a prompt,
right?

87
00:05:06.850 --> 00:05:11.381
And a prompt is basically,
you can think of it as a command or

88
00:05:11.381 --> 00:05:16.457
an ask or a question, but
it also comes down to trying to understand

89
00:05:16.457 --> 00:05:21.647
the limitations of something like GPT and
what it can and cannot do.

90
00:05:21.647 --> 00:05:25.661
And you have to figure out how to steer
it in a direction to get the results that

91
00:05:25.661 --> 00:05:26.950
you want.

92
00:05:26.950 --> 00:05:28.530
And that's a whole job on its own.

93
00:05:28.530 --> 00:05:31.672
It's prompt engineering,
something that I recommend everyone

94
00:05:31.672 --> 00:05:34.080
at least taking a course
on if you're technical.

95
00:05:34.080 --> 00:05:36.900
So, we will be talking about some of that.

96
00:05:36.900 --> 00:05:40.035
And then the last thing, so
we can finally get into the code,

97
00:05:40.035 --> 00:05:41.990
is something called LangChain.

98
00:05:41.990 --> 00:05:43.940
So, we'll be using this a lot.

99
00:05:43.940 --> 00:05:48.621
So LangChain is,
it's pretty much the number

100
00:05:48.621 --> 00:05:53.850
one framework right now for
using AI or LLMs.

101
00:05:53.850 --> 00:05:57.100
It's like an SDK for LLMs, basically.

102
00:05:57.100 --> 00:06:01.359
It's mostly built in Python, we won't
be using a Python version because they

103
00:06:01.359 --> 00:06:05.359
also have a JavaScript version, and
that is pretty much in feature parody

104
00:06:05.359 --> 00:06:08.350
with the Python version,
at least for what we need.

105
00:06:08.350 --> 00:06:10.275
There are like some things that
are in Python that aren't in

106
00:06:10.275 --> 00:06:12.040
the JavaScript version.

107
00:06:12.040 --> 00:06:14.720
But it's like really advanced
bleeding edge stuff.

108
00:06:14.720 --> 00:06:15.930
So we don't really care about it.

109
00:06:17.240 --> 00:06:18.300
We'll be using this.

110
00:06:18.300 --> 00:06:22.050
It's js.langchain.com.

111
00:06:22.050 --> 00:06:25.397
And basically the LangChain,
like I said, it's just an SDK,

112
00:06:25.397 --> 00:06:27.640
it interface with pretty much every LLM.

113
00:06:27.640 --> 00:06:30.070
We'll be using an interface with OpenAI.

114
00:06:30.070 --> 00:06:32.000
It's a quite sophisticated product.

115
00:06:32.000 --> 00:06:34.699
You can do some pretty interesting stuff,
as you'll soon find out.

116
00:06:35.880 --> 00:06:38.560
And yeah, any questions on all that stuff?

117
00:06:38.560 --> 00:06:40.522
And that was so high level.

118
00:06:40.522 --> 00:06:44.018
This was basically like
standing on the USS,

119
00:06:44.018 --> 00:06:48.993
the International Space Station,
and looking down at a penny.

120
00:06:48.993 --> 00:06:50.760
That's how high level this was.

121
00:06:50.760 --> 00:06:52.734
So [LAUGH] I glanced over a lot of stuff,

122
00:06:52.734 --> 00:06:55.030
but I just wanted to
make sure you have that.

