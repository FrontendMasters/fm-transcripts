WEBVTT

1
00:00:00.031 --> 00:00:03.188
&gt;&gt; Richard Feldman: So if we go back and
apply this concept to Hello, World,

2
00:00:03.188 --> 00:00:07.896
one of the things that might surprise you
is that when we're doing this write call,

3
00:00:07.896 --> 00:00:11.691
and we're passing in 1,
that was actually a 32-bit integer.

4
00:00:11.691 --> 00:00:14.397
So that was 32 ones and
zeros, which was to say,

5
00:00:14.397 --> 00:00:17.050
the bottom one that we had
on the previous slide.

6
00:00:17.050 --> 00:00:20.166
That basically means that if you want to,

7
00:00:20.166 --> 00:00:24.810
you can specify up to 2 to the 32
different numbers in that.

8
00:00:24.810 --> 00:00:28.880
And that gets you up to things like
4 billion or something like that.

9
00:00:28.880 --> 00:00:32.819
Is the highest number you can
express in a 32-bit integer.

10
00:00:32.819 --> 00:00:34.373
If you wanna go higher than that,

11
00:00:34.373 --> 00:00:36.990
you would need to actually
reserve more bits in memory.

12
00:00:36.990 --> 00:00:38.819
And if they wanted,
they could have said, well,

13
00:00:38.819 --> 00:00:40.485
we're gonna have a lower
number of options.

14
00:00:40.485 --> 00:00:43.709
And rather than it being a 32-bit
integer we'll save some memory, and

15
00:00:43.709 --> 00:00:47.567
say you only get a 16-bit integer here,
and then you only get up to 65,000 max.

16
00:00:47.567 --> 00:00:51.003
Or you could say it's only
an 8-bit integer, a byte, and

17
00:00:51.003 --> 00:00:52.530
then you only get 256.

18
00:00:52.530 --> 00:00:57.385
So it's really up to the designer of
the particular function to decide how

19
00:00:57.385 --> 00:01:00.139
big the integers they wanna support are.

20
00:01:00.139 --> 00:01:04.544
So yeah, this is literally what's
happening in memory, when we were seeing

21
00:01:04.544 --> 00:01:09.290
the assembly instructions that were moving
the data into here it's literally this

22
00:01:09.290 --> 00:01:12.906
many ones and zeros in memory
that's getting copied in there.

23
00:01:12.906 --> 00:01:15.867
Now this right here,
if we want to just look at in memory,

24
00:01:15.867 --> 00:01:18.657
what are the ones and
zeros that correspond to Hello?

25
00:01:18.657 --> 00:01:22.321
These are the actual ones and
zeros in memory that correspond to Hello.

26
00:01:22.321 --> 00:01:27.327
They are the bytes 48 which
corresponds to H, 65 is e,

27
00:01:27.327 --> 00:01:32.042
108 is l, which is repeated,
and then 111 is o.

28
00:01:32.042 --> 00:01:35.507
And this is basically a mapping that,
again, we've just decided upon.

29
00:01:35.507 --> 00:01:36.515
We just made this up.

30
00:01:36.515 --> 00:01:38.574
We just decided that when
you see these bit patterns,

31
00:01:38.574 --> 00:01:40.372
this is what should get
printed on the screen.

32
00:01:40.372 --> 00:01:43.751
It's an entirely,
outside the hardware concept,

33
00:01:43.751 --> 00:01:46.685
the CPU has no idea what
a string encoding is.

34
00:01:46.685 --> 00:01:51.376
It's just, yeah, I know about ones and
zeros, and we've just decided that,

35
00:01:51.376 --> 00:01:56.137
yeah, when you do this, this is what
your teletype, I mean, what your screen,

36
00:01:56.137 --> 00:02:00.576
should print out is these lines and
drawings that we recognize as letters.

37
00:02:00.576 --> 00:02:02.271
So again,
we decide how to interpret the memory.

38
00:02:02.271 --> 00:02:04.757
This is all about
the hardware doesn't care.

39
00:02:04.757 --> 00:02:10.988
This is just we have decided to ascribe
certain meaning to these ones and zeros.

40
00:02:10.988 --> 00:02:15.161
This particular encoding happens to be
ASCII, which is the American standard for

41
00:02:15.161 --> 00:02:18.499
C, I actually don't remember
[LAUGH] Exactly what it stands for.

42
00:02:18.499 --> 00:02:22.385
But it's basically a character encoding
that's just a way of saying, here's how

43
00:02:22.385 --> 00:02:25.955
we're going to map these ones and
zeros to actual letters in that language.

44
00:02:25.955 --> 00:02:30.423
Original ASCII did use,
in fact, 1-byte per character.

45
00:02:30.423 --> 00:02:35.895
And actually, fortunately,
it only used 7 of the 8 bits.

46
00:02:35.895 --> 00:02:39.060
Notice every single one of these starts
with a 0, and that's not a coincidence.

47
00:02:39.060 --> 00:02:40.472
In ASCII, that's just always true.

48
00:02:40.472 --> 00:02:44.609
ASCII always has the very first bit
set to be 0 just because they only

49
00:02:44.609 --> 00:02:49.123
use 7 out of the 8 bits to represent
all of the letters they wanted to use.

50
00:02:49.123 --> 00:02:53.513
Now, as it turns out, American English
is not all the languages in the world.

51
00:02:53.513 --> 00:02:55.336
There's actually others.

52
00:02:55.336 --> 00:02:57.636
And so yeah, this is where we got Unicode.

53
00:02:57.636 --> 00:03:00.667
And fortunately, there was this really,
really clever encoding.

54
00:03:00.667 --> 00:03:03.979
And it's required that if you ever
introduce someone to the concept of UTF-8,

55
00:03:03.979 --> 00:03:05.601
you have to explain how clever this is.

56
00:03:05.601 --> 00:03:10.542
But they managed to mix an encoding that
was backwards and compatible with ASCII,

57
00:03:10.542 --> 00:03:14.479
which is basically if we were to
look at Unicode characters here,

58
00:03:14.479 --> 00:03:18.938
this stuff that we have right here,
this is both ASCII and valid UTF-8.

59
00:03:18.938 --> 00:03:23.502
Because UTF-8 cleverly said for
all of the first 256 characters that we

60
00:03:23.502 --> 00:03:28.506
support in UTF-8, we're gonna choose to
have them be exactly the same as ASCII.

61
00:03:28.506 --> 00:03:30.640
So UTF-8 is totally backwards
compatible with ASCII.

62
00:03:30.640 --> 00:03:34.748
If you have some program that was using
ASCII for its strings in the 1970s and you

63
00:03:34.748 --> 00:03:38.855
decide to upgrade UTF-8, all of that code
will still just completely work as is,

64
00:03:38.855 --> 00:03:42.572
because UTF-8 represents those strings
exactly the same way in memory.

65
00:03:42.572 --> 00:03:46.806
What they did was they said, we're gonna
use the fact that that first bit was

66
00:03:46.806 --> 00:03:51.524
unused in ASCII and say, if we set that
first bit to a 1, now we're in UTF-8 land.

67
00:03:51.524 --> 00:03:55.886
And that 1 indicates, okay, now we're
gonna shift gears and get into, okay,

68
00:03:55.886 --> 00:03:58.143
now we're gonna do fancy Unicode stuff.

69
00:03:58.143 --> 00:04:00.517
And all the old code
wasn't using that anyway.

70
00:04:00.517 --> 00:04:02.791
None of the ASCII stuff ever
actually had a 1 there.

71
00:04:02.791 --> 00:04:05.441
So it was totally fine to do that,
because that was sort of unused.

72
00:04:05.441 --> 00:04:09.823
And so that first digit in ASCII is
unused, but in Unicode, sorry, in UTF-8,

73
00:04:09.823 --> 00:04:13.188
it's a flag saying,
are we doing fancy Unicode stuff or not?

74
00:04:13.188 --> 00:04:17.356
And then the other 7 bits are interpreted
differently depending on whether that

75
00:04:17.356 --> 00:04:18.344
first bit was set.

76
00:04:18.344 --> 00:04:21.850
And one of the things that UTF-8
allows is, again, we decide how

77
00:04:21.850 --> 00:04:25.857
to interpret memory, is sometimes
depending on what the other 7 bits are,

78
00:04:25.857 --> 00:04:30.258
UTF-8 might say, you know what, this
letter actually takes up multiple bytes.

79
00:04:30.258 --> 00:04:33.068
We're gonna take this one, and
also we're gonna use this one, and

80
00:04:33.068 --> 00:04:35.002
possibly also this one,
maybe even this one.

81
00:04:35.002 --> 00:04:40.138
So that's one of the things that
they use to be able to expand

82
00:04:40.138 --> 00:04:45.610
how many different character sets
you can encode in your text.

83
00:04:45.610 --> 00:04:49.935
Okay, and then the final argument of write
is a 64-bit integer, meaning 64 ones and

84
00:04:49.935 --> 00:04:50.809
zeros in memory.

85
00:04:50.809 --> 00:04:54.474
So that means that you can have
a string that is very long here,

86
00:04:54.474 --> 00:04:58.723
not just 32-bits, we said that goes
up to a string about 4 billion.

87
00:04:58.723 --> 00:05:01.157
So you if this were a 32-bit integer,

88
00:05:01.157 --> 00:05:04.963
you could only give write strings
that were up to 4 gigabytes.

89
00:05:04.963 --> 00:05:07.497
And if it's an unsigned integer,
only 2 gigabytes.

90
00:05:07.497 --> 00:05:10.672
If you've ever used Java,
Java came out in 1995 and

91
00:05:10.672 --> 00:05:14.469
they made the decision to use
32-bit integers for everything.

92
00:05:14.469 --> 00:05:18.397
And as a consequence of this, you can find
stack overflow posts and stuff talking

93
00:05:18.397 --> 00:05:22.151
about hey, I'm trying to ingest this
CSV that's more than 2 gigabytes, or

94
00:05:22.151 --> 00:05:25.270
maybe more than 4 gigabytes,
whatever, 5 gigabyte CSV, and

95
00:05:25.270 --> 00:05:26.757
Java just won't let me do it.

96
00:05:26.757 --> 00:05:30.022
And the reason for that is that
Java's arrays are 32-bit integers.

97
00:05:30.022 --> 00:05:33.557
So they just literally cannot
conceptualize something that's

98
00:05:33.557 --> 00:05:34.609
longer than that.

99
00:05:34.609 --> 00:05:38.765
You have to break it up into multiple
arrays and kind of work around that.

100
00:05:38.765 --> 00:05:40.671
Whereas in C,
you can have multiple different sizes,

101
00:05:40.671 --> 00:05:41.838
including 64-bit integers.

102
00:05:41.838 --> 00:05:47.217
So 64-bit means you can have,
I think it's 9 undecillion.

103
00:05:47.217 --> 00:05:50.923
I had to learn the word undecillion
in order to learn how big

104
00:05:50.923 --> 00:05:53.733
64-bit integers can represent things.

105
00:05:53.733 --> 00:05:55.338
It's more than gigabytes,
I'll tell you that.

106
00:05:55.338 --> 00:05:56.650
[LAUGH] So yeah.

107
00:05:56.650 --> 00:06:00.651
We actually don't currently have any piece
of hardware in the world that can max out

108
00:06:00.651 --> 00:06:04.095
a 64-bit integer worth of memory,
nothing supports that that big.

109
00:06:04.095 --> 00:06:05.683
Maybe someday they will.

110
00:06:05.683 --> 00:06:09.764
But, yeah, it's gonna be quite a long time
before we feel, we really gotta bump it up

111
00:06:09.764 --> 00:06:12.906
to 128-bit integers to really
maximally support these things.

112
00:06:12.906 --> 00:06:14.393
Yeah?

113
00:06:14.393 --> 00:06:20.177
&gt;&gt; Speaker 2: Where in the translation or
where does the translation for

114
00:06:20.177 --> 00:06:22.902
UTF-8 actually happen?

115
00:06:22.902 --> 00:06:28.107
Is that on the operating system level or
where?

116
00:06:28.107 --> 00:06:28.641
&gt;&gt; Richard Feldman: That's
a great question.

117
00:06:28.641 --> 00:06:32.362
So the question was, where does
the translation for UTF-8 actually happen?

118
00:06:32.362 --> 00:06:36.160
It actually depends on
what's doing the displaying.

119
00:06:36.160 --> 00:06:39.354
So for example, when we're running at the
Command line, it's actually your terminal

120
00:06:39.354 --> 00:06:42.219
application, or more specifically,
the shell that's interpreting that.

121
00:06:42.219 --> 00:06:45.993
So when you say, write to standard out,
you're basically just, here's some bytes,

122
00:06:45.993 --> 00:06:46.864
have fun with that.

123
00:06:46.864 --> 00:06:50.032
And then it's up to some combination of
the shell and the terminal to read those

124
00:06:50.032 --> 00:06:53.642
bytes and be, I'm deciding to interpret
these bytes that I'm getting as UTF-8.

125
00:06:53.642 --> 00:06:55.986
I'm gonna render them accordingly.

126
00:06:55.986 --> 00:06:57.714
I don't know if this is still supported,
but

127
00:06:57.714 --> 00:07:00.726
certainly some older shells would give
you a drop down for how to interpret it.

128
00:07:00.726 --> 00:07:03.931
So, for example, before Unicode
sort of emerged as the standard,

129
00:07:03.931 --> 00:07:05.909
you would have different localization.

130
00:07:05.909 --> 00:07:07.861
So for example, if you were in Japan,

131
00:07:07.861 --> 00:07:11.715
you would have a terminal it was
interpreting the same bytes differently.

132
00:07:11.715 --> 00:07:14.431
And you could select from a drop down,

133
00:07:14.431 --> 00:07:17.533
this is the Japanese
encoding of of these bytes.

134
00:07:17.533 --> 00:07:21.383
I have a friend who was into
anime back in the 90s, and

135
00:07:21.383 --> 00:07:26.343
he would get these video or
audio files that were encoded in Japanese.

136
00:07:26.343 --> 00:07:31.205
And literally, his software that tried
to play them would crash because it was

137
00:07:31.205 --> 00:07:34.678
trying to display these
Japanese encoded characters.

138
00:07:34.678 --> 00:07:38.307
They would get the bytes off of the disc,
and it's, here's some bytes for you.

139
00:07:38.307 --> 00:07:42.288
And his program, the music player or
whatever, would try to display them and

140
00:07:42.288 --> 00:07:46.527
it would crash because it was trying to
interpret them as ASCII or something else.

141
00:07:46.527 --> 00:07:50.828
And they just didn't fit what it was
expecting and some edge case happened and

142
00:07:50.828 --> 00:07:52.026
it would just crash.

143
00:07:52.026 --> 00:07:54.735
So yeah, the short answer is it's
not the operating system, but

144
00:07:54.735 --> 00:07:57.249
rather some program running on
the operating system that's

145
00:07:57.249 --> 00:08:00.321
actually in charge of translating
these bytes into pixels on the screen.

146
00:08:00.321 --> 00:08:05.118
Whoever his job is to do that, somewhere
in that thing, probably the shell, or

147
00:08:05.118 --> 00:08:07.923
it could just be your UI
program is doing that.

148
00:08:07.923 --> 00:08:12.380
Also worth noting that in Windows, it's
actually super common not to use UTF-8,

149
00:08:12.380 --> 00:08:13.959
but rather to use UTF-16.

150
00:08:13.959 --> 00:08:17.629
And also in the browser,
it's super common to use UTF-16.

151
00:08:17.629 --> 00:08:21.926
We're gonna use UTF-8 here because
when you're doing HTTP, HTTP is UTF-8,

152
00:08:21.926 --> 00:08:23.433
that's what we're doing.

153
00:08:23.433 --> 00:08:26.358
And also UTF-8 has definitely kind of
emerged as the most popular standard

154
00:08:26.358 --> 00:08:26.865
these days.

155
00:08:26.865 --> 00:08:27.718
These days,

156
00:08:27.718 --> 00:08:32.766
people tend to view UTF-16 as sort of
a maybe kind of a historical mistake.

157
00:08:32.766 --> 00:08:34.721
It's sort of being phased out.

158
00:08:34.721 --> 00:08:40.150
But along the way, Java use UTF-16,
JavaScript use UTF-16 behind the scenes

159
00:08:40.150 --> 00:08:45.669
in the browser, and all the Microsoft
Windows APIs are UTF-16, at least for now.

160
00:08:45.669 --> 00:08:48.981
Now you might be surprised to learn that
the write function actually takes three

161
00:08:48.981 --> 00:08:49.481
integers.

162
00:08:49.481 --> 00:08:52.674
Now that might be strange because it
looks like it takes an integer and

163
00:08:52.674 --> 00:08:56.650
then a string and then an integer, but for
real, it really does take three integers.

164
00:08:56.650 --> 00:09:01.193
And that's because what's actually being
passed to write is an integer namely

165
00:09:01.193 --> 00:09:05.283
the memory address of the first byte
in the string in the actual binary.

166
00:09:05.283 --> 00:09:08.136
So back when we were talking
about the assembly stuff,

167
00:09:08.136 --> 00:09:12.354
we talked about how there was that LC0
section that corresponds to there's some

168
00:09:12.354 --> 00:09:14.479
bytes in your final binary executable.

169
00:09:14.479 --> 00:09:18.877
And one of the things the C compiler will
do is everywhere you see this string,

170
00:09:18.877 --> 00:09:21.867
it's gonna substitute
the memory address of that.

171
00:09:21.867 --> 00:09:25.276
So essentially, what's happening
here is that these ones and

172
00:09:25.276 --> 00:09:27.990
zeros, these exist at some
point in your binary.

173
00:09:27.990 --> 00:09:30.991
And this right here is
the integer of this first one.

174
00:09:30.991 --> 00:09:34.914
It's this 0 right here, this very
beginning of this section of memory,

175
00:09:34.914 --> 00:09:38.539
I just made this number up,
I don't know what it actually would be.

176
00:09:38.539 --> 00:09:41.916
But it's some whatever,
wherever in your binary this thing starts,

177
00:09:41.916 --> 00:09:44.333
that's what the C compiler
is gonna insert here.

178
00:09:44.333 --> 00:09:46.917
And when you get down to the actual
assembly, when you call write,

179
00:09:46.917 --> 00:09:49.286
you're passing an integer for
this, a 32-bit integer.

180
00:09:49.286 --> 00:09:52.564
This is gonna be a 64-bit integer, and
this is gonna be another 64-bit integer.

181
00:09:52.564 --> 00:09:55.638
That's what's actually getting
passed to that function.

182
00:09:55.638 --> 00:09:59.092
Each of these you can think
of as essentially memory is

183
00:09:59.092 --> 00:10:01.333
basically a giant array of bytes.

184
00:10:01.333 --> 00:10:04.399
So the address of this
one is ending in a 2.

185
00:10:04.399 --> 00:10:06.467
This one is the same thing ending in a 3.

186
00:10:06.467 --> 00:10:08.232
This one's ending in a 4.

187
00:10:08.232 --> 00:10:12.800
That's the address of each of these,
essentially an index into that gigantic

188
00:10:12.800 --> 00:10:17.434
array that you can conceptually think of
as the memory of the running process and

189
00:10:17.434 --> 00:10:18.558
so on and so forth.

190
00:10:18.558 --> 00:10:22.111
So basically, I think this is a really
good way to think about memory,

191
00:10:22.111 --> 00:10:24.656
is that it is basically
a gigantic array of bytes.

192
00:10:24.656 --> 00:10:28.338
And then we can choose to interpret that
gigantic array of bytes in different ways,

193
00:10:28.338 --> 00:10:31.860
depending on whether we wanna think of
this part of that gigantic array as,

194
00:10:31.860 --> 00:10:35.383
this part's the string right here,
and this part's a 32-bit integer, and

195
00:10:35.383 --> 00:10:36.981
this part's a 64-bit integer.

196
00:10:36.981 --> 00:10:41.242
Something that we're gonna do a lot of in
this course and that is a major part of C

197
00:10:41.242 --> 00:10:44.556
programming is working in terms
of these memory addresses.

198
00:10:44.556 --> 00:10:49.178
These are actual integers that
correspond to an index into this

199
00:10:49.178 --> 00:10:50.930
giant array of memory.

200
00:10:50.930 --> 00:10:55.603
So essentially, the reason that we saw we
were able to read memory garbage in our

201
00:10:55.603 --> 00:11:00.487
example is that basically what write takes
in addition to where do you wanna write,

202
00:11:00.487 --> 00:11:02.896
it's give me a start index into memory.

203
00:11:02.896 --> 00:11:06.910
This is the address into that array,
the index into this gigantic global array.

204
00:11:06.910 --> 00:11:10.098
And then tell me the length, tell me
how many slots do you wanna to read,

205
00:11:10.098 --> 00:11:10.854
how many bytes?

206
00:11:10.854 --> 00:11:13.217
So basically saying yeah, start here and

207
00:11:13.217 --> 00:11:15.992
just print out whatever
the next 13 bytes are.

208
00:11:15.992 --> 00:11:19.800
Now that's exactly why when I changed it
to 200 or 2,000 it was start here and

209
00:11:19.800 --> 00:11:22.277
print out the next 2,000
elements in that array.

210
00:11:22.277 --> 00:11:25.146
That's exactly how you're able
to get [LAUGH] exposed secrets.

211
00:11:25.146 --> 00:11:28.604
It's yeah, if we keep going and there's
some secrets later on in the array,

212
00:11:28.604 --> 00:11:31.440
they're getting printed cuz
that's all this function does.

213
00:11:31.440 --> 00:11:34.115
So on the one hand,
this is a very simple design.

214
00:11:34.115 --> 00:11:36.518
It's basically, where do you wanna write?

215
00:11:36.518 --> 00:11:39.254
What's the start index of
the bytes in memory and

216
00:11:39.254 --> 00:11:43.536
how many bytes of that gigantic array
that we call memory do you wanna write?

217
00:11:43.536 --> 00:11:47.160
On the other hand, we can also see how
this simplicity is both powerful and

218
00:11:47.160 --> 00:11:48.809
also very, very error-prone.

219
00:11:48.809 --> 00:11:53.128
If you get this wrong, all bets are off.

220
00:11:53.128 --> 00:11:57.958
Whatever happens to be in that memory is
being sent to whatever destination we give

221
00:11:57.958 --> 00:11:58.592
to write.

222
00:11:58.592 --> 00:12:02.069
Okay, now I wanna address
the term pointer.

223
00:12:02.069 --> 00:12:05.237
For the purposes of this workshop, I'm
gonna tend to use the term memory address

224
00:12:05.237 --> 00:12:08.464
because I think it's more intuitive, but
you will encounter this term pointer.

225
00:12:08.464 --> 00:12:12.124
And it's famously a term that a lot
of people tend to associate with,

226
00:12:12.124 --> 00:12:14.912
it took me a long time to
understand what pointers mean.

227
00:12:14.912 --> 00:12:16.587
That's how I'm gonna use
the term memory address.

228
00:12:16.587 --> 00:12:19.868
But if you ever do encounter this term,
pointer just means memory address.

229
00:12:19.868 --> 00:12:21.432
There's this kind of funny tweet.

230
00:12:21.432 --> 00:12:23.566
It's, hey, why are people
constantly complaining they don't

231
00:12:23.566 --> 00:12:24.352
understand pointers?

232
00:12:24.352 --> 00:12:26.894
Pointers are just an index into a global
array of bytes that we call memory.

233
00:12:26.894 --> 00:12:27.963
What's the problem?

234
00:12:27.963 --> 00:12:31.419
And I think the formulation of this is
supposed to be a take on the old joke.

235
00:12:31.419 --> 00:12:34.597
Cuz, of course, monads are also
famously hard for people to understand.

236
00:12:34.597 --> 00:12:38.105
And the old joke is, monads are just
monoids in the category of endofunctors,

237
00:12:38.105 --> 00:12:39.128
what's the problem?

238
00:12:39.128 --> 00:12:41.868
I assume that's what he's referencing
with what's the problem here?

239
00:12:41.868 --> 00:12:44.746
So the point of this is
essentially that the term pointer,

240
00:12:44.746 --> 00:12:48.991
whenever you hear anyone talk about that,
they literally just mean memory address.

241
00:12:48.991 --> 00:12:53.457
They mean a number that's an index into
the giant global array of bytes that we

242
00:12:53.457 --> 00:12:54.291
call memory.

243
00:12:54.291 --> 00:12:56.275
So I'm gonna use the term memory
address for this workshop, but

244
00:12:56.275 --> 00:12:57.782
if you ever hear people
talking about pointers,

245
00:12:57.782 --> 00:12:59.010
that's what they're talking about.

