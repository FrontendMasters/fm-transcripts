WEBVTT

1
00:00:00.000 --> 00:00:03.180
What I will need to do in my problem,

2
00:00:03.180 --> 00:00:08.374
to solve to basically figure
out how to heat this red line,

3
00:00:08.374 --> 00:00:13.887
into the green line I can minimize
the loss function, right?

4
00:00:13.887 --> 00:00:19.716
And I can do it several ways, I can just
randomly modify w's and b's, by weight and

5
00:00:19.716 --> 00:00:25.350
by bias, right, and see what's
the current value of the lost function.

6
00:00:25.350 --> 00:00:30.460
And then just choose the set were w and

7
00:00:30.460 --> 00:00:35.780
b gives me the smallest loss function,
but it's random guessing, right, and

8
00:00:35.780 --> 00:00:42.270
it will take me forever just to go through
all of those combinations of w's and b's.

9
00:00:42.270 --> 00:00:47.810
So, we need somehow to get to those

10
00:00:47.810 --> 00:00:53.540
proper w's and b's by doing something with
them and probably is differentiation,

11
00:00:53.540 --> 00:00:57.360
right, so that's what neural networks
doing during the back propagation.

12
00:00:57.360 --> 00:01:02.020
They also looking at where we should go,

13
00:01:02.020 --> 00:01:05.025
basically how we should
change change w's and

14
00:01:05.025 --> 00:01:09.320
b's to make our loss
function slightly smaller.

15
00:01:10.730 --> 00:01:15.374
So, let's implement this, right, so

16
00:01:15.374 --> 00:01:19.889
[SOUND] what we'll do is the following.

17
00:01:19.889 --> 00:01:24.928
So I'm trying to reproduce right
now what neural network is

18
00:01:24.928 --> 00:01:32.201
doing during the training and we will
be using methods similar to basically.

19
00:01:32.201 --> 00:01:36.909
So first we need to specify
what is our learning_rate.

20
00:01:38.656 --> 00:01:43.431
And the learning rate is usually defined,
because when you

21
00:01:43.431 --> 00:01:49.120
calculate the slope where it should go,
you want to go there slowly.

22
00:01:49.120 --> 00:01:53.440
Let's say we have some
independent parameters, right?

23
00:01:53.440 --> 00:02:00.700
In our case, we just simply playing
with the w's and b's, right?

24
00:02:00.700 --> 00:02:06.770
So let's say that's our bias and

25
00:02:06.770 --> 00:02:12.684
loss function depends on bias
something like this, right,

26
00:02:12.684 --> 00:02:18.540
and last function is just the error.

27
00:02:18.540 --> 00:02:22.005
So kind of how far away in our
case this line we're trying

28
00:02:22.005 --> 00:02:26.670
to fit into the distribution, how far
it is from all those points, right?

29
00:02:26.670 --> 00:02:31.324
It's all this average squared
distances between the line and

30
00:02:31.324 --> 00:02:36.256
as soon as we get to this ideal position,
we will be at this point.

31
00:02:36.256 --> 00:02:40.735
But we starting from somewhere random,
right, so

32
00:02:40.735 --> 00:02:46.256
that will be, actually,
let me just draw it with a green line,

33
00:02:46.256 --> 00:02:50.337
so let's say that's our
initial init guess.

34
00:02:53.142 --> 00:02:59.134
The reason why we want to have learning
rate because after you have calculated

35
00:02:59.134 --> 00:03:05.720
the gradient, after you have calculated
the direction where you need to go.

36
00:03:05.720 --> 00:03:09.352
So basically gradient will tell you, okay,

37
00:03:09.352 --> 00:03:15.384
it looks like you need to go this way
to minimize the loss function, right?

38
00:03:15.384 --> 00:03:19.519
If you not apply learning
rate what might happen,

39
00:03:19.519 --> 00:03:24.441
it might say you need to change your b,
so there's delta b,

40
00:03:24.441 --> 00:03:27.502
can be too big and you can overshoot.

41
00:03:30.253 --> 00:03:34.885
Well, we basically going this direction,
but delta b can be anything for

42
00:03:34.885 --> 00:03:38.184
instance, we can end up
being somewhere over here.

43
00:03:38.184 --> 00:03:41.814
So that's gonna be our b new.

44
00:03:43.535 --> 00:03:47.094
And that's gonna be out next set of b's,
and

45
00:03:47.094 --> 00:03:50.286
we will do the same thing with w, right?

46
00:03:50.286 --> 00:03:54.808
So for w, this loss function
might look slightly different.

47
00:03:57.047 --> 00:04:02.884
So, we also probably started,
let's say that was our original w zero.

48
00:04:02.884 --> 00:04:07.571
And if we're not accurate
with our learning rate,

49
00:04:07.571 --> 00:04:11.931
we might, for instance,
go this direction but

50
00:04:11.931 --> 00:04:17.580
might overshoot and
get to w1 somewhere over here.

51
00:04:17.580 --> 00:04:19.216
And in worst case scenario,

52
00:04:19.216 --> 00:04:23.460
what might happen you might kind
of jump in out of your minimum.

53
00:04:23.460 --> 00:04:28.282
That's not what you want and
learning rate allows you to simply go to

54
00:04:28.282 --> 00:04:33.117
the direction where we should get
to the minimum but not overshoot.

55
00:04:33.117 --> 00:04:37.720
So what it's doing is just taking this w,

56
00:04:37.720 --> 00:04:42.719
delta b and
multiplying by learning rate, so

57
00:04:42.719 --> 00:04:48.111
instead of jumping all to here,
we'll only jump

58
00:04:48.111 --> 00:04:53.524
to let's say here and
the next b will be this one.

59
00:04:53.524 --> 00:04:58.507
And then learning rates simply
gonna get us on the next step,

60
00:04:58.507 --> 00:05:03.405
this value, and
hopefully we'll end up somewhere close.

61
00:05:03.405 --> 00:05:08.826
But you can see, for instance, I'm in the
distribution right now with this learning

62
00:05:08.826 --> 00:05:13.710
rate, where I will be jumping around
the minimum but not gonna get to it.

63
00:05:13.710 --> 00:05:15.390
So there are a couple of tricks for
instance,

64
00:05:15.390 --> 00:05:17.410
learning rates can be decreased as you go.

65
00:05:17.410 --> 00:05:21.990
So you can get to the point where
you almost like isolating around

66
00:05:21.990 --> 00:05:25.890
minimum points, you're shrinking the
learning rates and getting closer to it.

