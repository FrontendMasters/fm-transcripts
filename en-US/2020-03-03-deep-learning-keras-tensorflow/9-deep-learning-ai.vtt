WEBVTT

1
00:00:00.720 --> 00:00:03.720
So back to our discussion.

2
00:00:05.120 --> 00:00:11.689
I said that statistics, historically
transition to machine learning and

3
00:00:11.689 --> 00:00:18.267
this transition was, I'd say basically
with the increased complexity.

4
00:00:18.267 --> 00:00:22.562
While statistics is trying to simplify
everything by reducing the information

5
00:00:22.562 --> 00:00:23.146
you have.

6
00:00:23.146 --> 00:00:26.063
And when I'm talking about information,
let's say,

7
00:00:26.063 --> 00:00:28.990
we asked 100 people about
their shoe sizes, right?

8
00:00:28.990 --> 00:00:33.937
But reduced all of this information
to just average size, so

9
00:00:33.937 --> 00:00:35.980
that was the reduction.

10
00:00:35.980 --> 00:00:39.520
Machine learning is operating
with the distributions.

11
00:00:39.520 --> 00:00:44.940
So for instance, machine learning can
help you to actually find not just

12
00:00:44.940 --> 00:00:50.450
only the average but
the distribution of the sizes, right?

13
00:00:50.450 --> 00:00:53.909
So for instance, if that's the size and

14
00:00:53.909 --> 00:00:59.912
let's say we can somehow figure out
that we do have two local maximus,

15
00:00:59.912 --> 00:01:03.902
for instance,
at size eight and at size 11.

16
00:01:03.902 --> 00:01:09.361
In this case, probably it's better for
me just to well,

17
00:01:09.361 --> 00:01:13.940
create shoes of that
size eight than size 11.

18
00:01:13.940 --> 00:01:20.050
So machine learning is kind of
doing this distribution analysis.

19
00:01:20.050 --> 00:01:27.110
And as I showed deep learning simply
was a progression of machine learning.

20
00:01:27.110 --> 00:01:30.532
So we're just playing around
with a lot of those neurons.

21
00:01:30.532 --> 00:01:37.910
We figure out the way how to, we'll
learn this information, transformation.

22
00:01:37.910 --> 00:01:41.760
If you think about it conceptually
even with deep learning,

23
00:01:41.760 --> 00:01:44.240
we're doing this
information transformation.

24
00:01:44.240 --> 00:01:48.770
So our input signal was the image itself,
right?

25
00:01:48.770 --> 00:01:52.590
It was those old pixels,
intensity on the pixels.

26
00:01:52.590 --> 00:01:58.616
But in the end, we reduced or kinda
shrink this information to just one beat,

27
00:01:58.616 --> 00:02:04.197
with a label like if we're looking
at the hotdog or not hotdog, right?

28
00:02:04.197 --> 00:02:08.620
Then that will be just your
last neuron activated or not.

29
00:02:08.620 --> 00:02:10.480
So deep learning is doing that, but

30
00:02:10.480 --> 00:02:14.780
at the same time when we talk
about artificial intelligence, AI.

31
00:02:16.270 --> 00:02:17.830
At least when I'm talking about AI,

32
00:02:17.830 --> 00:02:21.390
I'm thinking something
towards like Turing test.

33
00:02:21.390 --> 00:02:24.630
I want my computer to be able to think.

34
00:02:24.630 --> 00:02:25.954
What does it mean to think?

35
00:02:25.954 --> 00:02:29.977
It means to somehow,
maybe even create things,

36
00:02:29.977 --> 00:02:37.400
your creativity is definitely something
we associating with the intelligence.

37
00:02:37.400 --> 00:02:41.680
But our deep learning models right
now they do not create anything,

38
00:02:41.680 --> 00:02:45.100
it's simply kind of reducing
the formation, right?

39
00:02:45.100 --> 00:02:50.002
So we taking the images on down just say,
if we're looking at the hotdog or

40
00:02:50.002 --> 00:02:56.060
not hotdog worse, is it a spam or
not spam if we're providing emails.

41
00:02:56.060 --> 00:03:02.160
So the next step with AI is what
we do with those deep learnings.

42
00:03:02.160 --> 00:03:06.289
So for instance,
remember with deep learning we

43
00:03:06.289 --> 00:03:10.128
have this almost triangle diagram, right?

44
00:03:10.128 --> 00:03:14.134
Where we just provided the inputs,
some inputs and

45
00:03:14.134 --> 00:03:18.480
then reduce it to the one neuron,
or several neurons.

46
00:03:18.480 --> 00:03:23.285
Interestingly enough,
you can do kind of the reverse operation.

47
00:03:23.285 --> 00:03:28.238
You can simply take this neuron and
by providing some of

48
00:03:28.238 --> 00:03:32.660
the informations we
collected in those layers.

49
00:03:35.760 --> 00:03:39.360
You can recreate some things.

50
00:03:40.440 --> 00:03:48.570
So for instance, you can trim your
model to redraw images for you.

51
00:03:49.660 --> 00:03:54.058
And one interesting example that's
the notebook we have actually.

52
00:03:54.058 --> 00:03:59.330
And you know what, I will switch to it and
demonstrate to you exactly what I mean.

53
00:03:59.330 --> 00:04:04.789
So in our notebooks,
if you look at creating stylist for

54
00:04:04.789 --> 00:04:08.168
images, our notebook 0104.

55
00:04:08.168 --> 00:04:13.690
We can even execute it right now.

56
00:04:15.770 --> 00:04:20.170
What we're doing we simply
grabbing pre trained model

57
00:04:21.490 --> 00:04:25.390
and we will need to input images.

58
00:04:26.570 --> 00:04:32.876
So let's say we can just take
downtown Minneapolis, right?

59
00:04:32.876 --> 00:04:34.320
That's the image.

60
00:04:34.320 --> 00:04:37.090
And style image right here.

61
00:04:37.090 --> 00:04:40.760
And what will happen,
let's go back to our diagram.

62
00:04:41.930 --> 00:04:47.420
When we'll be training our
model on our stylized image,

63
00:04:47.420 --> 00:04:54.290
we can just see what neurons were
activated on the first lower levels.

64
00:04:54.290 --> 00:04:57.840
Maybe couple of first layers.

65
00:04:57.840 --> 00:05:02.180
The idea is that our
neurons might somehow graph

66
00:05:02.180 --> 00:05:07.160
the style of drawing
from that original image,

67
00:05:07.160 --> 00:05:11.560
original picture, or drawing in our case.

68
00:05:11.560 --> 00:05:16.070
But then we can use second picture
to do kind of the same thing but

69
00:05:16.070 --> 00:05:23.450
capture the weight and all the activating
pathways from the last layers.

70
00:05:26.240 --> 00:05:30.580
And then when we are gonna be
recreating the image, we'll just

71
00:05:30.580 --> 00:05:36.690
substitute those first layers where we
have the information about the style.

72
00:05:36.690 --> 00:05:40.420
And we're gonna put it right here.

73
00:05:40.420 --> 00:05:45.680
So it means that the main information
kind of what objects we're looking at

74
00:05:47.070 --> 00:05:52.200
will be preserved from the original image,
in our case, downtown Minneapolis.

75
00:05:52.200 --> 00:06:01.090
But this style will be modified to
whatever style we have in our drawing.

76
00:06:01.090 --> 00:06:07.859
So the end result will basically be
stylized picture of downtown Minneapolis.

