WEBVTT

1
00:00:00.580 --> 00:00:03.150
Let's keep our models simple for now.

2
00:00:03.150 --> 00:00:07.190
So basically, what we did so
far, we flatten our image and

3
00:00:07.190 --> 00:00:10.610
then we had those 256 neurons.

4
00:00:10.610 --> 00:00:13.390
And how can we get the results?

5
00:00:13.390 --> 00:00:18.790
Well, we need to add output layer,
which can be dense as well.

6
00:00:20.780 --> 00:00:27.160
And let's see, since we have 10 digits,

7
00:00:27.160 --> 00:00:31.930
the one we're trying to recognize, we
can say that we want to have 10 neurons,

8
00:00:33.620 --> 00:00:36.910
and activation function for them.

9
00:00:36.910 --> 00:00:39.510
So basically, what I'm doing right now,

10
00:00:39.510 --> 00:00:45.430
I'm saying that all my 256
neurons from the previous layer

11
00:00:45.430 --> 00:00:50.870
should be connected to all 10 units or
10 neurons in my last layer.

12
00:00:50.870 --> 00:00:56.350
And each of those 10 neurons should print
out something, so activation function for

13
00:00:56.350 --> 00:01:01.096
them can be Softmax for instance, yeah.

14
00:01:01.096 --> 00:01:06.880
So softmax, it's kinda similar to sigmoid,

15
00:01:06.880 --> 00:01:11.280
so it gives you the distribution,
10 numbers between zero and one.

16
00:01:11.280 --> 00:01:17.160
But if you have multiple units in
the last layer, what softmax is doing,

17
00:01:17.160 --> 00:01:21.900
it's simply normalise all the outputs
of those 10 units to one.

18
00:01:21.900 --> 00:01:26.960
So let me probably just repeat it
once again through the diagram.

19
00:01:26.960 --> 00:01:29.600
So if you have sigmoid,

20
00:01:31.840 --> 00:01:36.400
we can get any numbers between zero and
one, right?

21
00:01:36.400 --> 00:01:38.558
So that's the distribution of outputs.

22
00:01:38.558 --> 00:01:43.519
So sigmoid output will be

23
00:01:43.519 --> 00:01:50.140
somewhere between zero and one.

24
00:01:52.760 --> 00:02:00.850
With the softmax,
it's also between zero and one.

25
00:02:00.850 --> 00:02:04.280
But if you have several units like this,

26
00:02:06.340 --> 00:02:13.100
it will guarantee that the sum of all
of those outputs will be equal to one.

27
00:02:15.460 --> 00:02:17.640
Why it is useful?

28
00:02:17.640 --> 00:02:21.137
Well, if we normalize that to one,

29
00:02:21.137 --> 00:02:26.266
it means that now we can look
at the outputs from those

30
00:02:26.266 --> 00:02:31.164
neurons as probabilities
of having digits zero,

31
00:02:31.164 --> 00:02:36.546
one, two, three, and so
on and so forth, till nine.

32
00:02:36.546 --> 00:02:42.880
And basically, that's can be considered
almost like your one hot encoding.

33
00:02:42.880 --> 00:02:48.760
So where we have the maximum signal,
so for instance,

34
00:02:48.760 --> 00:02:54.960
let's say we were providing zero
handwritten digits to our model.

35
00:02:54.960 --> 00:02:58.415
And in this case,
probably when the model will be trained,

36
00:02:58.415 --> 00:03:02.550
we'll get something like 0.99 probability.

37
00:03:02.550 --> 00:03:07.570
Well, no, it's not percentage, it's
just the value, but in percentage shift

38
00:03:07.570 --> 00:03:13.958
will means 99%, right,
that we are looking at Digit zero.

39
00:03:13.958 --> 00:03:18.370
And others will have something really,
really, really, really small.

40
00:03:18.370 --> 00:03:24.316
But some of those all really,
really small probabilities will end up,

41
00:03:24.316 --> 00:03:27.199
okay, the sum will be equal to 1.

42
00:03:27.199 --> 00:03:29.680
So that's what softmaxes do.

