WEBVTT

1
00:00:00.000 --> 00:00:05.652
The only notebook we left untouched, but

2
00:00:05.652 --> 00:00:11.149
we probably have to just do it on our own.

3
00:00:11.149 --> 00:00:14.670
But at the same time,
this notebook is very, very documented.

4
00:00:14.670 --> 00:00:17.000
And it's talking about
reinforcement learning.

5
00:00:17.000 --> 00:00:23.160
So, with this example,
you will be using OpenAI gym.

6
00:00:23.160 --> 00:00:30.350
If you look at their website, OpenAI
actually created amazing environment for

7
00:00:30.350 --> 00:00:34.700
playing around different games, and
trying to figure out what to do with them.

8
00:00:34.700 --> 00:00:41.317
In my examples, I'm using a simple
classic control, CartPole-v1.

9
00:00:41.317 --> 00:00:44.634
So it just simply,
this pole you have on carts, and

10
00:00:44.634 --> 00:00:48.480
you can move cart to the left or
you can move it to the right.

11
00:00:48.480 --> 00:00:51.353
And the task is to try to
balance your pole, and

12
00:00:51.353 --> 00:00:54.236
it's trying to preserve
the laws of physics.

13
00:00:54.236 --> 00:00:56.684
Meaning that if it's bemt,
it's probably gonna fall, and

14
00:00:56.684 --> 00:01:00.630
you're trying to avoid it by just moving
your cart in the corresponding direction.

15
00:01:00.630 --> 00:01:04.433
And in this notebook first,
and by the way, yeah,

16
00:01:04.433 --> 00:01:08.684
that's all the experiments
available through the gym.

17
00:01:08.684 --> 00:01:13.504
For instance,
you can even play Atari games,

18
00:01:13.504 --> 00:01:18.265
and try to, For instance,

19
00:01:18.265 --> 00:01:23.214
educate your Pac-Man how to
await all those ghosts, or

20
00:01:23.214 --> 00:01:26.172
play Box, and so on and so forth.

21
00:01:26.172 --> 00:01:31.159
There's more advanced ones,
for instance, with MuJoCo,

22
00:01:31.159 --> 00:01:36.550
you can even try to manipulate the,
Robots,

23
00:01:36.550 --> 00:01:43.497
and try to educate humanoids to walk, but
you will need 3D physical engine for this.

24
00:01:43.497 --> 00:01:48.419
Yeah, all right, so
in our example in this notebook, yes,

25
00:01:48.419 --> 00:01:53.540
there are 800 different
environments listed all here.

26
00:01:53.540 --> 00:01:55.396
Well, 859 of them, actually.

27
00:01:55.396 --> 00:02:00.896
But with the CartPole,
you can just simply load this environment.

28
00:02:00.896 --> 00:02:07.212
And everything to you will be
provided as those four numbers,

29
00:02:07.212 --> 00:02:12.565
which actually correspond
to position of the cart.

30
00:02:12.565 --> 00:02:14.989
The current velocity,
and if it's negative,

31
00:02:14.989 --> 00:02:19.180
it means it's moving to the left, if
it's positive, it's moving to the right.

32
00:02:19.180 --> 00:02:25.170
And that's the angle of your pole and
corresponding angular velocity.

33
00:02:25.170 --> 00:02:30.018
So given all those coordinates,
you can even render.

34
00:02:30.018 --> 00:02:33.516
Let's see if we can
render the environment.

35
00:02:36.387 --> 00:02:39.992
Right, so
it will visualize it in a separate window.

36
00:02:39.992 --> 00:02:43.718
And now,
anything I'm gonna be doing in the code,

37
00:02:43.718 --> 00:02:47.283
it will simply change
the position of this cart.

38
00:02:47.283 --> 00:02:55.511
So for instance, I can say what's
the size of my action space,

39
00:02:55.511 --> 00:03:00.110
and it's 2, it means that I can move
to the left or I can move to the right.

40
00:03:00.110 --> 00:03:07.165
And for instance, action number 1
will just move my cart to the right.

41
00:03:07.165 --> 00:03:12.257
Let me try to actually put
the visualization window and

42
00:03:12.257 --> 00:03:14.919
my codes on the same screen.

43
00:03:18.980 --> 00:03:23.780
Here we go, okay, so
if I do this, you see it's

44
00:03:23.780 --> 00:03:28.710
actually slightly moving
our cart to the right.

45
00:03:28.710 --> 00:03:34.392
And we see that every time I execute this
cell, our cart's moving faster and faster.

46
00:03:34.392 --> 00:03:39.090
And it's basically making
this pole fall down.

47
00:03:39.090 --> 00:03:42.970
But anyway, that's the setup,
and you can play around with it.

48
00:03:42.970 --> 00:03:47.882
So you can try to figure out
what action you need to take to,

49
00:03:47.882 --> 00:03:50.095
well, balance the pole.

50
00:03:50.095 --> 00:03:54.964
And I explained first that you can
probably try something simple.

51
00:03:54.964 --> 00:03:57.959
For instance, if angle is negative,
just move to the right, and

52
00:03:57.959 --> 00:04:01.352
to the left if it's positive,
move to the right, right, to balance it.

53
00:04:01.352 --> 00:04:06.434
But you will also figure out quickly
that it actually introduce more and

54
00:04:06.434 --> 00:04:09.030
more fluctuations to your pole.

55
00:04:09.030 --> 00:04:12.970
And so
we need to be slightly smarter than that.

56
00:04:12.970 --> 00:04:19.860
And Q-values or Q-Learning is
one technique how to do this.

57
00:04:19.860 --> 00:04:24.260
With Q-Learning, you're just simply
saying, we have different states and

58
00:04:24.260 --> 00:04:27.170
we have particular actions we can take.

59
00:04:27.170 --> 00:04:30.095
And you're just simply
trying to figure out,

60
00:04:30.095 --> 00:04:33.545
depending on the state,
what you have in your system,

61
00:04:33.545 --> 00:04:38.125
what actions you should take to
maximize the profit or reward function.

62
00:04:38.125 --> 00:04:43.410
Right, and Q-Learning is one technique
which can help you to do that.

63
00:04:43.410 --> 00:04:50.096
So once again, as I said, this is
a pretty nicely documented notebook.

64
00:04:50.096 --> 00:04:53.717
So I would recommend just
to go through it yourself,

65
00:04:53.717 --> 00:04:56.280
play around with the provided code.

66
00:04:56.280 --> 00:05:02.166
And at the end, it's actually
even using optimized solution,

67
00:05:02.166 --> 00:05:07.782
the one available through
the TensorFlow hub itself.

68
00:05:07.782 --> 00:05:11.955
So TensorFlow-Agents allows
you to use some of the agents

69
00:05:11.955 --> 00:05:15.629
which are already being
created by somebody else.

70
00:05:15.629 --> 00:05:20.140
And you can just apply those to
train your agents in, for instance,

71
00:05:20.140 --> 00:05:24.744
in this case, I think it's one of
the Atari games, yeah, this one.

72
00:05:24.744 --> 00:05:30.390
So you're just trying to move
your agent right to basically,

73
00:05:32.317 --> 00:05:36.750
Keep this bouncing ball in the field.

74
00:05:36.750 --> 00:05:41.500
But you can teach your your agent to do
exactly that, and don't lose the ball.

