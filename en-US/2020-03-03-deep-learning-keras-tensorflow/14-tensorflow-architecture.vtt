WEBVTT

1
00:00:00.041 --> 00:00:05.340
Why I'm discussing all of this?

2
00:00:05.340 --> 00:00:10.020
It's to explain how the TensorFlow works.

3
00:00:11.190 --> 00:00:13.110
So with all of that,

4
00:00:13.110 --> 00:00:19.042
a TensorFlow is trying to optimize
the utilization of the hardware.

5
00:00:19.042 --> 00:00:23.510
So TensorFlow,
can actually combine T and F together.

6
00:00:25.130 --> 00:00:30.990
And actually even Keras, but
Keras is the higher level API.

7
00:00:30.990 --> 00:00:35.324
So Keras is behind the scenes
actually translating or

8
00:00:35.324 --> 00:00:39.000
can use TensorFlow directly, right?

9
00:00:39.000 --> 00:00:43.120
And TensorFlow, mistakenly,

10
00:00:43.120 --> 00:00:45.620
some people think that
TensorFlow is written in Python.

11
00:00:45.620 --> 00:00:48.458
It's not actually true.

12
00:00:48.458 --> 00:00:54.842
TensorFlow is written in C and
C++, and using Python,

13
00:00:54.842 --> 00:01:00.029
if only I can spell [LAUGH]
using Python to simply

14
00:01:00.029 --> 00:01:04.951
grab the description of the model, right?

15
00:01:04.951 --> 00:01:08.790
So a user is describing models in Python.

16
00:01:08.790 --> 00:01:15.569
But then this description is actually
kinda transformed and using C and C++,

17
00:01:19.584 --> 00:01:26.160
Source code for implementation
of particular reverberations.

18
00:01:26.160 --> 00:01:31.674
And now I need to also tell you about
computational graph, comp graph.

19
00:01:34.400 --> 00:01:38.992
What TensorFlow version
1 was using by default,

20
00:01:38.992 --> 00:01:45.010
it was building computational
graph as the default behavior.

21
00:01:45.010 --> 00:01:46.440
What is computational graph?

22
00:01:46.440 --> 00:01:51.370
It's basically all the code
you create in Python.

23
00:01:53.930 --> 00:01:57.440
It will be rebuilt as part
of the computational graphs,

24
00:01:57.440 --> 00:02:02.680
with notes being your operations and links
between those graphs ease the operation.

25
00:02:02.680 --> 00:02:07.376
So for instance, if I'm in Python,

26
00:02:07.376 --> 00:02:13.586
just writing a + b, or
actually in TensorFlow,

27
00:02:13.586 --> 00:02:20.570
it will be tf.constant(a)
+ tf.constant(b).

28
00:02:20.570 --> 00:02:26.800
It will create simply node a,
node b, right?

29
00:02:26.800 --> 00:02:28.059
That's our constants.

30
00:02:29.140 --> 00:02:32.880
And it will be additional node
the operation of addition.

31
00:02:32.880 --> 00:02:36.690
And the link between those
will be your dependencies.

32
00:02:36.690 --> 00:02:43.685
And the thing is why TensorFlow was doing
that and actually still can do that,

33
00:02:43.685 --> 00:02:48.542
because it allows you to optimize for
a lot of things.

34
00:02:48.542 --> 00:02:54.518
First of all, when you have all of
those operations, for instance,

35
00:02:54.518 --> 00:03:00.923
if I want to multiply this by c now,
right, I can just put this operation.

36
00:03:00.923 --> 00:03:08.500
And then let's say add additional value v,
I don't know.

37
00:03:09.600 --> 00:03:15.300
Basically, I can figure out what
operations I can do in parallel.

38
00:03:15.300 --> 00:03:21.362
Also, how much memory do I need,
right, so for all of those constants.

39
00:03:21.362 --> 00:03:28.920
And right now it just simply found one
variables, but it might be tensors here.

40
00:03:28.920 --> 00:03:31.750
I might actually want
to allocate megabytes,

41
00:03:31.750 --> 00:03:35.520
if not terabytes of RAM,
right, for my tensor.

42
00:03:35.520 --> 00:03:39.549
And TensorFlow,
when building this computational graph,

43
00:03:39.549 --> 00:03:42.162
it can optimize for those operations.

44
00:03:42.162 --> 00:03:47.430
Because one of the slowest
operation is the memory allocation.

45
00:03:47.430 --> 00:03:52.670
And when you need to allocate a lot,
you do it through the operating system,

46
00:03:52.670 --> 00:03:54.258
it will take a lot of time.

47
00:03:54.258 --> 00:03:56.467
And what TensorFlow can do,

48
00:03:56.467 --> 00:04:01.806
it can just allocate one buffer in
the memory and it keep track of it and

49
00:04:01.806 --> 00:04:06.702
just put, for instance,
your tensor v in some parts, right?

50
00:04:06.702 --> 00:04:11.615
And when we're done with v,
when we don't need it,

51
00:04:11.615 --> 00:04:17.771
you can just simply overwrite
the space with some other tensors.

52
00:04:17.771 --> 00:04:21.560
But at the same time,
there will be no additional allocations.

53
00:04:21.560 --> 00:04:26.360
We're just using this buffer,
the one TensorFlow is keeping track of.

54
00:04:26.360 --> 00:04:31.190
And also with all those operations,
as I said, TensorFlow can look for

55
00:04:31.190 --> 00:04:34.153
the parallelism optimizations.

56
00:04:34.153 --> 00:04:36.643
So it can figure out if, for instance,

57
00:04:36.643 --> 00:04:40.345
we're doing some operations
which are not independent.

58
00:04:40.345 --> 00:04:43.902
So for instance,
I can have additional graph here.

59
00:04:43.902 --> 00:04:50.290
Then we can probably just run
this part of the graph on core 0.

60
00:04:50.290 --> 00:04:55.298
And this part of the graph can
be executed on core 1, right?

61
00:04:55.298 --> 00:04:58.830
Or if we're talking about GPUs, then

62
00:05:00.540 --> 00:05:05.970
we probably can just use different
cores for those operations as well.

63
00:05:05.970 --> 00:05:11.088
Or even if we are talking about
heterogeneous programming,

64
00:05:11.088 --> 00:05:14.512
we can execute part of the graph on a GPU.

65
00:05:14.512 --> 00:05:19.426
And then transition the data when the most
kinda, let's say compute intensive

66
00:05:19.426 --> 00:05:23.558
parts were calculated,
we can transition the result back to CPU.

67
00:05:23.558 --> 00:05:28.432
So that's what TensorFlow
was doing behind the scenes.

68
00:05:28.432 --> 00:05:32.685
And for a lot of those
compute intensive operations,

69
00:05:32.685 --> 00:05:36.380
it's actually using pre-built libraries.

70
00:05:36.380 --> 00:05:39.732
So remember,
I said that we have this linking part.

71
00:05:39.732 --> 00:05:46.095
And TensorFlow are relying on CuDNN.

72
00:05:46.095 --> 00:05:51.994
So Cuda was originally, I can probably

73
00:05:51.994 --> 00:05:57.546
even call it programming language.

74
00:05:57.546 --> 00:06:02.659
Although it is heavily based on C,
C++, but Cuda was specifically

75
00:06:02.659 --> 00:06:08.140
designed to program GPUs, right,
NVIDIA GPUs, to be specific.

76
00:06:08.140 --> 00:06:14.700
And now NVIDIA also created CuDNN,
for instance.

77
00:06:14.700 --> 00:06:18.950
That's the heavily optimized library for
parallelism and

78
00:06:18.950 --> 00:06:23.670
utilization of the GPUs with basic

79
00:06:24.690 --> 00:06:29.370
building blocks of deep neural networks.

80
00:06:29.370 --> 00:06:32.370
So it's doing, it's optimizing.

81
00:06:32.370 --> 00:06:35.135
Hopefully I will demonstrate to you later,

82
00:06:35.135 --> 00:06:39.770
but a lot of this neural network
operations, they're relying on general

83
00:06:39.770 --> 00:06:44.635
matrix-matrix multiplication or
matrix-vector multiplication.

84
00:06:44.635 --> 00:06:49.644
So remember a lot of
those connections between

85
00:06:49.644 --> 00:06:53.760
the neurons with different weights.

86
00:06:53.760 --> 00:06:58.899
So you can have just data as one vector.

87
00:06:58.899 --> 00:07:04.076
And weights in each layer
can be your matrix with v11,

88
00:07:04.076 --> 00:07:07.740
v12, and so on, so forth, right?

89
00:07:07.740 --> 00:07:13.800
And what you can do, you can just simply
take the data and multiply by the matrix.

90
00:07:13.800 --> 00:07:21.665
Anyway, there are a lot of details
behind this I don't wanna go into.

91
00:07:21.665 --> 00:07:27.498
But basically your general
matrix-matrix multiplication,

92
00:07:27.498 --> 00:07:31.900
GMM [LAUGH] or
matrix-vector multiplication,

93
00:07:31.900 --> 00:07:36.301
that's what's used as
the implementation for

94
00:07:36.301 --> 00:07:41.158
your deep neural networks training and
inference.

95
00:07:41.158 --> 00:07:48.720
And CuDNN is providing this
functionality on the GPU side.

96
00:07:48.720 --> 00:07:56.887
For CPUs, we have MKL, that's the math
kernel library created by Intel.

97
00:07:59.660 --> 00:08:06.987
But there is backend abstraction which
using actually Intel MKL called NumPy.

98
00:08:09.371 --> 00:08:15.200
So NumPy, numeric Python package,
is what is TensorFlow is using.

99
00:08:15.200 --> 00:08:17.460
And if you want to do something simple,

100
00:08:17.460 --> 00:08:20.141
I would recommend to get
familiar with NumPy.

101
00:08:20.141 --> 00:08:24.359
Because a lot of mathematical operations,

102
00:08:24.359 --> 00:08:29.580
NumPy is the way kinda to
link your Python and C.

103
00:08:29.580 --> 00:08:35.872
So a lot of math-intensive operations can
be implemented quite effectively in C.

104
00:08:35.872 --> 00:08:43.590
There is no overhead of object-oriented
programming and classes on all of that.

105
00:08:43.590 --> 00:08:46.370
So in C,
you can do a lot of things pretty fast.

106
00:08:46.370 --> 00:08:51.392
And NumPy allows you to draw
this from the kinda Python

107
00:08:51.392 --> 00:08:55.856
representation of the data and
use NumPy to just

108
00:08:55.856 --> 00:09:00.892
almost get to the bare metal
to manipulate your data.

109
00:09:00.892 --> 00:09:05.610
And TensorFlow is using
NumPy quite a bit a lot.

110
00:09:05.610 --> 00:09:09.490
Okay, so let me kinda finalize everything.

111
00:09:09.490 --> 00:09:14.530
I said that, yes, TensorFlow is
creating computational graph,

112
00:09:14.530 --> 00:09:17.850
which simply allows us to
optimize a lot of things.

113
00:09:17.850 --> 00:09:24.212
So it will try to use libraries depending
on what architecture you have available.

114
00:09:24.212 --> 00:09:28.320
If you have GPUs,
then a CuDNN will be linked.

115
00:09:29.950 --> 00:09:33.818
Or MKL, NumPy,
we'll be using those as well.

116
00:09:33.818 --> 00:09:38.280
And basically TensorFlow, what it's doing,

117
00:09:38.280 --> 00:09:44.400
it's simply looking at your Python
description of the model, right?

118
00:09:44.400 --> 00:09:50.254
And then figuring out what
C routines it should use,

119
00:09:50.254 --> 00:09:58.116
optimize C routines to get the best
speed out of the hardware you have.

120
00:09:58.116 --> 00:10:03.116
So [LAUGH] the reason why I
was describing it is to tell

121
00:10:03.116 --> 00:10:10.180
you the original purpose of TensorFlow,
why it was created, right?

122
00:10:10.180 --> 00:10:14.768
And what is happening right
now with TensorFlow 2.0,

123
00:10:14.768 --> 00:10:18.310
they actually switching
their default behavior.

124
00:10:19.375 --> 00:10:26.020
Previously, creating the computational
graph was the default behavior.

125
00:10:26.020 --> 00:10:28.490
Right now,
the default behavior is eager execution.

126
00:10:29.600 --> 00:10:33.600
Eager execution is different.

127
00:10:33.600 --> 00:10:40.015
It means that we will have
access to all of our variables.

128
00:10:40.015 --> 00:10:44.800
And all operations will be executed,
not in a lazy form,

129
00:10:44.800 --> 00:10:49.100
but right when we have it
typed as a Python command.

130
00:10:50.180 --> 00:10:55.390
It makes debugging your Python
code significantly easier, right?

131
00:10:55.390 --> 00:10:59.716
And that was a really
big benefit of PyTorch.

132
00:10:59.716 --> 00:11:04.560
PyTorch had eager execution from
the beginning, and that's why it was so

133
00:11:04.560 --> 00:11:08.830
convenient for searchers to play
around with their new ideas, right?

134
00:11:08.830 --> 00:11:12.300
Because they could have just print
out what's currently happening with

135
00:11:12.300 --> 00:11:13.400
the tensor, right?

136
00:11:13.400 --> 00:11:16.620
Or what's gonna happen if I normalize or

137
00:11:16.620 --> 00:11:22.722
do some sort of transformation on my code,
on my tensor, right?

138
00:11:22.722 --> 00:11:28.326
So TensorFlow team realized that
because of this complexity,

139
00:11:28.326 --> 00:11:32.662
because of computational graph,
you cannot see

140
00:11:32.662 --> 00:11:37.541
the values until the computational
graph is created.

141
00:11:37.541 --> 00:11:40.556
And there is also,
you need to create session, right,

142
00:11:40.556 --> 00:11:43.456
in which this computational
graph will be executed.

143
00:11:43.456 --> 00:11:47.870
And then you can debug it, or
at least look at the values.

144
00:11:47.870 --> 00:11:53.130
So that's why they decided, okay,
we can keep the computational graph for

145
00:11:53.130 --> 00:11:57.570
the optimization, but the default behavior
will be just using eager execution and

146
00:11:57.570 --> 00:11:59.370
execute your Python code.

147
00:11:59.370 --> 00:12:03.030
But when you're satisfied with your code,
you can just use special wrappers,

148
00:12:03.030 --> 00:12:07.640
Python decorators, right, or
TensorFlow function to just say,

149
00:12:07.640 --> 00:12:09.920
okay, I'm done with debugging my code.

150
00:12:09.920 --> 00:12:11.445
I'm done with eager execution.

151
00:12:11.445 --> 00:12:13.620
Switch to computational graph.

152
00:12:13.620 --> 00:12:17.947
And then TensorFlow will just
look at your Python code,

153
00:12:17.947 --> 00:12:20.814
recreate the computational graph.

154
00:12:20.814 --> 00:12:26.539
We'll figure out the ways how parts
of this graph can be optimized,

155
00:12:26.539 --> 00:12:29.360
right, which C codes we can use.

156
00:12:29.360 --> 00:12:36.366
So we'll then rely heavily on CuDNN and
MKL, NumPy, right?

157
00:12:36.366 --> 00:12:41.170
And everything will be
executed significantly faster.

158
00:12:41.170 --> 00:12:45.817
So don't be surprised
if your eager executed

159
00:12:45.817 --> 00:12:50.464
code on TensorFlow 2.0 is somewhat slow,

160
00:12:50.464 --> 00:12:57.455
especially if you tried to execute
with the TensorFlow 1, right?

161
00:12:57.455 --> 00:13:01.777
It's just because TensorFlow
1 already using all of those

162
00:13:01.777 --> 00:13:04.462
optimizations from the beginning.

163
00:13:04.462 --> 00:13:09.262
And TensorFlow 2.0 is allowing
you the development setup first,

164
00:13:09.262 --> 00:13:13.080
and then only transition
to the computational graph.

