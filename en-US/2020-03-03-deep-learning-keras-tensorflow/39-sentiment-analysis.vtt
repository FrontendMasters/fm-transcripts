WEBVTT

1
00:00:00.090 --> 00:00:03.970
Now we can actually do
the sentiment analysis itself.

2
00:00:03.970 --> 00:00:07.904
So Right now,

3
00:00:07.904 --> 00:00:12.398
let's see,
we can use the IMDb data set once again.

4
00:00:12.398 --> 00:00:20.111
And in our training set there
will be 25,000 feedbacks.

5
00:00:20.111 --> 00:00:23.100
We are gonna use them for
training entries, and for

6
00:00:23.100 --> 00:00:26.860
the test entries it will be another 25.

7
00:00:26.860 --> 00:00:27.450
Just for once.

8
00:00:27.450 --> 00:00:32.400
And the train examples look like this,
right, very close.

9
00:00:32.400 --> 00:00:34.900
It's actually the text itself,

10
00:00:34.900 --> 00:00:39.915
that's what we have in our
training examples and labels, yes.

11
00:00:39.915 --> 00:00:44.460
One for positive feedbacks and
zero for negative ones.

12
00:00:44.460 --> 00:00:50.310
What we'll be doing is we'll just
grab the pre-trained model and

13
00:00:50.310 --> 00:00:55.220
I can just download it
from TensorFlow hub.

14
00:00:55.220 --> 00:00:56.999
So let me actually execute this code.

15
00:00:56.999 --> 00:01:01.527
I think it should be
fun to try it in life.

16
00:01:05.374 --> 00:01:11.928
And it's printing out the labels, yeah,
exactly the same and, We can just download

17
00:01:11.928 --> 00:01:17.035
this from TensorFlow Hub and you can play
there are a couple of other available.

18
00:01:18.125 --> 00:01:22.553
So, these pre-trained layers will then be

19
00:01:22.553 --> 00:01:27.905
talking to 16 fully connected neurons,
right?

20
00:01:27.905 --> 00:01:32.895
And at the end once again one neuron
which correspond to positive or negative

21
00:01:32.895 --> 00:01:38.880
feedback and
training should not take that long.

22
00:01:38.880 --> 00:01:41.690
Let's see, on my previous training,
it was only one minute.

23
00:01:41.690 --> 00:01:45.660
So, I think we can spend this
time training our model.

24
00:01:46.900 --> 00:01:52.294
So, right here, let's go quickly
back to what we're doing here.

25
00:01:52.294 --> 00:01:57.563
It is if you see those 20 dim,
it means that we

26
00:01:57.563 --> 00:02:03.350
using word embedding with 20 dimensions.

27
00:02:03.350 --> 00:02:05.810
So it means that our

28
00:02:05.810 --> 00:02:09.264
embedding layer is the one I've
downloaded from TensorFlow hub.

29
00:02:09.264 --> 00:02:13.050
It was pre-trained on some
other big data set and

30
00:02:13.050 --> 00:02:18.420
try to once again,
put those words into those 20 dimensions.

31
00:02:18.420 --> 00:02:25.852
And try to, can I understand
how words relate to each other,

32
00:02:25.852 --> 00:02:31.993
right, to find synonyms and
so on, and so forth.

33
00:02:31.993 --> 00:02:36.186
And that help us to process the text and
it is usually recommend that if

34
00:02:36.186 --> 00:02:40.669
you're trying to, for instance,
create your own text analysis model,

35
00:02:40.669 --> 00:02:44.821
just use pre-trained embedding,
especially embedding layers.

36
00:02:44.821 --> 00:02:47.351
And, let's see.

37
00:02:47.351 --> 00:02:53.810
The accuracy we getting on our
predictions right now is 86%, almost 87.

38
00:02:53.810 --> 00:02:54.383
Which is not that bad.

39
00:02:56.005 --> 00:03:03.382
And let's try plot the history just
to see if we not over trained and

40
00:03:03.382 --> 00:03:08.590
I think we're performing quite well right.

41
00:03:08.590 --> 00:03:12.138
But at the same time,
we almost plateaued at around 20 epochs.

42
00:03:12.138 --> 00:03:16.430
So additional 20 was kind of overkill.

43
00:03:16.430 --> 00:03:20.180
I could have stopped my training
at around 20 epochs and

44
00:03:20.180 --> 00:03:22.390
that should have been enough.

45
00:03:22.390 --> 00:03:25.190
And yeah, we can even do the prediction.

46
00:03:25.190 --> 00:03:29.140
So for instance,
provided this feedback, right?

47
00:03:29.140 --> 00:03:32.720
And that was negative feedback.

48
00:03:32.720 --> 00:03:40.120
We can do model predict and then predict
for instance, the first three comments.

49
00:03:40.120 --> 00:03:44.950
So you can see that our model said that
first two comments were negative and

50
00:03:44.950 --> 00:03:46.830
only the third comment was positive.

51
00:03:46.830 --> 00:03:50.570
So if I, for instance, print out so

52
00:03:50.570 --> 00:03:55.790
third comments is number two,
because we zero in the space,

53
00:03:55.790 --> 00:04:00.090
right so this comments while pre arch but
there's definitely.

54
00:04:01.270 --> 00:04:05.350
Can we see that it's negative
right from the get go?

55
00:04:05.350 --> 00:04:06.590
Probably not, okay.

56
00:04:08.100 --> 00:04:09.270
But yes, it is positive.

57
00:04:10.560 --> 00:04:17.361
And it was recognized by our system
with almost 99% that it is positive.

58
00:04:17.361 --> 00:04:19.412
So, yeah, that's how you or

59
00:04:19.412 --> 00:04:23.720
you can actually do the predictions
on your own phrases.

60
00:04:23.720 --> 00:04:25.390
Let's play around with it a little bit.

61
00:04:25.390 --> 00:04:31.250
So, for instance, yeah,
good movie but bad actor.

62
00:04:31.250 --> 00:04:36.050
So embedding,
the best thing about the embedding is that

63
00:04:36.050 --> 00:04:39.670
it's not actually taking into
account the word positioning.

64
00:04:40.780 --> 00:04:45.231
So a phrase good movie, but
bad actor and bad movie but good actor,

65
00:04:45.231 --> 00:04:47.945
would provide exactly the same result.

66
00:04:47.945 --> 00:04:51.814
At the same time, capitalized good and

67
00:04:51.814 --> 00:04:57.334
lower case good,
giving slightly different results.

68
00:04:57.334 --> 00:05:04.500
Once again, everything depends on the data
set and pre-processing of your text.

69
00:05:04.500 --> 00:05:11.180
So ideally, to do not distinguish between
capitalized good and lowercase good.

70
00:05:11.180 --> 00:05:16.420
I should have lowercased all my training
data set before doing all the trainings.

71
00:05:17.470 --> 00:05:22.760
But to take into account the order
of the words in my sentences,

72
00:05:22.760 --> 00:05:25.300
we need to switch to
a different polygenes.

73
00:05:25.300 --> 00:05:31.300
So now fully connected neural network,
right, the one we used at the beginning.

74
00:05:31.300 --> 00:05:37.270
Remember that it's connecting all
the inputs to our neuron, right?

75
00:05:37.270 --> 00:05:39.110
So if you think about it,

76
00:05:39.110 --> 00:05:44.550
it simply losing the information
about where the word was coming from.

77
00:05:45.630 --> 00:05:51.054
So that's why it basically doesn't
take into account the word order.

78
00:05:51.054 --> 00:05:55.650
But there are different models, for
instance, recurrent neural networks.

79
00:05:55.650 --> 00:06:01.243
What they do, they actually trying to
memorize the position where a particular

80
00:06:01.243 --> 00:06:07.177
instance we're at, and try to take into
account the order of, in our case, words.

81
00:06:07.177 --> 00:06:11.222
And that's the last example in our text,
Part C.

