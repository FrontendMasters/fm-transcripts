WEBVTT

1
00:00:00.340 --> 00:00:06.080
Now let's talk about where the convolution
neural networks is looking,

2
00:00:06.080 --> 00:00:08.620
where the making a prediction.

3
00:00:08.620 --> 00:00:14.610
And I will actually introduce another
concept of transfer learning.

4
00:00:14.610 --> 00:00:19.260
So with the transfer learning what you
can do, you can simply grab already

5
00:00:19.260 --> 00:00:24.170
trained models and
load them in your notebooks.

6
00:00:24.170 --> 00:00:28.270
So skipping all the training parts,
somebody else already did that job for

7
00:00:28.270 --> 00:00:32.280
you and probably used tons of GPU compute,
just the training module.

8
00:00:32.280 --> 00:00:38.158
And you can simply download it explicitly
for instance by using karas applications,

9
00:00:38.158 --> 00:00:41.990
VGG16 it's specifically
one of the architectures.

10
00:00:41.990 --> 00:00:44.350
So let me quickly load it.

11
00:00:45.990 --> 00:00:51.790
And we can probably even try
to say something like model

12
00:00:54.060 --> 00:00:57.560
summary, to find out more
about this architecture.

13
00:00:57.560 --> 00:01:05.500
So you can see it's actually taking 24 by
24 images with three channels, RGB, right?

14
00:01:05.500 --> 00:01:09.100
And doing a lot of convolutions,
so we're actually

15
00:01:09.100 --> 00:01:14.040
doing two convolutions in the sequence and
only after that we're doing max pooling.

16
00:01:15.040 --> 00:01:19.660
And you see that there are a lot
of convolutions happening here and

17
00:01:19.660 --> 00:01:24.950
when we're reducing everything,
almost to 7 by 7 filters, right?

18
00:01:24.950 --> 00:01:31.460
And only after that flattened image and
our last two fully connected layers.

19
00:01:33.440 --> 00:01:41.320
We'll reduce everything to those thousands
neurons which corresponds to the classes.

20
00:01:41.320 --> 00:01:44.150
Remember, in image net I
said that we're trying

21
00:01:44.150 --> 00:01:47.170
to recognize between
1,000 different objects.

22
00:01:47.170 --> 00:01:51.830
So that's why we need 1000
neurons at the lowest layer.

23
00:01:51.830 --> 00:01:59.800
But when we initialize that's when we grab
this VGG class from Kerris applications.

24
00:01:59.800 --> 00:02:05.870
We also can say that I want my model
to load not the random weights,

25
00:02:05.870 --> 00:02:09.420
I won't actually grab the weights
which have been trained on image net.

26
00:02:10.840 --> 00:02:15.290
So basically, I will already by having
this small, I can do prediction directly.

27
00:02:16.490 --> 00:02:21.760
I just need to provide image 224 by 224

28
00:02:21.760 --> 00:02:28.170
pixels and color image, and that's what
I will be showing you how to do next.

29
00:02:28.170 --> 00:02:33.571
So with this command, I'm just simply
downloading image of elephant.

30
00:02:36.434 --> 00:02:39.954
I can even visualize it here if you want,
and

31
00:02:39.954 --> 00:02:43.635
they can use mark down directly for this.

32
00:02:43.635 --> 00:02:48.725
So with the mark down to
visualize image allophones

33
00:02:48.725 --> 00:02:54.670
I need just to say, read if out [LAUGH].

34
00:02:54.670 --> 00:02:57.860
It's not actually after
completing in mark down but

35
00:02:57.860 --> 00:03:01.980
I can just copy/paste the name of
the file, I've just downloaded.

36
00:03:01.980 --> 00:03:08.010
And that's the image
we will be processing.

37
00:03:08.010 --> 00:03:13.270
Just beautiful picture of two elephants
under creative commons license,

38
00:03:13.270 --> 00:03:13.900
which is good for me.

39
00:03:15.070 --> 00:03:20.520
And what we'll do first we need to change
the resolution of our image, right?

40
00:03:20.520 --> 00:03:24.520
So that's what I'm doing here,
image load image,

41
00:03:24.520 --> 00:03:31.600
I'm just loading it's using image
from Keras pre processing, right?

42
00:03:31.600 --> 00:03:36.070
So basically, these class on their
tensorflow Keras pre processing image,

43
00:03:36.070 --> 00:03:43.120
is got its own function load image
from a particular location, right?

44
00:03:43.120 --> 00:03:45.840
From the file, store.

45
00:03:45.840 --> 00:03:51.579
And it can actually changed
the resolution of the image on the fly

46
00:03:51.579 --> 00:03:56.586
while loading it so
we converted it to 224 by 224.

47
00:03:56.586 --> 00:04:03.660
Right here, I'm just converting image to
the array and adding additional array.

48
00:04:03.660 --> 00:04:05.140
Once again, our model

49
00:04:06.240 --> 00:04:11.030
assume that we will be providing
multiple images as the input, right?

50
00:04:11.030 --> 00:04:15.220
But if I want just to provide one,
I still need to create a list with only

51
00:04:15.220 --> 00:04:18.240
one image inside so
that's what I'm doing here.

52
00:04:18.240 --> 00:04:23.479
And preprocess input
is probably just doing

53
00:04:23.479 --> 00:04:29.392
the, I'm actually not exactly
sure what is the doing,

54
00:04:29.392 --> 00:04:33.190
preprocessing, preprocessing,
I don't think it was defined anywhere.

55
00:04:33.190 --> 00:04:38.419
The main part of this code is just simply
converting our image to this resolution,

56
00:04:39.470 --> 00:04:42.000
preprocesin input is part of the VGG.

57
00:04:42.000 --> 00:04:46.210
So it's basically doing
conversion to whatever

58
00:04:46.210 --> 00:04:49.540
this model expect the preprocesin
to the image to be done.

59
00:04:50.750 --> 00:04:53.410
And it's now we can actually
do the predictions.

60
00:04:53.410 --> 00:04:59.284
So in our x if we just print the x, no x.

61
00:04:59.284 --> 00:05:03.452
So you can see it's just the array with

62
00:05:03.452 --> 00:05:08.440
each of those three numbers,

63
00:05:08.440 --> 00:05:14.410
that's the RGB channel of each individual
pixel, and I have tons of them.

64
00:05:14.410 --> 00:05:18.580
So what they can do I can just
say model do the prediction and

65
00:05:18.580 --> 00:05:22.830
provide this list of
continue of our image.

66
00:05:22.830 --> 00:05:26.170
And print out what our model predicted.

67
00:05:27.760 --> 00:05:30.372
So it predicted with 90%,

68
00:05:30.372 --> 00:05:36.227
almost 91% accuracy that we're
looking at African elephant.

69
00:05:36.227 --> 00:05:42.606
ABout 8%, almost 9% it's a Tusker,
and it's India elephant about 0.4%.

70
00:05:42.606 --> 00:05:48.640
Pretty accurate prediction, but what
is interesting is next what I gonna do.

71
00:05:48.640 --> 00:05:52.570
So remember, we're choosing
between a 1,000 classes, right?

72
00:05:52.570 --> 00:05:59.360
And then it kinda curious what was
the number of the neuron activated,

73
00:05:59.360 --> 00:06:04.900
which correspond to African elephant.

74
00:06:04.900 --> 00:06:09.680
And I can simply just look at the index

75
00:06:11.830 --> 00:06:15.920
of this neuron and it was 386,000, right?

76
00:06:17.470 --> 00:06:21.720
So why am I caring about this index?

77
00:06:21.720 --> 00:06:26.340
Well, it's basically what
I will be doing next,

78
00:06:26.340 --> 00:06:30.950
I will try to look at
the last convolutional layer.

79
00:06:30.950 --> 00:06:35.556
And we'll simply try to find
all the activated connections,

80
00:06:35.556 --> 00:06:42.120
which activated this neuron,
300 whatever the number was, 386, right?

81
00:06:42.120 --> 00:06:45.730
I can just see which of

82
00:06:45.730 --> 00:06:49.980
the location of the neurons which
activated that last neuron.

83
00:06:49.980 --> 00:06:54.630
It basically allows me to do
the predictions which area of

84
00:06:54.630 --> 00:06:59.340
the image my neuron network was looking at

85
00:06:59.340 --> 00:07:02.340
when doing the prediction that
we looking at the elephant.

86
00:07:03.380 --> 00:07:08.100
So to some degree I'm trying
to visualize where my narrow

87
00:07:08.100 --> 00:07:12.050
convolution neural network is paying
attention while doing the prediction.

88
00:07:12.050 --> 00:07:16.546
So I said that,
neuron networks are black boxes, right?

89
00:07:16.546 --> 00:07:20.630
And the way it weights and
biases are adjusted, it is a mystery,

90
00:07:20.630 --> 00:07:25.080
to some degree it is and a lot depends
on the initial way distributions, right?

91
00:07:25.080 --> 00:07:29.900
And for instance, if you initialize
your model with one random numbers, but

92
00:07:29.900 --> 00:07:31.730
then reinitialize them.

93
00:07:31.730 --> 00:07:38.450
You might get completely different
numbers of weights and biases, right?

94
00:07:38.450 --> 00:07:43.420
But with this thing, at least I can
visualize or get some understanding

95
00:07:43.420 --> 00:07:47.380
where the neural network is looking
at from the making a prediction.

96
00:07:47.380 --> 00:07:50.610
And that's that's what I'm doing here.

97
00:07:50.610 --> 00:07:55.221
I'm just grabbing the last
convolutional layer so

98
00:07:55.221 --> 00:07:59.508
block5_conv3, if we look at the topology,

99
00:07:59.508 --> 00:08:04.775
it's right before our fully
connected neuron networks.

100
00:08:04.775 --> 00:08:11.760
Before the MaxPooling, Block5_conv3,
that's our lost convolutional layer.

101
00:08:11.760 --> 00:08:16.855
And again, What I can see,
I can see at all

102
00:08:16.855 --> 00:08:24.579
the gradients which initialized
my African elephant output.

103
00:08:24.579 --> 00:08:27.490
So model output,
that's our last layer, right?

104
00:08:27.490 --> 00:08:30.850
So and
we just looking at this last neuron, and

105
00:08:30.850 --> 00:08:35.520
we can find all of those
activations where they happen.

106
00:08:35.520 --> 00:08:40.280
So, it's kinda just looking
at those filters and

107
00:08:40.280 --> 00:08:45.660
in our last convolutional layer,
it was 7 by 7 pixels.

108
00:08:45.660 --> 00:08:51.090
That was the result of all the previous
convolutions and we can see which

109
00:08:51.090 --> 00:08:57.550
of those particular 7 by 7 pixels were the
most active when they did the prediction.

110
00:08:57.550 --> 00:08:59.400
That we're looking at the elephant.

111
00:09:00.670 --> 00:09:05.585
And I can build the,
I'm not sure if I already executed this

112
00:09:05.585 --> 00:09:09.631
block of code, so
let's just run it once again.

113
00:09:09.631 --> 00:09:13.796
Yeah, I can visualize the heatmap now.

114
00:09:13.796 --> 00:09:18.998
So heatmap just saying here that
this region was the one which

115
00:09:18.998 --> 00:09:25.180
resulted into the prediction that
I'm looking at the elephant.

116
00:09:25.180 --> 00:09:28.610
And it doesn't really give me
a lot of useful information.

117
00:09:28.610 --> 00:09:32.670
So I can actually overlay this heatmap
on top of the original image to

118
00:09:32.670 --> 00:09:34.799
actually visualize what is happening.

119
00:09:38.250 --> 00:09:42.800
So apparently the prediction was done
because we're looking at the heads of

120
00:09:42.800 --> 00:09:47.925
the elephant and
Tosca yes of the mother elephant.

121
00:09:47.925 --> 00:09:53.040
[LAUGH] And the thing is, you can play
with this pretty much with any picture,

122
00:09:53.040 --> 00:09:57.640
do you want to try something,
you just for fun?

123
00:09:57.640 --> 00:09:58.580
&gt;&gt; I have a question.

124
00:09:58.580 --> 00:10:02.320
&gt;&gt; Yes.
&gt;&gt; So I know that you said that we already

125
00:10:02.320 --> 00:10:06.689
trained the model like on elephants,
I assume.

126
00:10:06.689 --> 00:10:07.278
&gt;&gt; Yes.

127
00:10:07.278 --> 00:10:12.582
&gt;&gt; And then it was expecting an array of
at least three different assume elephants,

128
00:10:12.582 --> 00:10:14.480
some sort of images.

129
00:10:14.480 --> 00:10:17.160
Why did we do the one
image in this example?

130
00:10:17.160 --> 00:10:19.570
Was it just for
ease in this specific case?

131
00:10:19.570 --> 00:10:24.410
Sorry, why didn't we do three creative
commons, different elephants?

132
00:10:24.410 --> 00:10:30.600
&gt;&gt; So I think you asking about
the prediction when we done

133
00:10:30.600 --> 00:10:36.390
that there is African wlephant,
there's tusker and Indian elephant.

134
00:10:36.390 --> 00:10:39.310
&gt;&gt; Towards the beginning, your input to.

135
00:10:40.390 --> 00:10:43.150
&gt;&gt; My input, you mean just this image?

136
00:10:43.150 --> 00:10:46.750
&gt;&gt; Yeah.
You said that it was expecting an array of

137
00:10:46.750 --> 00:10:48.220
at least three different images.

138
00:10:48.220 --> 00:10:50.820
&gt;&gt; No.
&gt;&gt; And then, we just copied the image.

139
00:10:52.220 --> 00:10:57.320
&gt;&gt; So, the way how our model

140
00:10:57.320 --> 00:11:02.500
usually trained with those VGG
systems they using mini batches.

141
00:11:02.500 --> 00:11:07.800
So, if you look at the input layer,
dimensionality is the following,

142
00:11:07.800 --> 00:11:09.760
none which just correspond to

143
00:11:10.960 --> 00:11:16.290
different number from 1 to
as many as you want images.

144
00:11:16.290 --> 00:11:22.650
And those images all supposed to
have 224 by 224 with 3 channels.

145
00:11:22.650 --> 00:11:27.765
So, just because we have
this non it means that our

146
00:11:27.765 --> 00:11:30.425
model expecting the list of images.

147
00:11:30.425 --> 00:11:36.035
And the reason why this is common and
you will see it with usually pre trained

148
00:11:36.035 --> 00:11:41.465
models is they using technique
called mini batching training.

149
00:11:41.465 --> 00:11:46.480
And basically,
when you're calculating the gradients,

150
00:11:46.480 --> 00:11:49.360
kind of how you need to change
those weights and biases.

151
00:11:49.360 --> 00:11:54.970
You're not doing that only by looking at
the single image, but you look at multiple

152
00:11:54.970 --> 00:11:59.880
images and calculating what should be
the average change of your weights and

153
00:11:59.880 --> 00:12:04.790
biases to avoid kind of noisy

154
00:12:04.790 --> 00:12:09.310
change of your weights and biases.

155
00:12:09.310 --> 00:12:15.370
So just can reduce the noisiness
of those changes and

156
00:12:15.370 --> 00:12:17.630
get more stability of the training.

157
00:12:17.630 --> 00:12:19.410
We introducing those mini batches and

158
00:12:19.410 --> 00:12:22.720
there is another benefits when
you do a mini batch training.

159
00:12:22.720 --> 00:12:26.950
It can actually utilize vectorization,

160
00:12:26.950 --> 00:12:32.398
it means that you can use CMD
instructions which you can skip explain.

161
00:12:32.398 --> 00:12:37.820
[LAUGH] But it can significantly
improve the performance and

162
00:12:37.820 --> 00:12:40.170
when I'm talking about
performance in this case.

163
00:12:40.170 --> 00:12:44.860
It's not the accuracy of the model, but
it's just computational performance.

164
00:12:44.860 --> 00:12:49.710
Just because you can do
the operations on individual images

165
00:12:49.710 --> 00:12:53.520
simultaneously in parallel
with those CMD instructions.

166
00:12:53.520 --> 00:13:00.488
So mini batching is a good way to get
to a more general solution, right?

167
00:13:00.488 --> 00:13:02.811
Just because you avoid too many noises,

168
00:13:02.811 --> 00:13:07.537
because let's say you provide only one
image of elephant when you train, right?

169
00:13:07.537 --> 00:13:13.390
And your whole model will kind of move
toward that recognizing one elephant,

170
00:13:13.390 --> 00:13:14.030
right?

171
00:13:14.030 --> 00:13:17.780
But what about other 999 classes?

172
00:13:17.780 --> 00:13:23.610
We most probably gonna decrease the weight
which correspond to those classes.

173
00:13:23.610 --> 00:13:28.490
So to avoid this type of problem, we're
processing several images simultaneously.

174
00:13:28.490 --> 00:13:33.420
And mini batch exercise can be anything
from 16 images to thousand images

175
00:13:33.420 --> 00:13:34.680
simultaneously.

176
00:13:34.680 --> 00:13:39.941
And it will simply look at what's the best
way to satisfy all of those classes.

177
00:13:39.941 --> 00:13:44.381
And mini batching is highly recommended
if you have a lot of classes like

178
00:13:44.381 --> 00:13:48.970
in this case, instead of 10, digit,
10 classes we have thousands.

179
00:13:48.970 --> 00:13:54.409
So to have, get a better distribution
of weights and bias modification,

180
00:13:54.409 --> 00:13:58.632
it's basically that's why
we're using mini batches.

181
00:13:58.632 --> 00:14:02.052
And we need to provide list of images
to our model when we training and

182
00:14:02.052 --> 00:14:03.200
also when we predicted

