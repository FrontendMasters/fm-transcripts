WEBVTT

1
00:00:00.482 --> 00:00:04.870
Okay, so we pretty much done with
the image processing at this point, right?

2
00:00:04.870 --> 00:00:09.477
And we saw how a couple of interesting
things about convolutional neural

3
00:00:09.477 --> 00:00:12.575
networks and
fully connected neural networks,

4
00:00:12.575 --> 00:00:16.371
and even how to try to explain
what the neural network done.

5
00:00:16.371 --> 00:00:20.785
But images,
it's kind of unique data source

6
00:00:20.785 --> 00:00:25.910
because you have a particular resolution,
right?

7
00:00:25.910 --> 00:00:30.450
And you can rescale, and
it's already in digital form, right?

8
00:00:30.450 --> 00:00:32.330
In pretty convenient digital form.

9
00:00:32.330 --> 00:00:34.880
Intensity of the pixel is just basically,

10
00:00:34.880 --> 00:00:38.890
this integer number which can easily
converted to point if you want them.

11
00:00:38.890 --> 00:00:42.316
And that can be provided as
the input to your neural network.

12
00:00:42.316 --> 00:00:48.798
And somewhat's different with text,
so when you have letters,

13
00:00:51.923 --> 00:00:58.305
Of course, you can just simply take
the character representation, right?

14
00:00:58.305 --> 00:01:04.023
But those characters does not
directly associated with the words.

15
00:01:04.023 --> 00:01:08.479
So the one how you can
actually converts text to

16
00:01:08.479 --> 00:01:13.620
numbers which can be used to
train your neural network

17
00:01:13.620 --> 00:01:18.315
is to simply create
the vocabulary of the words.

18
00:01:18.315 --> 00:01:22.242
And actually,
if you want to play with the datasets,

19
00:01:22.242 --> 00:01:27.062
the one is already provided as
part of the keras datasets.

20
00:01:27.062 --> 00:01:32.719
IMDB dataset,
it's just the reviews from IMDB,

21
00:01:32.719 --> 00:01:37.304
well famous movie, reviews database.

22
00:01:37.304 --> 00:01:43.420
And simply if you try to visualize
what's training looks like,

23
00:01:43.420 --> 00:01:46.720
just those list of integers.

24
00:01:46.720 --> 00:01:50.870
And the thing is, you can even specify
how many unique words you want to have.

25
00:01:50.870 --> 00:01:54.114
In this case, 10,000 unique words, and

26
00:01:54.114 --> 00:01:59.722
basically those integer numbers correspond
to a particular word in our reviews.

27
00:01:59.722 --> 00:02:05.122
And training labels,
so if I just let's say,

28
00:02:05.122 --> 00:02:11.089
print not the training data,
but training label.

29
00:02:15.712 --> 00:02:20.721
Labels, it will simply tell you 1 or 0,

30
00:02:20.721 --> 00:02:28.170
1 being positive review and
0 being negative review.

31
00:02:28.170 --> 00:02:31.790
And the thing how you can do,
what you can do, by the way,

32
00:02:31.790 --> 00:02:37.790
if you convert those, so
let me go back to data quickly.

33
00:02:37.790 --> 00:02:42.670
So if you just convert those indexes
of words back to the vocabulary,

34
00:02:42.670 --> 00:02:44.870
and that's I'm showing you how to do this.

35
00:02:44.870 --> 00:02:47.464
You're just getting the word indexes,
right?

36
00:02:47.464 --> 00:02:51.316
And kind of transforming the dictionary
to get from value to key,

37
00:02:51.316 --> 00:02:55.440
you basically reforming dictionary
to get key from the value.

38
00:02:55.440 --> 00:02:58.610
And you can print out for
instance, first review, so

39
00:02:58.610 --> 00:03:01.550
this field was just brilliant casting,
da da da da da.

40
00:03:01.550 --> 00:03:05.590
So that's the dataset but explicit,
but for our neural networks,

41
00:03:05.590 --> 00:03:08.080
we would like those numbers, right?

42
00:03:08.080 --> 00:03:13.420
And you can even see the whole can at
least have words and corresponding

43
00:03:13.420 --> 00:03:18.290
index and those indexes, they are sorted.

44
00:03:18.290 --> 00:03:23.950
So the most common word
will have the lowest index.

45
00:03:23.950 --> 00:03:28.694
So having those indexes and those words,

46
00:03:28.694 --> 00:03:36.023
the next step is to basically
vectorize your sequence, right?

47
00:03:36.023 --> 00:03:42.720
Vectorization, let me
quickly just go through it.

48
00:03:42.720 --> 00:03:47.905
Vectorization means that we'll just
look at all of the indixes, create one

49
00:03:47.905 --> 00:03:53.748
list with, remember, we specified that
we won't have 10,000 unique words.

50
00:03:53.748 --> 00:04:01.292
And then we will just put the In
this 10,000 integer sequence,

51
00:04:01.292 --> 00:04:07.420
we will just flip those where
we have the warts located.

52
00:04:07.420 --> 00:04:13.380
So let's say if we had that,

53
00:04:13.380 --> 00:04:20.240
I think the first index correspond to or
that, there's ants and third one is A.

54
00:04:20.240 --> 00:04:24.046
So if we had the word the, the article,

55
00:04:24.046 --> 00:04:29.150
the, and ant in our article those
corresponding beats will be flipped.

56
00:04:29.150 --> 00:04:32.730
And that will be the input
to our neural network.

57
00:04:32.730 --> 00:04:37.930
So the simplest one if you
use this hot encoding,

58
00:04:37.930 --> 00:04:43.730
that's what vectorization technique is
used for, you can use the dense layer and

59
00:04:43.730 --> 00:04:47.720
so that your neural network
actually can be really simple.

60
00:04:47.720 --> 00:04:50.812
For instance, in this case,
I'm just using 16 neurons in dense layer,

61
00:04:50.812 --> 00:04:52.396
connecting another 16 [INAUDIBLE].

62
00:04:52.396 --> 00:04:57.387
[INAUDIBLE] just because I'm trying to
figure out if the review was positive or

63
00:04:57.387 --> 00:05:01.456
negative, only one neuron is
enough to actually activate it or

64
00:05:01.456 --> 00:05:05.831
not activate, so it will be activated
if the review was positive and

65
00:05:05.831 --> 00:05:08.900
that is what I'm gonna be training it.

66
00:05:08.900 --> 00:05:13.761
So let's just quickly split our

67
00:05:13.761 --> 00:05:19.349
dataset into training and testing.

68
00:05:19.349 --> 00:05:24.959
And we can start streaming, let's see
how much each of them are two seconds,

69
00:05:24.959 --> 00:05:27.389
yeah, it should not be that bad.

70
00:05:31.260 --> 00:05:36.690
So you can see that from
the beginning I am getting about 82%

71
00:05:36.690 --> 00:05:44.300
prediction on the validation dataset,
which is not bad, it's not bad at all.

72
00:05:44.300 --> 00:05:50.450
And we can get up to 90%, I think.

73
00:05:50.450 --> 00:05:56.540
So later on, I can use this
trained model to provide any,

74
00:05:58.050 --> 00:06:03.780
yeah, let's finish the the training
session because I wanna visualize how our

75
00:06:03.780 --> 00:06:09.970
accuracy was changing for the training
data set and for the validation data set.

76
00:06:09.970 --> 00:06:13.270
So remember,
I can take a look at the history, right?

77
00:06:13.270 --> 00:06:17.380
And in the history, it's a dictionary
with loss binary accuracy,

78
00:06:17.380 --> 00:06:20.490
validation loss, and
validation binary accuracy.

79
00:06:21.695 --> 00:06:24.340
We'll use and
I can simply visualize those.

80
00:06:26.160 --> 00:06:30.244
So you see that the training
accuracy was increasing, right?

81
00:06:30.244 --> 00:06:34.414
And almost converge to a 100%,
but at the same time accuracy for

82
00:06:34.414 --> 00:06:38.820
the validation data set increased
initially and then start dropping.

83
00:06:39.960 --> 00:06:43.524
So that's the example of overfitting and
you should definitely avoid it.

84
00:06:43.524 --> 00:06:48.171
So what to do in this case is
probably stop my training at around,

85
00:06:48.171 --> 00:06:53.255
I didn't know four epochs, and
that's gonna be good enough.

86
00:06:53.255 --> 00:06:55.833
So I can simply redo this, right?

87
00:06:55.833 --> 00:06:59.141
So reinitialize the model and
say, train my model only for

88
00:06:59.141 --> 00:07:02.975
four epochs and
that should be done pretty quickly.

89
00:07:02.975 --> 00:07:07.645
And if I look at the results, so

90
00:07:07.645 --> 00:07:11.885
model evolution will just simply,
if you provide the test, it will give you

91
00:07:13.400 --> 00:07:17.740
the value of the loss function and
the final accuracy.

92
00:07:17.740 --> 00:07:21.230
So up to 88%, not bad.

93
00:07:21.230 --> 00:07:27.350
All right, so that's how you
basically process the text and

94
00:07:27.350 --> 00:07:32.460
one way of the processing the text
is just grading this hot encoding,

95
00:07:32.460 --> 00:07:39.582
the problem with this is
that I'm creating 10,000

96
00:07:39.582 --> 00:07:45.960
element array for each kind of input line.

97
00:07:45.960 --> 00:07:51.450
It means that I'm basically sacrificing
a lot of memory to prepare this data,

98
00:07:51.450 --> 00:07:54.120
because I'm kinda switching
to this representation.

99
00:07:54.120 --> 00:07:58.860
But it's really easy to feed
this into my model afterwards.

100
00:07:58.860 --> 00:08:04.630
So the trade off is basically, yeah,
[LAUGH] we're spending more memory but

101
00:08:04.630 --> 00:08:08.640
it's gonna be easier to
process the data afterwards.

102
00:08:08.640 --> 00:08:13.706
Alternative way to prepare
your words instead of playing

103
00:08:13.706 --> 00:08:17.692
with indexing and creating this hot-
&gt;&gt; I have a question.

104
00:08:17.692 --> 00:08:21.474
&gt;&gt; Yes?
&gt;&gt; So you said in this example that you

105
00:08:21.474 --> 00:08:25.858
overfed the data and so the validation-
&gt;&gt; Dropped.

106
00:08:25.858 --> 00:08:29.589
&gt;&gt; Yeah, dropped,
with this be an example of,

107
00:08:29.589 --> 00:08:32.640
I guess we went to the end of the demo.

108
00:08:32.640 --> 00:08:36.110
But would this be invalid data,

109
00:08:36.110 --> 00:08:42.010
would you rerun this with a different
input so you wouldn't see that drop or

110
00:08:42.010 --> 00:08:47.250
is this-
&gt;&gt; So basically, what happens when I rerun

111
00:08:47.250 --> 00:08:52.520
it, I should return back.

112
00:08:52.520 --> 00:08:56.540
So I was running this, the original,
the first one, I was running for

113
00:08:56.540 --> 00:08:58.520
20 epochs, right?

114
00:08:58.520 --> 00:09:03.470
So I get to some like
almost 90% accuracy and

115
00:09:03.470 --> 00:09:09.960
then the accuracy stop dropping to
something like 85% for instance, right?

116
00:09:09.960 --> 00:09:15.680
So with this what I did, I simply rerun
my model but only used four epochs.

117
00:09:15.680 --> 00:09:23.163
So I basically stopped as soon as we
get kinda to this point right here.

118
00:09:23.163 --> 00:09:27.133
&gt;&gt; So you knew that this was gonna
happen so the rest of the example-

119
00:09:27.133 --> 00:09:27.817
&gt;&gt; Exactly, so

120
00:09:27.817 --> 00:09:30.063
I'm just simply ignore the overfitting.

