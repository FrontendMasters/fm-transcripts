WEBVTT

1
00:00:00.150 --> 00:00:02.507
Now let's actually debug our code.

2
00:00:02.507 --> 00:00:06.231
NoneType and float, I think I didn't
initialize something properly again.

3
00:00:06.231 --> 00:00:10.566
Okay, callbacks, callbacks,
callbacks we define as,

4
00:00:12.340 --> 00:00:16.974
Hm, all right, that should be working.

5
00:00:16.974 --> 00:00:19.413
And then callback.

6
00:00:21.732 --> 00:00:25.351
Log, accuracy, all right.

7
00:00:31.107 --> 00:00:34.179
There's no mistake.

8
00:00:34.179 --> 00:00:38.890
One second, okay, what if we, for

9
00:00:38.890 --> 00:00:42.984
now, just ignore callbacks.

10
00:00:42.984 --> 00:00:46.378
Let me just make sure that it's
actually callback, which, no,

11
00:00:46.378 --> 00:00:48.919
it's not the callback,
it's something else.

12
00:00:48.919 --> 00:00:55.384
Shape mismatch, the mismatch shape label,
I see, I see, I see, okay, so.

13
00:00:57.155 --> 00:01:02.541
&gt;&gt; I don't get the same error if we
add back in the flatten on the dense.

14
00:01:02.541 --> 00:01:04.874
&gt;&gt; You're getting the same layers or
different one?

15
00:01:04.874 --> 00:01:05.860
The same?

16
00:01:05.860 --> 00:01:09.262
&gt;&gt; The same,
cuz you deleted the two lines.

17
00:01:09.262 --> 00:01:09.972
&gt;&gt; Yeah.

18
00:01:09.972 --> 00:01:11.857
&gt;&gt; I did not get the same.

19
00:01:11.857 --> 00:01:15.355
Because I got the same error
initially when I deleted them, and

20
00:01:15.355 --> 00:01:18.391
then when I added them back,
I no longer got the error.

21
00:01:23.886 --> 00:01:24.386
&gt;&gt; Let's see.

22
00:01:26.465 --> 00:01:34.179
So the error happening after
the first epoch training is done.

23
00:01:36.144 --> 00:01:38.076
Or not, let's see.

24
00:01:41.124 --> 00:01:43.258
No, okay.

25
00:01:43.258 --> 00:01:45.090
So let's see, what we're doing here.

26
00:01:45.090 --> 00:01:50.274
So we did convolutions first.

27
00:01:50.274 --> 00:01:56.297
We're doing pooling, and
we always need to flatten, exactly.

28
00:01:56.297 --> 00:02:00.533
So basically,
our convolutional operations,

29
00:02:00.533 --> 00:02:05.193
right, Conv2D,
that's what 2D is standing for.

30
00:02:05.193 --> 00:02:09.354
[LAUGH] We have 2D images
provided as the input, right, and

31
00:02:09.354 --> 00:02:12.611
that's what we even have
in the input shapes.

32
00:02:12.611 --> 00:02:19.933
And our max pooling 2D,
also operating on 2D, well, images.

33
00:02:19.933 --> 00:02:24.193
But then, to our dense layer,
we need to flatten everything, right,

34
00:02:24.193 --> 00:02:25.897
to have these connections.

35
00:02:25.897 --> 00:02:29.197
So we can probably get rid of this.

36
00:02:29.197 --> 00:02:31.214
But we need to leave flatten so

37
00:02:31.214 --> 00:02:36.231
that the operations from the pooling
can be then fed into our dense layer.

38
00:02:36.231 --> 00:02:39.376
And this will work.

39
00:02:39.376 --> 00:02:45.243
So it will take about three
minutes to train our simple model.

40
00:02:45.243 --> 00:02:49.206
And quite often,
after pooling the operations,

41
00:02:49.206 --> 00:02:53.653
you want to have couple of
more convolutions, as well.

42
00:02:53.653 --> 00:02:58.866
And the idea, as I said, so
in the first convolution layer,

43
00:02:58.866 --> 00:03:04.388
just because our filters are pretty small,
only 3x3 pixels,

44
00:03:04.388 --> 00:03:09.502
it will be learning some really,
really simple features,

45
00:03:09.502 --> 00:03:14.849
which can be represented in
those small 3x3 pixels images.

46
00:03:14.849 --> 00:03:18.342
So, for instance, it can be diagonal line,
right, vertical line.

47
00:03:18.342 --> 00:03:19.217
Or, for instance,

48
00:03:19.217 --> 00:03:22.316
if you're working with different colors,
it can be a specific color.

49
00:03:22.316 --> 00:03:27.239
So we're paying attention more to
red channel, for instance, right?

50
00:03:27.239 --> 00:03:31.960
But then, as soon as you do pooling,
it means that your image shrink.

51
00:03:31.960 --> 00:03:36.256
And if you apply a new convolutional
layer, even though, for

52
00:03:36.256 --> 00:03:40.560
instance, you will be using
another layer of 3x3 filters.

53
00:03:40.560 --> 00:03:45.452
But now, if you think about it,
your each filter will

54
00:03:45.452 --> 00:03:50.682
have access to 6x6 original pixels,
does make sense?

55
00:03:50.682 --> 00:03:56.357
So, during pooling,
we simply shrink the image, right?

56
00:03:56.357 --> 00:04:01.193
But the information, just because
we kept the most intensive pixels,

57
00:04:01.193 --> 00:04:04.105
we kinda think that it will be propagated.

58
00:04:04.105 --> 00:04:06.060
The image will be smaller,

59
00:04:06.060 --> 00:04:10.228
but the most important bits and
pieces will be preserved.

60
00:04:10.228 --> 00:04:16.362
And then the next convolutional layer will
have kinda slightly bigger point of view,

61
00:04:16.362 --> 00:04:20.547
although it will be processing
still those 3x3 pixels.

62
00:04:20.547 --> 00:04:25.230
So what we can do, let's see,
with convolutional neural network,

63
00:04:25.230 --> 00:04:30.253
we actually get slightly better accuracy,
even on validation data set.

64
00:04:30.253 --> 00:04:36.071
Remember that we were getting to about
97% with fully connected neural network.

65
00:04:36.071 --> 00:04:39.113
With this one,
it's actually significantly smaller.

66
00:04:39.113 --> 00:04:45.368
So, for instance, if we just print model,

67
00:04:45.368 --> 00:04:52.860
I need to change the Cell to Code and
type model.summary.

68
00:04:52.860 --> 00:05:01.881
Basically, to preserve the information
from those 32 filters,

69
00:05:01.881 --> 00:05:08.222
we only needed 320 floating point numbers.

70
00:05:08.222 --> 00:05:14.358
And our dense layer with 128, so it was,
well, significantly larger, right?

71
00:05:14.358 --> 00:05:18.103
So with convolutions, we can save space.

72
00:05:18.103 --> 00:05:21.507
And also, due to those convolutions,

73
00:05:21.507 --> 00:05:27.596
because we kind of working on pixels
sitting right next to each other,

74
00:05:27.596 --> 00:05:34.530
we're taking into account the information
about kinda spatial information.

75
00:05:34.530 --> 00:05:37.105
So if we are looking at
the object remember,

76
00:05:37.105 --> 00:05:41.083
with a fully connected neural network
we have to flatten our image.

77
00:05:41.083 --> 00:05:45.692
It means that we kind of
don't know which pixels were

78
00:05:45.692 --> 00:05:50.102
sitting right next to each
other in the next rows.

79
00:05:50.102 --> 00:05:54.205
And convolutional operations,
actually taking this into account.

80
00:05:54.205 --> 00:05:56.443
Just because this filter, 3x3,

81
00:05:56.443 --> 00:06:01.453
will be looking at those 3x3 pixels on
the original image, if it makes sense.

82
00:06:01.453 --> 00:06:03.453
All right, so let's do the following.

83
00:06:03.453 --> 00:06:08.693
I already commented out our
fully connected neural network.

84
00:06:08.693 --> 00:06:13.810
I can actually put several
convolutional layers,

85
00:06:13.810 --> 00:06:17.383
I don't need input shape anymore.

86
00:06:17.383 --> 00:06:19.954
It will be automatically tracked.

87
00:06:19.954 --> 00:06:23.931
And for fun,
let's actually do it once again.

88
00:06:23.931 --> 00:06:27.642
So what I will have,
I will have convolution pooling,

89
00:06:27.642 --> 00:06:31.931
convolution pooling,
convolution pooling, layer, okay.

90
00:06:31.931 --> 00:06:37.693
And let's see how quickly
we can train our model and

91
00:06:37.693 --> 00:06:40.644
how big the model will be.

92
00:06:40.644 --> 00:06:45.971
It seems, well, it's 60,000 images, hm.

93
00:06:45.971 --> 00:06:49.445
&gt;&gt; So what does it do when
you do the convolutions and

94
00:06:49.445 --> 00:06:51.661
the poolings multiple times?

95
00:06:51.661 --> 00:06:57.035
&gt;&gt; So, okay,
I think I know how to explain,

96
00:06:57.035 --> 00:07:02.268
but I need to go back to my drawing board.

97
00:07:02.268 --> 00:07:09.235
So that's kind of the basic
operation of the convolutions.

98
00:07:09.235 --> 00:07:14.944
But if we take a look at
the multiple convolutions pooling,

99
00:07:14.944 --> 00:07:19.284
convolution pooling, our original image,

100
00:07:19.284 --> 00:07:24.423
28x28, after convolutions and
after pooling,

101
00:07:24.423 --> 00:07:29.694
will be reduced to something like 14x14,
right?

102
00:07:29.694 --> 00:07:35.048
And then, for instance,
if I apply another 3x3 convolutions,

103
00:07:35.048 --> 00:07:39.842
it will be now looking at
particular features in this area.

104
00:07:39.842 --> 00:07:44.740
But this area on the smaller image will

105
00:07:44.740 --> 00:07:49.342
correspond to 6x6 original one.

106
00:07:49.342 --> 00:07:54.280
So that's after convolutions and pooling.

107
00:07:54.280 --> 00:07:58.718
After next convolution and pooling,

108
00:07:58.718 --> 00:08:02.051
now the image will be 7x7.

109
00:08:02.051 --> 00:08:09.436
And if we look at the 3x3,
it almost correspond to half the image.

110
00:08:09.436 --> 00:08:14.664
So what's happening, the convolutions,
weights which correspond

111
00:08:14.664 --> 00:08:19.459
to the filters on the lower level,
will learn bigger features.

112
00:08:19.459 --> 00:08:23.780
So, for instance, if on the smaller scale,

113
00:08:23.780 --> 00:08:28.670
kind of on the first level,
we learned where we have

114
00:08:28.670 --> 00:08:33.360
diagonal lines,
where we have vertical lines.

115
00:08:33.360 --> 00:08:39.605
For instance, on the middle layer,
we can learn something, I'm sorry.

116
00:08:41.444 --> 00:08:46.218
Filters can, for instance, look at,

117
00:08:46.218 --> 00:08:49.998
I know, almost like circles.

118
00:08:49.998 --> 00:08:54.902
Right, or maybe not the full circle.

119
00:08:54.902 --> 00:09:00.237
Because probably on the last layer,
it will learn something like this.

120
00:09:00.237 --> 00:09:05.001
So what I'm trying to say is that,
imagine how much information or

121
00:09:05.001 --> 00:09:08.829
how many useful things you
can see from the image if you

122
00:09:08.829 --> 00:09:13.535
just divide it in small parts of
3x3 pixels, maybe not so much.

123
00:09:13.535 --> 00:09:14.288
But still,

124
00:09:14.288 --> 00:09:19.195
something interesting can be found there
where you have kind of those lines.

125
00:09:19.195 --> 00:09:24.183
On the next convolutional layer, you will
just combine where you observed those

126
00:09:24.183 --> 00:09:27.557
vertical lines,
where you saw horizontal lines, and

127
00:09:27.557 --> 00:09:30.880
a combination of those
features will be propagated.

128
00:09:30.880 --> 00:09:35.251
For handwritten digits,
it's kind of difficult to explain.

129
00:09:35.251 --> 00:09:38.233
It's better to explain, for
instance, with face recognition.

130
00:09:38.233 --> 00:09:44.215
So let's say we have a image or
a picture of some person, his face, right?

131
00:09:44.215 --> 00:09:48.951
And what can happen on the lower
convolutions, we might find, once again,

132
00:09:48.951 --> 00:09:51.693
those horizontal lines, vertical lines.

133
00:09:51.693 --> 00:09:58.246
And maybe on the next convolutional layer,
we will combine those lines and

134
00:09:58.246 --> 00:10:02.199
say, okay, having two horizontal lines and

135
00:10:02.199 --> 00:10:09.083
maybe a couple of diagonal lines,
most probably we're looking at human eye.

136
00:10:09.083 --> 00:10:13.263
Okay, so probably somewhere on the image,
where convolution will happen,

137
00:10:13.263 --> 00:10:16.430
that's gonna be one eye,
there's gonna be another eye.

138
00:10:16.430 --> 00:10:20.833
And then if we have a couple of vertical
lines, most probably that's the nose and

139
00:10:20.833 --> 00:10:21.883
so on and so forth.

140
00:10:21.883 --> 00:10:26.456
And then on the last layer, it will
take into account all of those kind of

141
00:10:26.456 --> 00:10:31.640
recognized horizontal, vertical lines and
say, okay, since we have those two

142
00:10:31.640 --> 00:10:36.697
eyes together right next to the nose,
we're definitely looking at the face.

143
00:10:36.697 --> 00:10:41.555
So, think about it like features,
those filters are filters.

144
00:10:41.555 --> 00:10:46.320
We're looking for something really
simple at the lower levels, right.

145
00:10:46.320 --> 00:10:48.759
Remember with the example of style.

146
00:10:48.759 --> 00:10:54.586
Think about it like this as the simplest

147
00:10:54.586 --> 00:10:59.717
styles, like the way how we draw.

148
00:10:59.717 --> 00:11:03.612
And then on the next
layers of convolutions,

149
00:11:03.612 --> 00:11:10.389
we are looking at combinations of those
small details into something bigger.

150
00:11:10.389 --> 00:11:15.341
And then combinations of those things,
for instance, like with face, yes,

151
00:11:15.341 --> 00:11:17.604
we recognize eye, eye, and nose.

152
00:11:17.604 --> 00:11:22.209
So now we can say okay, we have those,
then probably we're looking at the face in

153
00:11:22.209 --> 00:11:26.034
this area, right, and
that will be recognized by the next layer.

154
00:11:26.034 --> 00:11:30.391
And then, maybe if you even want to have
multiple faces recognized, right, so

155
00:11:30.391 --> 00:11:33.706
maybe there will be another
layer where we'll find, okay,

156
00:11:33.706 --> 00:11:37.042
there are several faces on the image,
and so on and so forth.

157
00:11:37.042 --> 00:11:41.336
So convolutions is basically doing that,
it's kind of looking for

158
00:11:41.336 --> 00:11:42.852
those small features.

159
00:11:42.852 --> 00:11:44.796
Let's see, have we done our, yeah.

160
00:11:44.796 --> 00:11:47.319
Now it takes five minutes.

161
00:11:47.319 --> 00:11:52.017
We get into 98.6%.

162
00:11:52.017 --> 00:11:56.756
And the thing is,
our model is not taking that much of

163
00:11:56.756 --> 00:12:01.628
a space in the memory,
it's really, really small.

164
00:12:01.628 --> 00:12:05.049
It takes slightly longer to train.

165
00:12:05.049 --> 00:12:09.945
So, basically, there's a trade-off
between how much memory you're using,

166
00:12:09.945 --> 00:12:12.472
right, and how quickly it take to train.

167
00:12:12.472 --> 00:12:15.186
Although it's actually
supposed to be faster,

168
00:12:15.186 --> 00:12:17.375
I think the way how it's implemented.

169
00:12:17.375 --> 00:12:22.070
So TensorFlow 2.0 right
now have some bugs, and

170
00:12:22.070 --> 00:12:26.340
that's been confirmed by Google engineers.

171
00:12:26.340 --> 00:12:29.313
For instance,
if you're using batch training,

172
00:12:29.313 --> 00:12:34.127
that's what I'm doing right here,
it's actually using not the optimized code

173
00:12:34.127 --> 00:12:38.184
from the computational graph,
but rather using eager execution.

174
00:12:38.184 --> 00:12:44.402
I think I can, if I disable eager,
I can make this run significantly faster.

175
00:12:44.402 --> 00:12:49.032
Let's actually try it, tf, no,

176
00:12:49.032 --> 00:12:55.258
I should do it after, and I, let's see,

177
00:12:55.258 --> 00:13:01.489
I don't remember what's the exact command,

178
00:13:01.489 --> 00:13:06.783
but I've used it in attention, yeah.

179
00:13:06.783 --> 00:13:10.667
Mm-hm, right, so basically, remember,

180
00:13:10.667 --> 00:13:15.687
I said that in version 1,
it was the behavior by default.

181
00:13:15.687 --> 00:13:20.899
So to use it, we need to use
compatibility module from v1,

182
00:13:20.899 --> 00:13:23.729
disable_eager_execution.

183
00:13:23.729 --> 00:13:28.255
And our training previously was five
minutes, I'm just kinda curious,

184
00:13:28.255 --> 00:13:33.085
seriously, I don't know if it will help or
not, but we'll see in the second.

185
00:13:33.085 --> 00:13:37.410
No, I think it's still gonna
be running pretty slow, 25.

186
00:13:37.410 --> 00:13:43.616
All right, so with convolutions, up here
actually, yeah, you can pull up the.

187
00:13:45.192 --> 00:13:50.942
Validation, accuracy, and training,
as well, and it will look like this.

188
00:13:50.942 --> 00:13:55.133
So with training,
the more we show the same images,

189
00:13:55.133 --> 00:13:58.580
the better accuracy you will get, right.

190
00:13:58.580 --> 00:14:03.537
But our test training set might
actually get better at the beginning.

191
00:14:03.537 --> 00:14:06.904
But then most probably is
going to be stabilized and

192
00:14:06.904 --> 00:14:11.769
most probably is going to show lower
accuracy than the training data set.

193
00:14:11.769 --> 00:14:15.602
And you need to be pretty accurate, well,

194
00:14:15.602 --> 00:14:20.396
you need to be careful with
the validation accuracy.

195
00:14:20.396 --> 00:14:25.442
Because sometimes, instead of just
stabilizing and being normal,

196
00:14:25.442 --> 00:14:30.669
it might start going down, and
that's the indicator of overfitting.

197
00:14:30.669 --> 00:14:35.731
It means that our model is so
drastically changing

198
00:14:35.731 --> 00:14:41.515
weights to recognize the images
from training data set,

199
00:14:41.515 --> 00:14:46.590
so it completely starts
ignoring something new.

200
00:14:46.590 --> 00:14:49.904
It means that's what the overfitting is,
right?

201
00:14:49.904 --> 00:14:54.872
It basically, just because you're
showing this training data set over and

202
00:14:54.872 --> 00:14:58.363
over to your model,
it's almost like fine tuning for

203
00:14:58.363 --> 00:15:02.028
those specific images, for
those specific features.

204
00:15:02.028 --> 00:15:06.386
And yes, if you will see,
I think I have the example with text,

205
00:15:06.386 --> 00:15:08.207
next I will show it to you.

206
00:15:08.207 --> 00:15:12.567
When this happen, your validation
accuracy will stop dropping.

207
00:15:12.567 --> 00:15:16.107
And the best way is to basically find
where the validation accuracy was

208
00:15:16.107 --> 00:15:19.412
the highest, or the same thing you
will see with a loss function.

209
00:15:19.412 --> 00:15:22.958
Loss function will be decreasing,
it can get to the minimum, and

210
00:15:22.958 --> 00:15:25.161
then will start increasing back again.

211
00:15:25.161 --> 00:15:29.022
It means that usually you need to
stop your training at the point where

212
00:15:29.022 --> 00:15:32.698
the loss function for the validation
data set was at the minimum.

213
00:15:32.698 --> 00:15:36.700
Just to avoid this, as I said,
overfitting for the models.

