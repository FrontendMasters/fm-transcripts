WEBVTT

1
00:00:00.360 --> 00:00:06.199
Does the learning rate make
a difference in how long it takes for

2
00:00:06.199 --> 00:00:13.595
your model to train if you have,
let's say, large, large, large data sets?

3
00:00:13.595 --> 00:00:18.237
&gt;&gt; Yeah, so right now,
those two independent,

4
00:00:18.237 --> 00:00:23.690
my current learning rates and
number of steps.

5
00:00:23.690 --> 00:00:28.610
So how long the algorithm will be
running depends on number of steps.

6
00:00:28.610 --> 00:00:33.775
But learning rate defines if we gonna
get to the solution or not at all.

7
00:00:33.775 --> 00:00:38.708
So just to demonstrate, let's say we
initialize with those variables, 0 and

8
00:00:38.708 --> 00:00:40.255
minus 1, right?

9
00:00:40.255 --> 00:00:44.635
But let's say our learning rate is really,
really small.

10
00:00:44.635 --> 00:00:49.745
So what will happen if I just
re-execute this code, right?

11
00:00:49.745 --> 00:00:55.112
So we did 200 steps, but
if we look at the w,

12
00:00:55.112 --> 00:00:58.978
originally it was 0.0 right?

13
00:00:58.978 --> 00:01:01.380
And our b was minus 1.

14
00:01:01.380 --> 00:01:06.369
So if we look after those 200 steps,
our w is

15
00:01:06.369 --> 00:01:12.120
actually changed and our b modified.

16
00:01:12.120 --> 00:01:15.330
And it's kind of going the right
direction, but it's really,

17
00:01:15.330 --> 00:01:19.530
really far from the true values just
because our learning rate is so small.

18
00:01:19.530 --> 00:01:27.364
If I run this code for
five billion trillion [LAUGH] iterations,

19
00:01:27.364 --> 00:01:33.690
it will get to the solution or at least
in the proximity of the solution, right?

20
00:01:33.690 --> 00:01:36.860
The absolute true value is not guaranteed.

21
00:01:36.860 --> 00:01:41.090
But this is really a simple problem, you
can probably just solve it analytically.

22
00:01:41.090 --> 00:01:45.177
But anyway, that's just to demonstrate
what's happening with this,

23
00:01:45.177 --> 00:01:46.654
it has degraded descent.

24
00:01:46.654 --> 00:01:49.974
You're also starting with
some random point, and

25
00:01:49.974 --> 00:01:54.560
just calculating what should be
the direction you need to go.

26
00:01:54.560 --> 00:01:58.641
And you also can control your learning
rates and how fast you're going, and

27
00:01:58.641 --> 00:02:00.317
you might overshoot, right?

28
00:02:00.317 --> 00:02:03.442
So for instance, right now,

29
00:02:03.442 --> 00:02:08.948
if I set learning rate equal to 1,
why not, right?

30
00:02:08.948 --> 00:02:10.875
Well, 10.

31
00:02:10.875 --> 00:02:13.270
If we're overshooting,
let's overshoot big.

32
00:02:13.270 --> 00:02:18.350
So if we execute this code, so
we just ran 200 steps again,

33
00:02:18.350 --> 00:02:23.641
let's just see, we actually over
jump from the dynamic range.

34
00:02:23.641 --> 00:02:26.198
So we got so
huge number is has become none.

35
00:02:26.198 --> 00:02:27.070
[LAUGH] I know.

36
00:02:28.320 --> 00:02:33.010
Let's probably get
learning rate equal to 1.

37
00:02:33.010 --> 00:02:35.767
I'm not sure what's gonna happen,
but we'll see in a second.

38
00:02:38.359 --> 00:02:39.683
Still none, okay.

39
00:02:39.683 --> 00:02:47.312
So it should be probably something 0.9.

40
00:02:50.110 --> 00:02:54.970
And that's the thing,
to some degree, yeah,

41
00:02:54.970 --> 00:03:01.059
it's this negative number in 10,
24th, it's huge.

42
00:03:02.663 --> 00:03:07.219
And just because, yes, for our function,

43
00:03:07.219 --> 00:03:11.911
loss function is basically your hyperbola.

44
00:03:11.911 --> 00:03:13.800
It's hyperbolic function, right?

45
00:03:13.800 --> 00:03:18.585
And what's happening with those learning
rates, you just jumping out of.

46
00:03:18.585 --> 00:03:23.614
And after 200 steps, yeah,
that's why you seen this 10 and

47
00:03:23.614 --> 00:03:30.208
24th power, just because you moving not
closer to the minimum, but actually out.

48
00:03:32.343 --> 00:03:35.100
You choosing the right direction,
you just over jumping significantly.

49
00:03:36.450 --> 00:03:39.328
So back to our original question.

50
00:03:39.328 --> 00:03:43.094
With the number of steps, you control
how many calculations you will do.

51
00:03:43.094 --> 00:03:47.994
But learning rate control how fast or
how slow you're gonna progress.

52
00:03:47.994 --> 00:03:49.724
So it's combination of both.

53
00:03:49.724 --> 00:03:51.118
To get to the solution,

54
00:03:51.118 --> 00:03:54.690
quite often you should start
with some large learning rate.

55
00:03:54.690 --> 00:04:00.610
And there is a method
called one shot learning.

56
00:04:00.610 --> 00:04:05.230
It basically tries different
learning rates, right,

57
00:04:05.230 --> 00:04:07.870
starting with something really,
really small.

58
00:04:07.870 --> 00:04:09.351
So basically, barely moving.

59
00:04:09.351 --> 00:04:11.787
But then, as you progress,

60
00:04:11.787 --> 00:04:17.475
it's seeing that loss function
starts dropping significantly,

61
00:04:17.475 --> 00:04:22.156
meaning that you getting to
the solution pretty fast.

62
00:04:22.156 --> 00:04:26.493
But you continue increasing the learning
rates to even start jumping outside of

63
00:04:26.493 --> 00:04:28.020
the solution.

64
00:04:28.020 --> 00:04:32.770
But this simple exploration will give you

65
00:04:32.770 --> 00:04:35.810
approximately optimal value for
the learning rate.

66
00:04:35.810 --> 00:04:39.360
So when you see the minimum
in the loss function,

67
00:04:39.360 --> 00:04:42.860
if you just take this value,
which you used for the learning rate and

68
00:04:42.860 --> 00:04:47.735
divided by 10, it should give you
an optimal learning rate value.

69
00:04:47.735 --> 00:04:54.310
It basically means that in 10 steps,
you will get kinda to the minimum.

70
00:04:54.310 --> 00:04:56.470
Makes sense?

71
00:04:56.470 --> 00:04:59.974
&gt;&gt; So it gives you the optimal learning
rate for the number of specified steps?

72
00:05:03.470 --> 00:05:07.322
&gt;&gt; Learning rate depends on the problem,
but

73
00:05:07.322 --> 00:05:12.780
you should consider them as
independent hyper parameters.

74
00:05:13.790 --> 00:05:17.750
But you need combination of both
to get to the optimal solution.

75
00:05:19.340 --> 00:05:22.460
&gt;&gt; So in this case, since you've
increased the learning rate to 0.9,

76
00:05:22.460 --> 00:05:25.880
what would you have to do
with the number of steps to

77
00:05:25.880 --> 00:05:27.560
get you closer to-
&gt;&gt; Nothing.

78
00:05:27.560 --> 00:05:30.660
That's the critical
[CROSSTALK] it will never help.

79
00:05:30.660 --> 00:05:35.071
Well, maybe, okay,
maybe reduce the number of steps to 1.

80
00:05:35.071 --> 00:05:38.245
[LAUGH] Let's see if it's help or not.

81
00:05:38.245 --> 00:05:42.232
So if we run this code,
let's see the value.

82
00:05:42.232 --> 00:05:45.680
1.4, 1.7, okay.

83
00:05:45.680 --> 00:05:48.660
It should be 0.1 and 0.5.

84
00:05:48.660 --> 00:05:53.840
So if we increase to 2, so 2 steps,

85
00:05:53.840 --> 00:05:59.960
okay, it was 1.4, it became 0.6.

86
00:05:59.960 --> 00:06:02.989
So we simply kinda jump this way first.

87
00:06:02.989 --> 00:06:04.460
Now, we jumping that way.

88
00:06:04.460 --> 00:06:11.880
And the more steps I will be doing,
the far I will be.

89
00:06:11.880 --> 00:06:15.607
Remember, it was 1.4, now it's 2.

90
00:06:15.607 --> 00:06:17.433
Yep, now it's 3.

91
00:06:17.433 --> 00:06:20.270
It was 1 something previously, I think.

92
00:06:20.270 --> 00:06:24.670
So it's just basically this
jumping out of your well.

93
00:06:26.050 --> 00:06:28.848
So nothing, nothing will help if
your learning rate is too big.

94
00:06:28.848 --> 00:06:30.850
That's why they recommend
to start with smaller.

95
00:06:30.850 --> 00:06:36.742
But as I said, this method I think I

96
00:06:36.742 --> 00:06:44.450
heard about it from author of Fast AI,
Jeremy Hulbert.

97
00:06:44.450 --> 00:06:49.439
Jeremy created the whole
deep learning course and

98
00:06:49.439 --> 00:06:53.850
framework with the same name, Fast dot AI.

99
00:06:53.850 --> 00:06:56.600
And he used this method quite a bit.

100
00:06:56.600 --> 00:06:59.470
So basically, you're starting
with learning rate pretty small,

101
00:06:59.470 --> 00:07:03.090
but on every step,
you just increase it a little bit.

102
00:07:03.090 --> 00:07:08.150
And you're doing that to just find the
optimal learning rate you should use for

103
00:07:08.150 --> 00:07:09.220
your problem.

104
00:07:09.220 --> 00:07:15.335
By plotting a learning
rate versus loss function.

105
00:07:15.335 --> 00:07:20.080
Let's take a closer look at
what we discovered so far.

106
00:07:20.080 --> 00:07:25.640
Actually, this notebook was already
prepared with all the bits in it.

107
00:07:25.640 --> 00:07:31.040
So if you want to play with it on your
own, you can just execute this code.

108
00:07:31.040 --> 00:07:34.910
So it's exactly what we did
generating the noisy data, right?

109
00:07:34.910 --> 00:07:36.681
Plotting the distribution,

110
00:07:36.681 --> 00:07:40.517
playing around a little bit with
other types of visualization.

111
00:07:40.517 --> 00:07:43.124
And once again, kinda creating variables,

112
00:07:43.124 --> 00:07:45.810
the one we will modify
additional functions.

113
00:07:45.810 --> 00:07:50.747
So I just wanted to provide
this example to demonstrate

114
00:07:50.747 --> 00:07:54.085
build the concept of loss function.

115
00:07:54.085 --> 00:07:58.997
So it's a function which is supposed
to represent a how mistaken you are,

116
00:07:58.997 --> 00:08:02.496
and what you can do to
imrove your mistakes, right?

117
00:08:02.496 --> 00:08:07.134
And what we're doing, we're just keeping
track of all the operations we did while

118
00:08:07.134 --> 00:08:10.862
calculating those predictions and
calculating loss functions.

119
00:08:10.862 --> 00:08:16.116
And then, just using gradients
relative to those variables,

120
00:08:16.116 --> 00:08:21.072
which will indicate how
the variables themselves have to be

121
00:08:21.072 --> 00:08:25.568
changed to get to the local
of this loss function.

122
00:08:25.568 --> 00:08:29.860
All right, so
if you want really cool visualization,

123
00:08:29.860 --> 00:08:33.010
it's actually at the end of this notebook.

124
00:08:33.010 --> 00:08:37.990
I've not only plot 2D image,
but the whole 3D right here.

125
00:08:39.090 --> 00:08:41.120
Right here.

126
00:08:41.120 --> 00:08:46.920
We should demonstrate how our
loss function looks in 3D,

127
00:08:46.920 --> 00:08:49.410
so that's actually
the image I'm referring.

128
00:08:49.410 --> 00:08:51.569
So we have weight as 1.

129
00:08:51.569 --> 00:08:59.290
One axis biases the other, and this
whole thing almost look like a saddle.

130
00:08:59.290 --> 00:09:04.136
And we're just getting to the minimum
by just trying all those points and

131
00:09:04.136 --> 00:09:05.731
slowly getting there.

