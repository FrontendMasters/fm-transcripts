WEBVTT

1
00:00:00.275 --> 00:00:03.403
We can start with a slightly
different model, right,

2
00:00:03.403 --> 00:00:05.512
by adding those additional layers.

3
00:00:05.512 --> 00:00:08.570
So right now, we have less neurons, right?

4
00:00:08.570 --> 00:00:12.816
Remember that in the original
one we had 256, we try,

5
00:00:12.816 --> 00:00:15.566
it's just only one layer with 16.

6
00:00:15.566 --> 00:00:20.139
Now there are 3 layers with 16, but
it's all about those interconnections.

7
00:00:20.139 --> 00:00:23.625
Well, let's first try run it, right.

8
00:00:23.625 --> 00:00:27.512
So, exactly the same optimizer.

9
00:00:27.512 --> 00:00:32.960
And if we train it, so
briefly, I think we're

10
00:00:32.960 --> 00:00:39.548
getting to almost 97.5%
with our 256 neurons.

11
00:00:39.548 --> 00:00:45.950
It was about 92%, as far as I remember,
for 16 neurons only.

12
00:00:45.950 --> 00:00:49.700
Right now we already at 95%

13
00:00:54.640 --> 00:00:59.916
So basically, yeah.

14
00:00:59.916 --> 00:01:05.802
Now it's up to us as the machine
learning experts [LAUGH], yeah.

15
00:01:05.802 --> 00:01:08.236
We can try different architectures, right?

16
00:01:08.236 --> 00:01:14.018
And by architecture, by typology, I mean
just try different number of layers or

17
00:01:14.018 --> 00:01:16.748
different number of neurons in them.

18
00:01:16.748 --> 00:01:21.843
And it still pretty common scenario
where you taking relatively

19
00:01:21.843 --> 00:01:26.841
big number of neurons in the first layer,
then reducing them,

20
00:01:26.841 --> 00:01:30.352
right, and getting to the final solution.

21
00:01:30.352 --> 00:01:34.350
But the thing is,
if you have multiple modules like this,

22
00:01:34.350 --> 00:01:39.657
right if you trying different number
of layers, different number neurons,

23
00:01:39.657 --> 00:01:44.412
what is the proper way to compare
the accuracies of those solutions?

24
00:01:44.412 --> 00:01:51.080
And that brings us to another question
how to splint your data, right?

25
00:01:51.080 --> 00:01:58.054
And right now, I'm only demonstrating
into splitting into two sub-data sets.

26
00:01:58.054 --> 00:02:02.199
We have our x_train and x_test.

27
00:02:02.199 --> 00:02:07.337
But commonly it is split
actually into three parts.

28
00:02:07.337 --> 00:02:12.617
And it can be something like 70% for
your training data set 20% for

29
00:02:12.617 --> 00:02:15.877
test data set, and 10% for validation.

30
00:02:15.877 --> 00:02:17.549
So what is this validation?

31
00:02:17.549 --> 00:02:21.239
It's basically a data
sets in our case images,

32
00:02:21.239 --> 00:02:24.478
which our models have never seen before.

33
00:02:24.478 --> 00:02:28.734
So it's not part of the training data
set and not part of test data set.

34
00:02:28.734 --> 00:02:33.665
And when you're trying different
models with different topologies with

35
00:02:33.665 --> 00:02:38.436
different number of layers,
you will be relying on this final number,

36
00:02:38.436 --> 00:02:40.595
validation accuracy, right?

37
00:02:40.595 --> 00:02:45.588
And that's basically gives you
an approximate value in our case it's

38
00:02:45.588 --> 00:02:46.610
like 95%.

39
00:02:46.610 --> 00:02:51.225
That was the accuracy
of the prediction from

40
00:02:51.225 --> 00:02:55.850
this split from those, let's say 20%.

41
00:02:55.850 --> 00:03:01.122
And you can just have different models and
those numbers in one column.

42
00:03:01.122 --> 00:03:05.449
And the way how you choose which
model is the best between those,

43
00:03:05.449 --> 00:03:11.078
is not by just looking at those numbers,
but rather using the validation data set.

44
00:03:11.078 --> 00:03:15.945
This new 10%,
our models have never seen before.

45
00:03:15.945 --> 00:03:20.060
And providing them to
calculate the accuracy, right?

46
00:03:20.060 --> 00:03:25.025
And that basically why you need to do
this, not to rely on these numbers alone,

47
00:03:25.025 --> 00:03:28.503
that prevents from cost contamination,
so to speak.

48
00:03:28.503 --> 00:03:32.267
So if your test data
set is somehow biased,

49
00:03:32.267 --> 00:03:37.088
it might affect the accuracy
you will see on the model.

50
00:03:37.088 --> 00:03:42.863
But if you use some new part of
the dataset which our model haven't

51
00:03:42.863 --> 00:03:48.113
seen before, it will sink well
it increase the trust, or so

52
00:03:48.113 --> 00:03:53.574
to speak, improve the accuracy
of the accuracy prediction.

53
00:03:53.574 --> 00:03:59.166
So it's kind of a new layer of obstruction
on top of what we already did,

54
00:03:59.166 --> 00:04:03.842
and, all right,
let's get back to our history and plots.

55
00:04:03.842 --> 00:04:08.813
Yeah, you can see that although
we are not getting to 98%,

56
00:04:08.813 --> 00:04:13.497
it's just because our number
of neurons is pretty small,

57
00:04:13.497 --> 00:04:18.465
I think I can fix it by just
putting 64 neurons here 32, for

58
00:04:18.465 --> 00:04:24.030
instance, and 16 at the end, and
that might give us better model.

59
00:04:24.030 --> 00:04:28.951
So once again,
I'm kinda recreating this triangle almost.

60
00:04:28.951 --> 00:04:33.262
So let's recompile, okay?

61
00:04:33.262 --> 00:04:34.871
Just out of curiosity,

62
00:04:34.871 --> 00:04:39.544
I'm kinda curious what's the final
accuracy I'm gonna get there.

63
00:04:39.544 --> 00:04:45.105
Almost 95% from the first epoch.

64
00:04:45.105 --> 00:04:49.521
At the highest accuracy we've seen so

65
00:04:49.521 --> 00:04:53.386
far was 97.5 about, right?

66
00:04:53.386 --> 00:04:56.091
Well, we already at 97%, which is good.

67
00:04:57.404 --> 00:04:59.146
Yep.

68
00:04:59.146 --> 00:05:04.019
So, you can almost imagine
that you just taking all of

69
00:05:04.019 --> 00:05:09.220
those degrees of freedom 700
of them right think about

70
00:05:09.220 --> 00:05:14.439
each pixel almost like
independence degree of freedom.

71
00:05:14.439 --> 00:05:21.481
Right, because our pixel can have
the intensity from 0 to 255, right?

72
00:05:21.481 --> 00:05:25.568
And we're simply taking
this information and

73
00:05:25.568 --> 00:05:31.337
trying to reduce it first to 64
degrees of freedom down to 32.

74
00:05:31.337 --> 00:05:36.028
But at the end we get into 10
neurons which are activated.

75
00:05:36.028 --> 00:05:40.748
There is a chance you can
lose some information.

76
00:05:40.748 --> 00:05:46.712
For instance, if I simply reduce
the number of neurons in our intermediate

77
00:05:46.712 --> 00:05:52.016
layers to something pretty small like for
instance four neurons,

78
00:05:52.016 --> 00:05:57.982
there is a chance that this four neurons
now will reduce the inflammation so

79
00:05:57.982 --> 00:06:04.360
much that we will actually not be able
properly recognized between our digits.

80
00:06:04.360 --> 00:06:09.639
So let's just out of curiosity
compile this one and

81
00:06:09.639 --> 00:06:16.287
see, by the way final accuracy was,
yeah about 97 and a half.

82
00:06:16.287 --> 00:06:20.629
So, but if we have only four neurons,

83
00:06:20.629 --> 00:06:25.122
so once again 64, 4, 16, 10.

84
00:06:25.122 --> 00:06:30.690
Wow, okay, actually,
we still can get 92% am I recompile?

85
00:06:30.690 --> 00:06:32.665
Yeah, I recompiled it, okay.

86
00:06:36.366 --> 00:06:41.618
Well technically four
neurons probably contain

87
00:06:41.618 --> 00:06:46.363
information about ten different classes.

88
00:06:46.363 --> 00:06:49.499
Maybe if we reduce it to one
neuron [LAUGH] I'm going for

89
00:06:49.499 --> 00:06:53.274
something really extreme something
to see the big difference.

90
00:06:53.274 --> 00:06:54.598
Yeah, 95% not bad.

91
00:06:54.598 --> 00:06:56.496
But what if we put only one neuron,

92
00:06:56.496 --> 00:07:00.768
in one neuron basically at the end we
get activation or not activation right?

93
00:07:00.768 --> 00:07:05.166
So there's pretty strong chance
that we will not be able to

94
00:07:05.166 --> 00:07:08.683
recognize between all
ten different digits.

95
00:07:10.359 --> 00:07:14.507
And the accuracy is 37%, okay.

96
00:07:14.507 --> 00:07:19.279
[LAUGH] 36, interesting.

97
00:07:20.911 --> 00:07:24.916
The reason why I'm doing this right now,

98
00:07:24.916 --> 00:07:31.793
I just want you to think about what
information is in this case, right?

99
00:07:31.793 --> 00:07:36.141
So we have this 28 pixels by 28 pixels,
and

100
00:07:36.141 --> 00:07:39.495
that's all of our input channels.

101
00:07:39.495 --> 00:07:43.222
And then we as well
transforming this information,

102
00:07:43.222 --> 00:07:47.041
reducing the degrees of freedom so
to speak, right?

103
00:07:47.041 --> 00:07:52.938
And interestingly enough
even having four neurons

104
00:07:52.938 --> 00:07:59.667
still is enough to propagate
all the useful information.

105
00:07:59.667 --> 00:08:03.508
That actually brings us
to the next example.

106
00:08:03.508 --> 00:08:05.018
But this one I will not be typing.

107
00:08:05.018 --> 00:08:09.695
I will just, run the notebook itself.

108
00:08:09.695 --> 00:08:12.235
It's called
&gt;&gt; Question about that with

109
00:08:12.235 --> 00:08:13.013
the four neuron.

110
00:08:13.013 --> 00:08:18.311
So if you think about four neurons, either
each of them is either activated or not.

111
00:08:18.311 --> 00:08:20.569
&gt;&gt; It's almost like
&gt;&gt; Essentially 2 to the 4th power.

112
00:08:20.569 --> 00:08:24.593
So you can see it's impossible values,
which is enough to accommodate

113
00:08:24.593 --> 00:08:27.361
the 10 digits
&gt;&gt; Exactly that's why having,

114
00:08:27.361 --> 00:08:31.676
I'm still surprised that we
getting almost what 40% accuracy.

115
00:08:31.676 --> 00:08:37.046
He said, we've having only one neuron.

116
00:08:37.046 --> 00:08:38.191
But it seems that.

117
00:08:38.191 --> 00:08:39.892
&gt;&gt; Yeah, that makes sense to right?

118
00:08:39.892 --> 00:08:43.609
Cuz it's about on or off right,
with one neuron and so

119
00:08:43.609 --> 00:08:46.672
you have about a 50% chance of guessing.

120
00:08:46.672 --> 00:08:49.537
[LAUGH]
&gt;&gt; But at the same time,

121
00:08:49.537 --> 00:08:54.178
we don't have only zeros and
ones we have everything in between.

122
00:08:54.178 --> 00:08:59.295
and there is a chance that our
neural networks somehow recognize

123
00:08:59.295 --> 00:09:05.087
that if the signal is somewhere,
let's say between 0.4 and 0.6.

124
00:09:05.087 --> 00:09:07.654
It should be number eight,
something like that.

125
00:09:07.654 --> 00:09:09.084
And that's the thing, yeah.

126
00:09:09.084 --> 00:09:11.700
To some degree,
what neuron network is doing.

127
00:09:11.700 --> 00:09:16.042
It's almost like a black box.

128
00:09:16.042 --> 00:09:20.601
Well, you can try to reason and
say what it might do.

129
00:09:20.601 --> 00:09:24.605
But what it's actually doing is
just modifying those weights and

130
00:09:24.605 --> 00:09:28.987
biases and yeah, it's difficult to
say what exactly happened there.

131
00:09:28.987 --> 00:09:33.618
But with a convolution neuron networks the
one we'll be using for image recognition,

132
00:09:33.618 --> 00:09:37.488
you can do some of the things which
will show you how the neuron networks is

133
00:09:37.488 --> 00:09:41.440
working and that's actually gonna be
an example or demo in a few minutes.

