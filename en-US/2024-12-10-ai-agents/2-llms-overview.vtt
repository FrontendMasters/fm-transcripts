WEBVTT

1
00:00:00.087 --> 00:00:01.128
&gt;&gt; Scott Moss: What is an LLM?

2
00:00:01.128 --> 00:00:02.091
Well, I guess it's right there.

3
00:00:02.091 --> 00:00:04.224
I was gonna ask,
does anybody know what an LLM stands for?

4
00:00:04.224 --> 00:00:06.525
It's right there,
it's a large language model.

5
00:00:06.525 --> 00:00:09.231
But what does that actually mean?

6
00:00:09.231 --> 00:00:13.685
So I'll kinda talk through
how I understand it, and

7
00:00:13.685 --> 00:00:19.256
what's the difference between
a large language model versus all

8
00:00:19.256 --> 00:00:26.162
the AI that's been happening before
the large language models come to present.

9
00:00:26.162 --> 00:00:27.183
And how is that different?

10
00:00:27.183 --> 00:00:33.713
So essentially, a large language model
is a, first of all, what is a model?

11
00:00:33.713 --> 00:00:37.496
If you're kind of new to AI,
a model is just,

12
00:00:37.496 --> 00:00:40.993
it's a file that's a set of statistics.

13
00:00:40.993 --> 00:00:46.014
It's a set of basically,
some trained system that has a set

14
00:00:46.014 --> 00:00:51.950
of probabilities based off of some
input that somebody trained it on.

15
00:00:51.950 --> 00:00:59.012
So a good example might be a classifier
that can detect cancer inside of X-rays.

16
00:00:59.012 --> 00:01:03.341
So, someone will create a system that
takes these images of X-rays and

17
00:01:03.341 --> 00:01:08.479
they will label these images as like,
this has cancer, this doesn't have cancer.

18
00:01:08.479 --> 00:01:11.863
And then they would train that on an AI,
and

19
00:01:11.863 --> 00:01:16.283
then that AI can correlate
all these different pixels,

20
00:01:16.283 --> 00:01:20.997
they turn that image into a matrix
of pixels and RGB values.

21
00:01:20.997 --> 00:01:24.472
And it can correlate those RGB values and
pixels to whether or

22
00:01:24.472 --> 00:01:28.101
not this image has cancer or not,
and then it can be corrected.

23
00:01:28.101 --> 00:01:35.715
So if the AI gets it wrong, you can
introduce things like punishing it.

24
00:01:35.715 --> 00:01:39.309
So it can go down a different
path on the probability.

25
00:01:39.309 --> 00:01:43.046
So if you think about decision making,
you can go left or you can go right.

26
00:01:43.046 --> 00:01:46.234
So like, if you say that, hey, that was
wrong, then maybe next time, when it gets

27
00:01:46.234 --> 00:01:49.715
to that decision, it'll go left instead of
right, and then it'll see if that's right.

28
00:01:49.715 --> 00:01:51.244
And just does that over and over and

29
00:01:51.244 --> 00:01:54.638
over again until it reaches a point
where you're not punishing it anymore.

30
00:01:54.638 --> 00:01:59.456
And then now you have this file of these
probabilities that have some form of

31
00:01:59.456 --> 00:02:01.837
accuracy on the data that you train.

32
00:02:01.837 --> 00:02:07.033
And I would say that's a very specific set
of AI, but that kind of goes in towards

33
00:02:07.033 --> 00:02:11.706
what you would see with LLMs and
diffusion models and things like that.

34
00:02:11.706 --> 00:02:16.305
And obviously, there's different types
of AI, but a model is mostly just,

35
00:02:16.305 --> 00:02:19.508
represents some statistics,
some probabilities.

36
00:02:19.508 --> 00:02:22.035
There's this other thing inside
of it called a neural network.

37
00:02:22.035 --> 00:02:24.305
We're not gonna talk about that,
nobody knows how that works.

38
00:02:24.305 --> 00:02:27.624
Even the people that make neural networks
have no idea what's inside of them,

39
00:02:27.624 --> 00:02:29.653
which is kind of trippy,
if you think about it.

40
00:02:29.653 --> 00:02:31.719
But an LLM is a variation of that.

41
00:02:31.719 --> 00:02:34.595
So what they discovered was,

42
00:02:34.595 --> 00:02:39.771
they can get pretty good at
having a model predict what

43
00:02:39.771 --> 00:02:44.846
the most probable next word
in a sentence should be.

44
00:02:44.846 --> 00:02:47.141
At the end of the day,
that's all an LLM is,

45
00:02:47.141 --> 00:02:49.697
it's really good at
predicting the next thing.

46
00:02:49.697 --> 00:02:53.391
And it's not something new,
we've tried to do this in the past, and

47
00:02:53.391 --> 00:02:56.589
there have been tons of models
that predict things for you.

48
00:02:56.589 --> 00:02:59.183
Like, imagine typing into
Google on search, and

49
00:02:59.183 --> 00:03:02.044
it shows you some prediction
of what you might search.

50
00:03:02.044 --> 00:03:06.796
That's always kind of been there, but that
is way different than what an LLM does.

51
00:03:06.796 --> 00:03:11.412
An LLM, what it does is, it takes
an account of everything you've typed so

52
00:03:11.412 --> 00:03:16.029
far up until this point, every single
sentence, and then that allows it to

53
00:03:16.029 --> 00:03:20.528
keep more context as it's trying to
predict what the next character is.

54
00:03:20.528 --> 00:03:22.326
We haven't had that before.

55
00:03:22.326 --> 00:03:26.087
And we'll kinda talk about that
architecture a little ways down on this

56
00:03:26.087 --> 00:03:28.915
page, but high level,
that's kinda what an LLM is.

57
00:03:28.915 --> 00:03:33.398
It's a model that's really good
at predicting the next thing.

58
00:03:33.398 --> 00:03:37.616
So sometimes it feels sentient sometimes,
but in reality,

59
00:03:37.616 --> 00:03:42.661
it's just predicting the next thing,
which as you know, if it has to get

60
00:03:42.661 --> 00:03:47.889
good at predicting the next thing,
then has to be trained on a lot of stuff.

61
00:03:47.889 --> 00:03:50.607
So that's where the word large comes from.

62
00:03:50.607 --> 00:03:55.451
It's very large, cuz it has to be
trained on billions of parameters,

63
00:03:55.451 --> 00:03:59.980
words, sentences, for
it to understand what the next thing is.

64
00:03:59.980 --> 00:04:02.947
So, that's my rant on LLMs.

65
00:04:02.947 --> 00:04:06.757
The part about the LLM
keeping track of context and

66
00:04:06.757 --> 00:04:11.575
knowing what to do,
that's something called transformers.

67
00:04:11.575 --> 00:04:15.235
So I'm just gonna go down to that part
because I think it's interesting.

68
00:04:15.235 --> 00:04:20.811
So, here's a white paper, created by some
folks, I think they were at Google at

69
00:04:20.811 --> 00:04:26.068
the time when they made this, and
it's called, Attention Is All You Need.

70
00:04:26.068 --> 00:04:29.811
Now, this is super deep, but I do highly
recommend people looking into this.

71
00:04:29.811 --> 00:04:34.563
You do not need to know this at all to
do anything that we're gonna be doing.

72
00:04:34.563 --> 00:04:37.584
And unless you're gonna be a research
scientist, literally, this doesn't matter.

73
00:04:37.584 --> 00:04:41.140
But the reason I'm recommending you
read this is because of how much I

74
00:04:41.140 --> 00:04:44.342
benefited from just understanding
what transformers were.

75
00:04:44.342 --> 00:04:47.698
And it helped me understand
the limitations of what an AI or

76
00:04:47.698 --> 00:04:48.524
an LLM can do.

77
00:04:48.524 --> 00:04:51.218
But basically, they describe
this concept of transformers.

78
00:04:51.218 --> 00:04:54.088
And you're gonna hear that so much,

79
00:04:54.088 --> 00:04:58.730
that if you ever heard of GPT,
the T in GPT is transformers.

80
00:04:58.730 --> 00:05:03.143
So attention is something that
they describe as what you get from

81
00:05:03.143 --> 00:05:04.291
transformers.

82
00:05:04.291 --> 00:05:06.583
So I highly recommend
reading this white paper.

83
00:05:06.583 --> 00:05:09.655
If you get deep into the AI space,
you'll be waking up and

84
00:05:09.655 --> 00:05:11.838
reading white papers every single day.

85
00:05:11.838 --> 00:05:14.925
So if you're really serious about that,
you can just start now.

86
00:05:14.925 --> 00:05:16.764
But this is probably the first
one I recommend reading.

87
00:05:16.764 --> 00:05:21.629
They describe how this transformer
architecture is really all you need to

88
00:05:21.629 --> 00:05:26.656
have some type of GPT-based model that
is really good at predicting stuff.

89
00:05:26.656 --> 00:05:29.600
And from there the bet is like,
let's just throw more data at it and

90
00:05:29.600 --> 00:05:30.769
then it'll get smarter.

91
00:05:30.769 --> 00:05:33.347
And that's kind of where we
are right now as a society is,

92
00:05:33.347 --> 00:05:36.038
throw more data at something
that's transformer based and

93
00:05:36.038 --> 00:05:38.213
see how better it can get
at predicting things.

94
00:05:38.213 --> 00:05:40.539
And we haven't really
deviated from that yet.

95
00:05:40.539 --> 00:05:43.930
So, I highly recommend reading that it.

96
00:05:43.930 --> 00:05:47.754
The trick is, if this just sounds crazy or
whatever, take it,

97
00:05:47.754 --> 00:05:51.444
throw it into ChatGPT or Claude and
say, tell me about this.

98
00:05:51.444 --> 00:05:55.025
[LAUGH] I'm a software engineer, I make
React apps, tell me what this means, and

99
00:05:55.025 --> 00:05:56.194
it'll tell you, right?

100
00:05:56.194 --> 00:05:59.156
So, use AI to to learn
more about this stuff.

101
00:05:59.156 --> 00:05:59.977
Highly recommend it.

102
00:05:59.977 --> 00:06:03.106
There's some more stuff in here about
like training processes and stuff.

103
00:06:03.106 --> 00:06:05.503
I don't think I'm gonna dive
into this because it's,

104
00:06:05.503 --> 00:06:08.360
I don't think you're ever gonna
do any LLM training in your life.

105
00:06:08.360 --> 00:06:13.341
Unless someone in here or someone online
or whoever's watching this is gonna go be,

106
00:06:13.341 --> 00:06:15.175
maybe they're doing LLM ops.

107
00:06:15.175 --> 00:06:19.186
Well, even LLM ops, probably not doing
training foundational models from scratch.

108
00:06:19.186 --> 00:06:22.768
But like, yeah, I don't think anyone's
gonna be doing a lot of training and

109
00:06:22.768 --> 00:06:26.255
stuff, just know that you need a lot
of information to train a model, yes.

110
00:06:26.255 --> 00:06:28.142
What are tokens?

111
00:06:28.142 --> 00:06:32.816
Basically, tokens, depending on
the model and how you encode it,

112
00:06:32.816 --> 00:06:34.949
are three to four characters.

113
00:06:34.949 --> 00:06:38.846
So if you have a sentence, every three or
four characters is a token.

114
00:06:38.846 --> 00:06:43.947
It's broken down that way because the AI
is trying to understand things like,

115
00:06:43.947 --> 00:06:46.122
I'm gonna get really deep here.

116
00:06:46.122 --> 00:06:51.182
[LAUGH] So there's a numerical
representation of all the words that

117
00:06:51.182 --> 00:06:56.348
the AI understands, and
those are called vectors and embeddings.

118
00:06:56.348 --> 00:07:02.875
And they represent the semantics,
the meaning of a word in a numerical way.

119
00:07:02.875 --> 00:07:07.065
So like you might have, depending on
how many dimensions that embedding is,

120
00:07:07.065 --> 00:07:08.619
let's say it's 1536.

121
00:07:08.619 --> 00:07:13.288
So you got 1,536 numbers
inside of an array, and

122
00:07:13.288 --> 00:07:20.054
those numbers will represent the meaning
of one token or something, right?

123
00:07:20.054 --> 00:07:25.465
So the tokens are just the the alpha
numeric way of representing

124
00:07:25.465 --> 00:07:30.786
those vectors and embeddings,
but the AI sees it as numbers.

125
00:07:30.786 --> 00:07:34.163
And the reason why is because then it
could do math to kinda figure out how

126
00:07:34.163 --> 00:07:35.186
things are similar.

127
00:07:35.186 --> 00:07:39.011
So like, if you imagine, if you had, I
don't know, an array of three numbers and

128
00:07:39.011 --> 00:07:43.073
another ray of three numbers and you threw
them on a plot, a three-dimensional plot.

129
00:07:43.073 --> 00:07:46.671
You could do math to figure out how
close those things were in relation.

130
00:07:46.671 --> 00:07:50.519
Okay, that's very similar to how
some of the stuff works with AI, but

131
00:07:50.519 --> 00:07:55.630
across many dimensions, not just three
dimensions, but 1,500 dimensions, right?

132
00:07:55.630 --> 00:07:57.159
So it's like,
you can't even plot that out in your head.

133
00:07:57.159 --> 00:07:58.929
What does that even look like?

134
00:07:58.929 --> 00:08:03.105
Yeah, we don't know, but all you need
to know is, tokens are every three or

135
00:08:03.105 --> 00:08:05.146
four characters, that's a token.

136
00:08:05.146 --> 00:08:09.017
And the reason you need to know that
is because every LLM is charging you

137
00:08:09.017 --> 00:08:12.771
based on how many tokens you give it and
how many tokens it gives you.

138
00:08:12.771 --> 00:08:14.994
So you need to understand
that token count, and

139
00:08:14.994 --> 00:08:17.504
that is the only reason you
need to know tokens, yes.

140
00:08:17.504 --> 00:08:23.409
Does the size of the token change
model to model or API to API?

141
00:08:23.409 --> 00:08:30.371
Yeah, good question, so, token limits are
determined by the model that you're using,

142
00:08:30.371 --> 00:08:36.067
and some models have what they call,
it's called a context window size.

143
00:08:36.067 --> 00:08:41.136
It's the size of how many tokens you
can give this thing before it dies.

144
00:08:41.136 --> 00:08:45.329
So every model is different,
there's generic models that strive to

145
00:08:45.329 --> 00:08:49.752
have the biggest context window so
you can fit as many tokens as you want.

146
00:08:49.752 --> 00:08:54.479
There's very specific models that
are hyper-focused on one thing that don't

147
00:08:54.479 --> 00:08:56.676
really have big windows for tokens.

148
00:08:56.676 --> 00:08:59.898
So yeah, look at the documentation for
the model that you use and

149
00:08:59.898 --> 00:09:03.548
it'll tell you how many tokens it can
handle on the input and the output.

150
00:09:03.548 --> 00:09:05.961
And if you go beyond that,
it'll just break.

151
00:09:05.961 --> 00:09:08.191
It'll just tell you,
sorry, I can't do that.

152
00:09:08.191 --> 00:09:11.497
And the reason that is, is for
a lot of reasons, but it's like,

153
00:09:11.497 --> 00:09:13.651
it could be a physical thing too, right?

154
00:09:13.651 --> 00:09:18.032
When you think of all these tokens, this
stuff has to be held in the GPU memory.

155
00:09:18.032 --> 00:09:22.111
So I'm not sure how much you
might know about GPUs, but

156
00:09:22.111 --> 00:09:26.546
GPUs have memory,
they have RAM associated with them, and

157
00:09:26.546 --> 00:09:31.793
you can only host so much of that in
memory before you run out of memory.

158
00:09:31.793 --> 00:09:36.586
So there's that part, and then there's
also just the limit of that model and how

159
00:09:36.586 --> 00:09:41.751
it was trained, and everything that can be
fit into its process, its neural network.

160
00:09:41.751 --> 00:09:43.103
So there's a lot of limits on that.

161
00:09:43.103 --> 00:09:47.918
But people are working hard
to create million plus token

162
00:09:47.918 --> 00:09:52.946
size foundational models that
solve problems and stuff.

163
00:09:52.946 --> 00:09:55.058
But at the end of the day, yeah, you're
going to run into a token problem and

164
00:09:55.058 --> 00:09:55.993
there's no way around it today.

165
00:09:55.993 --> 00:09:57.558
No good way.

166
00:09:57.558 --> 00:09:58.726
We'll talk about different strategies.

