WEBVTT

1
00:00:00.260 --> 00:00:01.498
&gt;&gt; Scott Moss: Let's do a Hello World.

2
00:00:01.498 --> 00:00:03.220
I'm gonna check out to step one.

3
00:00:03.220 --> 00:00:05.310
You should follow along with me here.

4
00:00:06.680 --> 00:00:11.635
And first thing you want to do is
make sure you make a .env file,

5
00:00:11.635 --> 00:00:16.700
add your OpenAI key in there,
just literally, just like that.

6
00:00:16.700 --> 00:00:21.582
I mean, don't put the brackets around it,
obviously but OpenAI equals and put your

7
00:00:21.582 --> 00:00:26.047
key if you don't know what a .env file is,
this is a file where you can put your

8
00:00:26.047 --> 00:00:30.585
secret variables in your repo and make
sure they don't get push up the GitHub so

9
00:00:30.585 --> 00:00:34.616
people don't take them and
then get your whole system compromised.

10
00:00:34.616 --> 00:00:38.353
So if you put in the .env, I've already
have it ignored in the Git ignore, so

11
00:00:38.353 --> 00:00:42.166
you're good, but without this,
you won't be able to, participate today.

12
00:00:42.166 --> 00:00:43.306
So make sure you have it.

13
00:00:43.306 --> 00:00:45.546
It has to be just like
that OPENAI_API_key.

14
00:00:45.546 --> 00:00:49.546
Put that in the .env file
in the root of your repo.

15
00:00:49.546 --> 00:00:52.966
I didn't include it because the .env
file's not checked into GitHub.

16
00:00:52.966 --> 00:00:53.596
I can include it.

17
00:00:53.596 --> 00:00:55.026
That's the whole point of it.

18
00:00:55.026 --> 00:00:58.024
So add your .env file and do that.

19
00:00:58.024 --> 00:01:02.603
Once you have that,
we're going to just make a super

20
00:01:02.603 --> 00:01:07.604
simple LLM call and
we're going to see how AI works.

21
00:01:07.604 --> 00:01:11.114
I guess I'll just give you a quick tour
of what you should see in this repo.

22
00:01:11.114 --> 00:01:17.484
So we have an index TS file that literally
does nothing but processes a user input.

23
00:01:17.484 --> 00:01:20.144
If you don't give it an input,
it'll freak out.

24
00:01:20.144 --> 00:01:22.403
I have some types in here that I use for
TypeScript.

25
00:01:22.403 --> 00:01:24.223
If you don't know TypeScript, that's fine.

26
00:01:24.223 --> 00:01:25.093
You don't really need to know.

27
00:01:25.093 --> 00:01:28.555
For the TypeScript folks,
this is some types that I made so

28
00:01:28.555 --> 00:01:30.613
our editor doesn't scream at us.

29
00:01:31.833 --> 00:01:34.623
And, we have the source folder.

30
00:01:34.623 --> 00:01:36.933
Most of these files are empty.

31
00:01:36.933 --> 00:01:40.501
I do have a AI folder here that just
instantiates OpenAI and just exports it so

32
00:01:40.501 --> 00:01:42.810
we have to do it every
single time we wanna use it.

33
00:01:42.810 --> 00:01:45.440
So nothing crazy here.

34
00:01:45.440 --> 00:01:47.080
Everything else is mostly empty.

35
00:01:48.860 --> 00:01:50.420
There's a UI file here.

36
00:01:50.420 --> 00:01:52.740
So we're not building
anything frontend-related.

37
00:01:52.740 --> 00:01:55.650
As you saw in the demo,
it's just all through the terminal.

38
00:01:55.650 --> 00:01:57.920
To make that look pretty,
I made a file to do that.

39
00:01:57.920 --> 00:02:00.160
So obviously we aren't gonna
walk through how to do that.

40
00:02:00.160 --> 00:02:01.090
That's just done for you.

41
00:02:01.090 --> 00:02:04.640
So everything else you will have
to make but this is our UI.

42
00:02:04.640 --> 00:02:07.549
This is the thing that I
showed you in the terminal,

43
00:02:07.549 --> 00:02:11.879
because I don't think learning about
AI has anything to do with, whether or

44
00:02:11.879 --> 00:02:15.890
not you can tease out some TypeScript
react, or something like that.

45
00:02:15.890 --> 00:02:16.740
It's irrelevant.

46
00:02:16.740 --> 00:02:17.640
We just wanna talk about AI.

47
00:02:17.640 --> 00:02:18.790
So we're gonna do that.

48
00:02:18.790 --> 00:02:22.239
Also be sure to make sure you install
your dependencies, NPM install.

49
00:02:22.239 --> 00:02:23.509
I have dependencies.

50
00:02:23.509 --> 00:02:25.059
They're in the package JSON.

51
00:02:25.059 --> 00:02:26.229
There's a lot of stuff here.

52
00:02:26.229 --> 00:02:29.629
Well, not really, but do NPM install
to install these dependencies?

53
00:02:29.629 --> 00:02:31.899
Our app depends on them to work.

54
00:02:31.899 --> 00:02:34.339
You will not get anything to
work if you don't do this.

55
00:02:34.339 --> 00:02:35.619
So do that.

56
00:02:35.619 --> 00:02:37.519
Make sure you install your dependencies.

57
00:02:37.519 --> 00:02:42.515
Like I said, I'm going to use Node and
NPM going forward, if you want to use BUN,

58
00:02:42.515 --> 00:02:46.206
you can use and just point BUN
to the index file and run that.

59
00:02:46.206 --> 00:02:47.761
If you're using MPM,
I have a command for you.

60
00:02:47.761 --> 00:02:48.732
It's NPM start.

61
00:02:48.732 --> 00:02:49.497
We can use that.

62
00:02:49.497 --> 00:02:53.632
So that's what we are gonna to do but
let's keep moving.

63
00:02:53.632 --> 00:02:57.007
So the first thing we're gonna do is
we're going to add a LLM function here

64
00:02:57.007 --> 00:02:57.776
called runLLM.

65
00:02:57.776 --> 00:03:01.330
And essentially what it's gonna do,
it's gonna take a message and

66
00:03:01.330 --> 00:03:06.195
it's just going to call this endpoint here
here, openai.chat.completions.create and

67
00:03:06.195 --> 00:03:07.400
get back the message.

68
00:03:07.400 --> 00:03:09.040
So let's do that.

69
00:03:09.040 --> 00:03:13.214
What you can do is you can
go to the LLM file here, and

70
00:03:13.214 --> 00:03:19.040
you can import OpenAI from AI, and
cursor is looking at my Git history so

71
00:03:19.040 --> 00:03:22.090
it knows exactly what I want to do.

72
00:03:22.090 --> 00:03:25.239
And this is some of the stuff
that's kind of annoying to

73
00:03:25.239 --> 00:03:29.963
me with these AI tools is like,
how do you know I want to do that exactly?

74
00:03:29.963 --> 00:03:32.393
Because you saw it in my Git, that
doesn't mean I want to do that right now.

75
00:03:32.393 --> 00:03:34.833
And sometimes it does get in the way.

76
00:03:34.833 --> 00:03:37.340
But when it's right, it's cool but no,

77
00:03:37.340 --> 00:03:41.865
we're just going to export a function
here, we're gonna call it runLLM.

78
00:03:43.116 --> 00:03:44.736
Like this, it's gonna be a async function.

79
00:03:44.736 --> 00:03:50.182
And then basically what we want
this to take is really there's only

80
00:03:50.182 --> 00:03:56.126
one thing that we need here it's what
is the message the user is sending?

81
00:03:56.126 --> 00:04:00.187
Because the reason it's called
a completion, if you look at the OpenAI,

82
00:04:00.187 --> 00:04:04.121
it's called completion,
because you're completing this request,

83
00:04:04.121 --> 00:04:08.335
you're completing this sentence like,
what is the next thing after this?

84
00:04:08.335 --> 00:04:09.885
So it's called a completion.

85
00:04:09.885 --> 00:04:12.515
So I'm gonna take the user
message here like this.

86
00:04:15.885 --> 00:04:17.877
See and now it's trying to suggest so

87
00:04:17.877 --> 00:04:20.945
much I don't even know where
my code is okay there we go.

88
00:04:20.945 --> 00:04:25.672
And then I'm just gonna add some
types here as well for my own sanity.

89
00:04:25.672 --> 00:04:27.462
So I'm just gonna say that's a string.

90
00:04:27.462 --> 00:04:32.300
And then what we wanna do here is we want

91
00:04:32.300 --> 00:04:36.532
to call the OpenAI completion.

92
00:04:36.532 --> 00:04:39.342
So what I can do is I can say
I wanna get a response here.

93
00:04:39.342 --> 00:04:41.112
That's gonna be awaits.

94
00:04:41.112 --> 00:04:48.890
I'm gonna say await
openai.chat.completion.create like this.

95
00:04:48.890 --> 00:04:52.145
There's a couple parameters
that we can take here.

96
00:04:52.145 --> 00:04:54.065
I'll walk through some of them.

97
00:04:54.065 --> 00:04:56.393
So the most important one
probably is the model.

98
00:04:56.393 --> 00:05:00.263
You have to go look on the open AI
documentation to see what models

99
00:05:00.263 --> 00:05:01.215
they have.

100
00:05:01.215 --> 00:05:03.689
I think if you even just,
Let me see if this is type checked.

101
00:05:03.689 --> 00:05:05.925
If you just put a string here,
yeah, if you put a string here,

102
00:05:05.925 --> 00:05:08.469
you can kind of see all the models
that they currently have.

103
00:05:08.469 --> 00:05:12.321
The way that it works with OpenAI
is that the bigger the number,

104
00:05:12.321 --> 00:05:15.049
the newer the model essentially.

105
00:05:15.049 --> 00:05:20.629
But just because it's newer,
that doesn't mean it's better.

106
00:05:20.629 --> 00:05:22.746
I think 3.5 to 4o.

107
00:05:22.746 --> 00:05:23.726
That rule stands.

108
00:05:23.726 --> 00:05:25.466
So all the way up to 4o.

109
00:05:25.466 --> 00:05:27.446
Anything before that 4o
is going to be better.

110
00:05:27.446 --> 00:05:30.726
But then once you get to 4o
there's just different variations.

111
00:05:30.726 --> 00:05:34.226
You have 4o mini, which is a small mini.

112
00:05:34.226 --> 00:05:36.486
I talked about models being trained
in different parameter sizes.

113
00:05:36.486 --> 00:05:39.419
Okay, it's like the same training
technique they use for 4o,

114
00:05:39.419 --> 00:05:41.386
which is some breakthrough that they had.

115
00:05:41.386 --> 00:05:42.774
And then they gave it less parameters.

116
00:05:42.774 --> 00:05:47.083
So it's not as good as 4o, but
it's probably better than or

117
00:05:47.083 --> 00:05:49.434
maybe as good as 4, all right?

118
00:05:49.434 --> 00:05:51.304
But it's smaller and it's cheaper.

119
00:05:51.304 --> 00:05:54.514
The smaller the model, the cheaper it is
too and the faster that it is as well.

120
00:05:54.514 --> 00:05:58.834
So you got 4o mini,
you got all these different variations.

121
00:05:58.834 --> 00:06:01.907
So they have dates on them
because when models get trained,

122
00:06:01.907 --> 00:06:04.065
that's the training cutoff date.

123
00:06:04.065 --> 00:06:06.595
So they update those models with new data.

124
00:06:06.595 --> 00:06:11.075
So they put the date on the model name so
you know when that model got trained.

125
00:06:11.075 --> 00:06:14.296
So if you wanna ask that model some
information up until that date,

126
00:06:14.296 --> 00:06:16.915
if it got trained on that data, it should.

127
00:06:16.915 --> 00:06:18.405
So you just know the training cutoff date.

128
00:06:18.405 --> 00:06:19.855
So you'll see that.

129
00:06:19.855 --> 00:06:22.951
And then you have their o1,
mini o1 preview.

130
00:06:22.951 --> 00:06:24.861
This is like the cream of the crop.

131
00:06:24.861 --> 00:06:26.911
Best stuff out there right now.

132
00:06:26.911 --> 00:06:29.211
If you ran this,
you might pay a couple of dollars today.

133
00:06:29.211 --> 00:06:32.311
So, don't run o1 mini or o1 preview.

134
00:06:32.311 --> 00:06:37.011
Those models are mostly only good for
writing code.

135
00:06:37.011 --> 00:06:40.266
I would never use that for
anything else other than writing code or

136
00:06:40.266 --> 00:06:42.406
doing heavy math stuff, PhD level stuff.

137
00:06:42.406 --> 00:06:45.860
So everything else, you can just
do 4o mini for the most part, and

138
00:06:45.860 --> 00:06:49.816
then 4o if you want something bigger,
we're just gonna use 4o mini here.

139
00:06:51.066 --> 00:06:54.178
If you got big bucks, you can just do 4o,
if you're feeling it,

140
00:06:54.178 --> 00:06:57.866
I'm just gonna do 4o mini, because
it's super fast and it's really good.

141
00:06:57.866 --> 00:07:02.283
And then the next thing is this
concept called temperature.

142
00:07:02.283 --> 00:07:07.213
This is just a measurement of how
creative you want the model to be.

143
00:07:07.213 --> 00:07:10.203
The higher the number, the more
creative I think it maxes out at two.

144
00:07:11.213 --> 00:07:14.223
So I recommend just
putting it on point one.

145
00:07:14.223 --> 00:07:15.983
Just don't do anything
other than point one.

146
00:07:15.983 --> 00:07:20.093
You do anything other than that,
you're asking for it.

147
00:07:20.093 --> 00:07:21.353
So just put it on point one.

148
00:07:21.353 --> 00:07:26.999
So experiment with it later, see how
it goes, but just put it on point one.

149
00:07:26.999 --> 00:07:28.839
It reduces the chaos.

150
00:07:28.839 --> 00:07:30.019
Chaos is like entropy.

151
00:07:30.019 --> 00:07:32.439
It's the randomness that the AI has.

152
00:07:32.439 --> 00:07:33.969
This reduces the randomness.

153
00:07:33.969 --> 00:07:37.858
It's never gonna be deterministic,
but this reduces the randomness, but

154
00:07:37.858 --> 00:07:41.563
sometimes you do want that up,
maybe when you're generating an image or

155
00:07:41.563 --> 00:07:42.734
something like that.

156
00:07:42.734 --> 00:07:43.734
Who knows.

157
00:07:43.734 --> 00:07:45.024
And then messages.

158
00:07:45.024 --> 00:07:46.744
Messages is interesting.

159
00:07:46.744 --> 00:07:48.624
And we're going to dive
into this a little more.

160
00:07:48.624 --> 00:07:51.694
But messages are an array of objects
that represent the messages in this

161
00:07:51.694 --> 00:07:52.264
chat so far.

162
00:07:52.264 --> 00:07:54.724
All you need to know for
now is that they're objects.

163
00:07:54.724 --> 00:07:56.731
They all have something called a role.

164
00:07:56.731 --> 00:08:00.164
In this case the role as you can see,
there's a few of them.

165
00:08:00.164 --> 00:08:03.128
There's assistant function,
system, tool, user,

166
00:08:03.128 --> 00:08:05.785
because this is a user message,
the role is user.

167
00:08:05.785 --> 00:08:09.595
And then they have a content, which has
got to be the user message like that.

168
00:08:12.265 --> 00:08:17.045
And then finally,
we just want to return response choices.

169
00:08:17.045 --> 00:08:19.975
It's pretty much always going
to be the first thing in there.

170
00:08:19.975 --> 00:08:21.995
We get back the message and
we want to get the content on there.

171
00:08:21.995 --> 00:08:23.575
So this is what the AI responds back with.

172
00:08:25.005 --> 00:08:27.165
This is all you need to hit an AI.

173
00:08:27.165 --> 00:08:28.325
That's it.

174
00:08:28.325 --> 00:08:28.825
This is it.

175
00:08:30.045 --> 00:08:31.903
Done.
You have AI in your app now.

176
00:08:31.903 --> 00:08:33.342
You're done.

177
00:08:33.342 --> 00:08:36.562
[LAUGH] Everything else is
just like manipulating it and

178
00:08:36.562 --> 00:08:38.595
building something really cool.

179
00:08:38.595 --> 00:08:40.558
But this is it.
This is AI right here.

180
00:08:40.558 --> 00:08:43.106
Yeah, then the next thing is let's
just add this to our index file so

181
00:08:43.106 --> 00:08:44.098
we could actually use it.

182
00:08:44.098 --> 00:08:46.628
So I'm going to go back to my index file.

183
00:08:46.628 --> 00:08:51.498
I'm going to import
that in here like that.

184
00:08:51.498 --> 00:08:55.564
I'm going to say const
response equals await

185
00:08:55.564 --> 00:09:00.181
run the LLM with the user
message like that.

186
00:09:00.181 --> 00:09:01.671
Then I'm just going to log the response.

187
00:09:01.671 --> 00:09:04.431
Let's see if I broke anything.

188
00:09:04.431 --> 00:09:08.201
So NPM start, and then say something.

189
00:09:08.201 --> 00:09:09.301
I'm going to say, hi.

190
00:09:10.401 --> 00:09:11.481
Then yeah, it says, hello.

191
00:09:11.481 --> 00:09:12.801
How can I help you today?

192
00:09:12.801 --> 00:09:15.881
Right, and then I'm going to say, what?

193
00:09:15.881 --> 00:09:19.464
I'm going to say, my name is Scott.

194
00:09:19.464 --> 00:09:23.314
And it's gonna say, cool,
nice to meet you, Scott.

195
00:09:23.314 --> 00:09:25.534
And then I'm going to ask it,
what is my name?

196
00:09:27.534 --> 00:09:28.614
What do you think it's gonna say?

197
00:09:32.074 --> 00:09:33.114
&gt;&gt; Speaker 2: Nice to meet you?

198
00:09:33.114 --> 00:09:35.687
&gt;&gt; Scott Moss: You think he's gonna say,
your name is Scott, right?

199
00:09:35.687 --> 00:09:37.754
Okay, sure.

200
00:09:38.904 --> 00:09:42.499
No it doesn't [LAUGH]
because it's not an agent.

201
00:09:42.499 --> 00:09:43.679
It doesn't have memory.

202
00:09:43.679 --> 00:09:44.979
It's transactional.

203
00:09:44.979 --> 00:09:47.849
We didn't feed it the previous messages.

204
00:09:47.849 --> 00:09:50.509
It only knows the message we sent it and
that's it.

205
00:09:50.509 --> 00:09:53.849
So problem number one,
AI is very forgetful.

206
00:09:53.849 --> 00:09:55.619
It has no memory by default.

207
00:09:55.619 --> 00:09:58.359
You have to supply that,
models don't remember.

208
00:09:58.359 --> 00:10:03.305
So that's part of what building
an agent is, is adding memory to it.

209
00:10:03.305 --> 00:10:05.645
And then it's just
a spider web from there.

210
00:10:05.645 --> 00:10:06.785
Okay, cool.
Before we take a break,

211
00:10:06.785 --> 00:10:08.515
let me address the LangChain
question that we had.

212
00:10:08.515 --> 00:10:10.355
So first of all, what is LangChain?

213
00:10:10.355 --> 00:10:15.101
LangChain is a framework
that you can use to build

214
00:10:15.101 --> 00:10:19.982
AI applications, whether it's agents.

215
00:10:19.982 --> 00:10:24.289
These retrieval mechanisms, input sources,

216
00:10:24.289 --> 00:10:31.622
everything you need to build a production
ready AI integration in your product.

217
00:10:31.622 --> 00:10:32.942
LangChain strives to be that.

218
00:10:32.942 --> 00:10:36.612
Originally created in Python,
there is a TypeScript implementation.

219
00:10:36.612 --> 00:10:43.385
In my opinion, I think LangChain's
value today is not that high.

220
00:10:43.385 --> 00:10:48.595
I think it was really good if you were
a Python developer back in the 3.5 days

221
00:10:48.595 --> 00:10:53.567
with OpenAI because OpenAI's API was
still being developed and they were

222
00:10:53.567 --> 00:10:58.805
still trying to figure it out and
there wasn't really cool things right now.

223
00:10:58.805 --> 00:11:03.805
Like tool calling and structured outputs,
and providing OpenAI a schema.

224
00:11:03.805 --> 00:11:07.175
You can do a schema now, as you'll see,
and today we'll use that.

225
00:11:07.175 --> 00:11:09.905
They didn't have that back then,
so LangChain filled that void.

226
00:11:09.905 --> 00:11:14.009
If you want to get something back
in a very specific JSON format,

227
00:11:14.009 --> 00:11:15.985
LangChain had a way to do that.

228
00:11:15.985 --> 00:11:19.319
It would basically just call your thing or
repeat and repeat until it did it or

229
00:11:19.319 --> 00:11:23.569
have another LLM check it for you, but
you don't have to do those things anymore.

230
00:11:23.569 --> 00:11:26.059
So I feel like the value is not there.

231
00:11:26.059 --> 00:11:29.214
I don't know, I'll probably use LangChain
for like a month a couple years ago, and

232
00:11:29.214 --> 00:11:30.409
I've never used it again.

233
00:11:30.409 --> 00:11:33.307
And the other thing is if
you're a JavaScript developer,

234
00:11:33.307 --> 00:11:36.209
I have no idea who's doing
that stuff on LangChain.

235
00:11:36.209 --> 00:11:40.014
But the way that it's created and
the way that you use it, the APIs for

236
00:11:40.014 --> 00:11:42.382
it are not what you
would do in JavaScript.

237
00:11:42.382 --> 00:11:43.342
It's very weird.

238
00:11:43.342 --> 00:11:46.812
It's like they ran the Python
code through an AI.

239
00:11:46.812 --> 00:11:49.492
It's like, convert this to TypeScript,
and that was the output of it.

240
00:11:49.492 --> 00:11:52.082
Because it's just it's way too crazy.

241
00:11:52.082 --> 00:11:55.772
They have a million ways to do one thing,
and none of it makes any sense.

242
00:11:55.772 --> 00:11:57.822
So I personally recommend not using it.

243
00:11:57.822 --> 00:12:01.534
And everyone that I've ever talked
to that has an LLM Integration and

244
00:12:01.534 --> 00:12:03.653
production just uses the native APIs.

245
00:12:03.653 --> 00:12:05.543
They almost never use a framework.

246
00:12:05.543 --> 00:12:07.343
They just make their own thing.

247
00:12:07.343 --> 00:12:09.533
At my startup, we just made our own thing.

248
00:12:09.533 --> 00:12:11.603
We just generated our own agent.

249
00:12:11.603 --> 00:12:15.943
And what I'm teaching today is an
abstraction of what I built at my company.

250
00:12:15.943 --> 00:12:18.453
So it's stuff that I
actually use in production.

251
00:12:18.453 --> 00:12:19.753
So, I mean, give it a try.

252
00:12:19.753 --> 00:12:23.435
I think Personally, using LangChain is
going to confuse the hell out of you, and

253
00:12:23.435 --> 00:12:26.959
you're probably better off just doing
this stuff, and I say from scratch.

254
00:12:26.959 --> 00:12:31.306
But in reality, OpenAI is really good, and

255
00:12:31.306 --> 00:12:36.249
every other AI provider
uses the OpenAI API spec.

256
00:12:36.249 --> 00:12:40.691
So to replace OpenAI with Claude or
something like that.

257
00:12:40.691 --> 00:12:43.272
It's as simple as just changing
the name of a model or URL, and

258
00:12:43.272 --> 00:12:45.071
you don't have to change any of your code.

259
00:12:45.071 --> 00:12:46.351
It's exactly the same.

260
00:12:46.351 --> 00:12:49.392
Assuming that model has the same
capabilities as the one that you're using,

261
00:12:49.392 --> 00:12:50.391
it's exactly the same.

262
00:12:50.391 --> 00:12:52.761
So that's my opinion.

263
00:12:52.761 --> 00:12:53.851
I don't recommend LangChain.

