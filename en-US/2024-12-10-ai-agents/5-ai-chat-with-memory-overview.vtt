WEBVTT

1
00:00:00.080 --> 00:00:02.784
&gt;&gt; Scott Moss: A couple things,
I pushed up the changes to step two,

2
00:00:02.784 --> 00:00:05.080
which is what we're
gonna be working on now.

3
00:00:05.080 --> 00:00:08.699
So if you wanna get those changes, you can
go to step two, check out to that branch,

4
00:00:08.699 --> 00:00:10.220
and all the changes will be there.

5
00:00:10.220 --> 00:00:13.412
The other thing is, if you did make
a new API account with OpenAI for

6
00:00:13.412 --> 00:00:17.403
the model that I'm using four O Mini, you
might have to give yourself permission in

7
00:00:17.403 --> 00:00:21.451
the OpenAI to use that model if you got an
error saying you don't have permission, so

8
00:00:21.451 --> 00:00:25.972
just make sure you go on the dashboard and
give yourself permission to that model.

9
00:00:25.972 --> 00:00:27.775
You can also just try a different model,

10
00:00:27.775 --> 00:00:30.932
like 3.5 which I think everyone
has permission to by default.

11
00:00:30.932 --> 00:00:32.122
It's just not as good.

12
00:00:32.122 --> 00:00:33.842
So your results may vary.

13
00:00:33.842 --> 00:00:34.572
Use four O Mini.

14
00:00:34.572 --> 00:00:36.212
Let's actually make a chat.

15
00:00:36.212 --> 00:00:40.293
So like we talked about and then you
saw I asked, hey, my name is Scott,

16
00:00:40.293 --> 00:00:42.212
and then I said, what is my name?

17
00:00:42.212 --> 00:00:44.222
And it had no idea.

18
00:00:44.222 --> 00:00:47.482
So we, we will work up to that.

19
00:00:47.482 --> 00:00:52.195
But the first part of of making
this work is just making

20
00:00:52.195 --> 00:00:56.175
sure that there is some
form of chat history,

21
00:00:56.175 --> 00:01:01.638
some form of back and forth,
that I can have with this system.

22
00:01:01.638 --> 00:01:05.988
So it's not just transactional,
we wanna build that.

23
00:01:05.988 --> 00:01:10.793
The difference between one off LLM calls
again these are really great if you

24
00:01:10.793 --> 00:01:14.378
have everything that you need
to answer the question and

25
00:01:14.378 --> 00:01:18.280
you can provide it to the LLM,
you don't really need a chat.

26
00:01:18.280 --> 00:01:21.420
Like for instance, if you just want
it to summarize like a PDF and

27
00:01:21.420 --> 00:01:25.079
you have the PDF you can just upload the
PDF and ask it to summarize the PDF and

28
00:01:25.079 --> 00:01:29.094
you don't really need to chat history
unless you want follow-up questions, and

29
00:01:29.094 --> 00:01:32.508
then then you do but these are really
good for like standalone tasks.

30
00:01:32.508 --> 00:01:37.158
So and like an assistant use case this
might be book this counter meeting or

31
00:01:37.158 --> 00:01:41.808
send us email or something like that
just one off things, really great for

32
00:01:41.808 --> 00:01:46.608
answering questions, simple analyses,
cogeneration that I showed you,

33
00:01:46.608 --> 00:01:48.793
translation is a really good one.

34
00:01:48.793 --> 00:01:50.883
It's actually really good at that.

35
00:01:50.883 --> 00:01:52.418
Then you get into chat-based, right?

36
00:01:52.418 --> 00:01:56.610
So chat-based is, again,
maintains the conversation history,

37
00:01:56.610 --> 00:01:59.841
understands context from
the previous messages.

38
00:01:59.841 --> 00:02:04.052
So as long as it is our
responsibility to feed the LM all

39
00:02:04.052 --> 00:02:08.741
the messages in the history,
it won't do that by default.

40
00:02:08.741 --> 00:02:13.115
So what you'll see in a lot of like
chat apps is that they're passing

41
00:02:13.115 --> 00:02:16.301
the entire history around all the time.

42
00:02:16.301 --> 00:02:17.551
At least that's like how it used to be.

43
00:02:17.551 --> 00:02:18.891
I think people have gotten a lot smarter.

44
00:02:18.891 --> 00:02:20.295
Like they would literally,

45
00:02:20.295 --> 00:02:23.342
you'll send the whole all
the messages down to the front end.

46
00:02:23.342 --> 00:02:26.315
And then when someone sends a message, you
send all the messages back, which is like,

47
00:02:26.315 --> 00:02:27.302
why would you do that?

48
00:02:27.302 --> 00:02:29.552
You just save it in the database or
Redis or whatever.

49
00:02:29.552 --> 00:02:30.972
But a lot of people did that.

50
00:02:30.972 --> 00:02:33.992
But the AI needs all the messages.

51
00:02:33.992 --> 00:02:36.386
For now, we'll talk about different
techniques because eventually that

52
00:02:36.386 --> 00:02:37.122
doesn't work.

53
00:02:37.122 --> 00:02:41.448
But for now, if you want it to know
something you have to give it all

54
00:02:41.448 --> 00:02:44.054
the information that it needs to know.

55
00:02:44.054 --> 00:02:48.918
So the other thing about chat based
interactions is we had that in our code

56
00:02:48.918 --> 00:02:49.964
here, right?

57
00:02:49.964 --> 00:02:53.304
We had this right here, this role of user.

58
00:02:53.304 --> 00:02:57.084
When you get to a chat based interface,
you start to use the other roles like

59
00:02:57.084 --> 00:02:59.793
system and assistant, and
we'll talk about those.

60
00:02:59.793 --> 00:03:04.818
There's also another one called tool that
you will talk about that one too [LAUGH].

61
00:03:04.818 --> 00:03:08.928
And then there's higher token
usage because remember you get

62
00:03:08.928 --> 00:03:13.523
charged on all the tokens on your
input and the tokens on your output.

63
00:03:13.523 --> 00:03:18.155
So if you're sending every single message
that you've ever said on every single

64
00:03:18.155 --> 00:03:22.715
request, so the AI knows all the history
that you had so far, you get charged for

65
00:03:22.715 --> 00:03:24.587
those tokens too.

66
00:03:24.587 --> 00:03:27.149
So it's not just the new
message that you add,

67
00:03:27.149 --> 00:03:31.107
it's every message ever that you add it,
plus the new message.

68
00:03:31.107 --> 00:03:33.087
And those are the tokens
you get charged on.

69
00:03:33.087 --> 00:03:38.707
The more tokens you add, the more it's
gonna cost, the longer it's going to take.

70
00:03:38.707 --> 00:03:41.054
So as you can see, that's a problem.

71
00:03:41.054 --> 00:03:43.924
And a lot of startups
are solving just that problem.

72
00:03:43.924 --> 00:03:46.444
Literally just that problem,
a lot of companies are solving that.

73
00:03:46.444 --> 00:03:48.964
And there's many ways and
we'll talk about some of them.

74
00:03:48.964 --> 00:03:55.024
I'll address them and practicality in
the production course version of this.

75
00:03:55.024 --> 00:03:57.594
But yeah,
there are some constraints there.

76
00:03:57.594 --> 00:03:58.644
It does get expensive.

77
00:03:58.644 --> 00:04:00.882
And then, yeah,
obviously more complex state management.

78
00:04:00.882 --> 00:04:02.812
Again, you need to save
these messages somewhere.

79
00:04:02.812 --> 00:04:05.572
You got to put them in a database and
different things like that.

80
00:04:05.572 --> 00:04:10.492
And we're gonna have like a database
in this course that we use as a file.

81
00:04:10.492 --> 00:04:13.412
But you'll see like the different
constraints and things around that.

82
00:04:13.412 --> 00:04:15.772
So, it just it just
complicates it a little more.

83
00:04:15.772 --> 00:04:18.472
If you wanna do a chat you
got to think about things.

84
00:04:18.472 --> 00:04:22.246
One thing though,
like as I said It does get more expensive,

85
00:04:22.246 --> 00:04:25.280
it goes up every message,
it's gonna go up, and

86
00:04:25.280 --> 00:04:29.350
the thing you have to think about
is when we get into tool calling.

87
00:04:30.420 --> 00:04:35.082
Sometimes, like for instance, if you
had a function that allowed the AI to

88
00:04:35.082 --> 00:04:39.522
search the web, and it searched the web,
and it came back with like 10

89
00:04:39.522 --> 00:04:44.951
full-blown HTML pages, and then that's
what you return back to the chat.

90
00:04:44.951 --> 00:04:47.852
Guess how many tokens that's
going to eat up in your costs,

91
00:04:47.852 --> 00:04:51.131
just so it can answer this one
question that you asked it.

92
00:04:51.131 --> 00:04:52.261
And it's always there.

93
00:04:52.261 --> 00:04:54.801
And you only ask that question once,
but it's still in your history.

94
00:04:54.801 --> 00:04:57.441
And then you keep adding and then you
do another thing that adds, right?

95
00:04:57.441 --> 00:05:00.439
Like eventually you run out of memory.

96
00:05:00.439 --> 00:05:02.639
And the token count goes up over time.

97
00:05:02.639 --> 00:05:05.269
So those are like things
you have to think about.

98
00:05:05.269 --> 00:05:08.679
And then once you hit that
context window size, you're done.

99
00:05:08.679 --> 00:05:12.668
So this is why sometimes like in Clawed,
if you use Clawed online, it'll like,

100
00:05:12.668 --> 00:05:15.486
eventually, once you start using it so
much in one chat,

101
00:05:15.486 --> 00:05:19.689
it'll have like a little thing at
the bottom says longer conversations.

102
00:05:19.689 --> 00:05:22.483
Or it'll be like a, I forgot exactly
what it says, but it's like, hey,

103
00:05:22.483 --> 00:05:24.279
you should start a new chat.

104
00:05:24.279 --> 00:05:28.589
You need to make a new chat because
like this, this is getting too long.

105
00:05:28.589 --> 00:05:33.837
GPT solution is they randomly,
depending on what you say, we'll take

106
00:05:33.837 --> 00:05:39.269
what you said and it'll say like
updating memory or something like that.

107
00:05:39.269 --> 00:05:42.836
And it'll, it'll remember what you just
said and save it to a database, so

108
00:05:42.836 --> 00:05:46.770
it doesn't have to feed all of your chat
messages over, and over, and over again.

109
00:05:46.770 --> 00:05:49.724
It just remembers the important
things that you said, and

110
00:05:49.724 --> 00:05:51.200
it feeds those summaries.

111
00:05:51.200 --> 00:05:54.573
It just summarizes everything you said so
far versus feeding word for

112
00:05:54.573 --> 00:05:56.330
word everything you said.

113
00:05:56.330 --> 00:05:58.200
So some things get lost,
some things don't.

114
00:05:58.200 --> 00:06:00.462
It's not exactly what you said,
but it's a summary, and

115
00:06:00.462 --> 00:06:03.123
that's one technique that
you'll see happen a lot.

116
00:06:03.123 --> 00:06:06.303
So lots of things.

117
00:06:06.303 --> 00:06:06.963
Cool, okay.

118
00:06:06.963 --> 00:06:08.883
So common patterns.

119
00:06:08.883 --> 00:06:11.183
We did the one-off pattern
just now recently, right?

120
00:06:11.183 --> 00:06:14.713
You literally call complete,
you give it a thing.

121
00:06:14.713 --> 00:06:15.693
It'll do a thing.

122
00:06:15.693 --> 00:06:18.774
This is just some example code,
it's not exactly our abstraction, but

123
00:06:18.774 --> 00:06:19.923
I think you get the point.

124
00:06:19.923 --> 00:06:22.368
This is, for instance, if you wanted to.

125
00:06:22.368 --> 00:06:26.842
And if you had an app
that generated topics for

126
00:06:26.842 --> 00:06:30.845
something, you could pass in a topic and

127
00:06:30.845 --> 00:06:36.048
then it'll generate a description for
that topic.

128
00:06:36.048 --> 00:06:38.258
So this is an example of
doing a one off thing.

129
00:06:38.258 --> 00:06:41.861
This is how you can incorporate AI
that literally just take something and

130
00:06:41.861 --> 00:06:43.928
completes and
does one thing at the end and

131
00:06:43.928 --> 00:06:46.563
you don't have to worry about like a chat.

132
00:06:46.563 --> 00:06:47.983
So this is a great example of that.

133
00:06:47.983 --> 00:06:52.142
But then you get into like a chat pattern
and it's very similar to like what we just

134
00:06:52.142 --> 00:06:55.333
did where we have to pass in
the messages an array of messages.

135
00:06:55.333 --> 00:06:56.463
But as you can see,

136
00:06:56.463 --> 00:07:01.713
the difference here is that like we prefix
those messages with the previous messages.

137
00:07:01.713 --> 00:07:04.358
So new messages go at
the end of the array.

138
00:07:04.358 --> 00:07:08.548
So, older messages are at the beginning
and then you add the new user one.

139
00:07:08.548 --> 00:07:10.288
And you have to do that every single time.

140
00:07:10.288 --> 00:07:14.858
So, like, you're passing in
the entire history every single time.

141
00:07:14.858 --> 00:07:16.828
So, it knows what the history is.

142
00:07:16.828 --> 00:07:20.538
So, that's pretty much
the big difference there.

143
00:07:20.538 --> 00:07:25.067
The big thing that you'll
have to do with chat

144
00:07:25.067 --> 00:07:30.205
based LLM stuff is managing history and
tokens.

145
00:07:30.205 --> 00:07:33.539
That's the biggest thing because
if you run out of tokens,

146
00:07:33.539 --> 00:07:36.885
your AI will never be able to
answer a question going for it.

147
00:07:36.885 --> 00:07:40.363
So imagine you just keep sending it
messages and then eventually it's like

148
00:07:40.363 --> 00:07:44.246
nope, nope, and it and because you
keep adding it, it just gets bigger.

149
00:07:44.246 --> 00:07:47.972
So you have to figure out what
is the best strategy for you,

150
00:07:47.972 --> 00:07:52.082
or strategies for your application
to handle all of this history

151
00:07:52.082 --> 00:07:55.766
that you need to provide if
you wanna maintain a chat.

152
00:07:55.766 --> 00:07:57.263
You don't have to do
that with a one-off one.

153
00:07:57.263 --> 00:08:00.526
The one-off one doesn't even know
what the last thing you said was.

154
00:08:00.526 --> 00:08:02.341
So it's a lot simpler.

155
00:08:02.341 --> 00:08:05.527
If you're not gonna build a chat
interface of somebody's chatting,

156
00:08:05.527 --> 00:08:06.873
you don't need to chat LM.

157
00:08:06.873 --> 00:08:08.803
How is what they talk to it?

158
00:08:08.803 --> 00:08:11.423
Unless you're doing like over
an email thread or something.

159
00:08:11.423 --> 00:08:14.813
If you otherwise like just
use a transactional one.

160
00:08:14.813 --> 00:08:19.693
And then, if you are building a chat
base UI UX, then you do need a chat LM.

161
00:08:19.693 --> 00:08:21.586
You wouldn't wanna do one off LLM,

162
00:08:21.586 --> 00:08:25.063
calls inside of a chat
that'd be really confusing.

163
00:08:25.063 --> 00:08:26.153
Nobody would know why.

164
00:08:26.153 --> 00:08:27.463
It doesn't remember anything.

165
00:08:27.463 --> 00:08:30.113
It has the shortest term memory,
and nothing would work.

166
00:08:30.113 --> 00:08:34.943
So that's how you decide,
are you building a chat app, Chat LLM?

167
00:08:34.943 --> 00:08:37.133
Are you not building a chat app?

168
00:08:37.133 --> 00:08:39.480
Probably not Chat LLM, that's about it.

