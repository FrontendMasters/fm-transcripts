WEBVTT

1
00:00:00.320 --> 00:00:05.570
&gt;&gt; Scott Moss: Any questions on
creating an agent from scratch?

2
00:00:05.570 --> 00:00:09.070
Because I'm gonna tell you, everything
that I just taught you today is literally

3
00:00:09.070 --> 00:00:11.810
the foundation of what we're
using in production right now.

4
00:00:11.810 --> 00:00:15.256
And what I can promise you what,
other tools with hundreds of thousands,

5
00:00:15.256 --> 00:00:18.270
if not millions of users are also
using in production right now.

6
00:00:18.270 --> 00:00:21.529
It's the same foundation, and
everything else that they have in it,

7
00:00:21.529 --> 00:00:24.526
it's just like guard rails,
performance, things like that.

8
00:00:24.526 --> 00:00:28.441
But this foundation,
this is really what people start with, and

9
00:00:28.441 --> 00:00:32.156
it's just more of the same, so
you really do have an example.

10
00:00:32.156 --> 00:00:35.946
You've went through it, you've seen how
people build with this stuff today.

11
00:00:35.946 --> 00:00:38.937
Because let's be fair,
there aren't any experts today.

12
00:00:38.937 --> 00:00:40.713
[LAUGH] There's no such
thing as an expert.

13
00:00:40.713 --> 00:00:43.302
Everyone is just figuring
this stuff out as they go.

14
00:00:43.302 --> 00:00:44.247
I get on Twitter and

15
00:00:44.247 --> 00:00:48.373
there's this person on Twitter who's
like a day ahead of me on something.

16
00:00:48.373 --> 00:00:51.344
And that person's on the edge, and
I'm like, okay, nobody knows this,

17
00:00:51.344 --> 00:00:52.913
we're all just floundering, right?

18
00:00:52.913 --> 00:00:55.558
So this stuff's just moving really,
really quickly,

19
00:00:55.558 --> 00:00:57.483
more quickly than we can keep up with it.

20
00:00:57.483 --> 00:01:01.896
So if you're feeling overwhelmed by it,
so am I, so is everyone else.

21
00:01:01.896 --> 00:01:03.509
But it doesn't mean you
can't use it today,

22
00:01:03.509 --> 00:01:05.386
it doesn't mean you can't
get value from it today.

23
00:01:05.386 --> 00:01:09.272
So trust me when I tell you what you
have here is literally what everyone is

24
00:01:09.272 --> 00:01:10.586
doing right now, yes?

25
00:01:10.586 --> 00:01:12.366
&gt;&gt; Speaker 1: Where do you like
to go to stand the cutting edge?

26
00:01:12.366 --> 00:01:13.886
Is that Twitter usually?

27
00:01:13.886 --> 00:01:16.795
&gt;&gt; Scott Moss: Twitter is really good, but
only if you're following the right people.

28
00:01:16.795 --> 00:01:20.715
So if you're not following the right
people, I would say Hugging Face is great,

29
00:01:20.715 --> 00:01:22.284
I love Hugging Face.

30
00:01:22.284 --> 00:01:25.852
It's like the Github of AI models, I
highly recommend going to Hugging Face and

31
00:01:25.852 --> 00:01:28.794
playing around with some of these models,
they're super cool.

32
00:01:29.834 --> 00:01:32.792
They're really easy to get started with,
some of them, just like APIs,

33
00:01:32.792 --> 00:01:34.284
you can just use them, super great.

34
00:01:35.754 --> 00:01:38.564
That and Hacker News and
a bunch of random Discords and

35
00:01:38.564 --> 00:01:41.010
Reddits is pretty much
where I stay up to date.

36
00:01:43.020 --> 00:01:44.470
Yeah, Mark?

37
00:01:44.470 --> 00:01:49.696
&gt;&gt; Mark: Is Google's AI worth looking
at or just go all in on OpenAI?

38
00:01:49.696 --> 00:01:54.136
&gt;&gt; Scott Moss: Gemini.

39
00:01:54.136 --> 00:01:57.150
&gt;&gt; Mark: Or even Llama?

40
00:01:57.150 --> 00:02:00.617
&gt;&gt; Scott Moss: Do not use Llama
unless you are LLM ops person and

41
00:02:00.617 --> 00:02:04.952
you know how to fine tune and
train and you got tons of data and

42
00:02:04.952 --> 00:02:09.403
cutting costs while increasing
performance is your goal.

43
00:02:09.403 --> 00:02:11.601
If that's not your goal, if you just
wanna build an app that use AI,

44
00:02:11.601 --> 00:02:12.973
you should not be using Llama.

45
00:02:12.973 --> 00:02:16.920
Llama is the end goal of a company that
is successful and wants to save money and

46
00:02:16.920 --> 00:02:18.483
keep the same performance.

47
00:02:18.483 --> 00:02:24.864
It's not something you should start with,
unless your product is offline.

48
00:02:24.864 --> 00:02:28.587
We only run on people's computers and our
customers are only people who have extreme

49
00:02:28.587 --> 00:02:30.329
GPUs, and yeah, I guess, go do that.

50
00:02:30.329 --> 00:02:36.129
But no, you probably won't do that,
because then you got to find out where

51
00:02:36.129 --> 00:02:42.226
you're gonna host this thing and like
GPU infrastructure right now is crazy.

52
00:02:42.226 --> 00:02:45.775
Google's Gemini thing is actually really
good, I'll admit it is really good.

53
00:02:45.775 --> 00:02:48.644
The problem with it is that it's so
hard to set up, it's so

54
00:02:48.644 --> 00:02:50.175
hard to set up Google's APIs.

55
00:02:50.175 --> 00:02:53.405
You got to go do this, you got to set
this up, you got to make an account.

56
00:02:53.405 --> 00:02:57.562
It takes 30 minutes to set up an account
with Google to start using their APIs,

57
00:02:57.562 --> 00:03:00.876
whereas, everything else is
like five minutes or less.

58
00:03:00.876 --> 00:03:04.216
&gt;&gt; Mark: Yeah, I saw levels on Twitter.

59
00:03:04.216 --> 00:03:05.216
&gt;&gt; Scott Moss: You talked about this?

60
00:03:05.216 --> 00:03:07.365
&gt;&gt; Mark: Yeah, today,
he was complaining about it.

61
00:03:07.365 --> 00:03:10.361
&gt;&gt; Scott Moss: Wow, [LAUGH]
&gt;&gt; Mark: And then Tyler on chat says that

62
00:03:10.361 --> 00:03:16.487
Hugging Face even has some great tutorials
that he's been following and yeah, cool.

63
00:03:16.487 --> 00:03:18.494
&gt;&gt; Scott Moss: Yes,
Hugging Face has a great tutorial, so

64
00:03:18.494 --> 00:03:20.147
there's this really cool thing.

65
00:03:20.147 --> 00:03:22.307
We talked about transformers, right?

66
00:03:22.307 --> 00:03:27.723
So check this out, transformers js,

67
00:03:27.723 --> 00:03:30.947
ooh, is this the one?

68
00:03:32.247 --> 00:03:37.201
Yes, yes, yes, so there's this really
cool thing, and is this the one?

69
00:03:37.201 --> 00:03:43.171
Hold on, okay, yeah, so this is really
cool thing of Python called transformers.

70
00:03:43.171 --> 00:03:45.937
The best way I can think about is,
it's like a framework for

71
00:03:45.937 --> 00:03:47.331
combining models together.

72
00:03:48.831 --> 00:03:50.194
They have it in JavaScript now, so

73
00:03:50.194 --> 00:03:53.627
there's some really cool models out there
that can do some really interesting stuff.

74
00:03:53.627 --> 00:03:57.563
And now you can just use all that same
stuff that you could in Python in

75
00:03:57.563 --> 00:04:02.558
JavaScript using transformers, so I highly
recommend checking out transformers.

76
00:04:02.558 --> 00:04:06.542
I actually use this a lot on my
gaming computer to generate images.

77
00:04:06.542 --> 00:04:11.116
I train images on, my friends faces,
my cat, all types of stuff,

78
00:04:11.116 --> 00:04:13.498
and I just generate stuff for fun.

79
00:04:13.498 --> 00:04:16.810
So transformers kind of
makes that possible, yes?

80
00:04:16.810 --> 00:04:19.717
&gt;&gt; Mark: Do you recommend any tools for
doing evals?

81
00:04:21.367 --> 00:04:23.930
&gt;&gt; Scott Moss: I'm going to get
into that in the next course,

82
00:04:23.930 --> 00:04:25.257
we will be doing evals.

83
00:04:25.257 --> 00:04:29.775
But as a preview to that,
I do use Braintrust.

84
00:04:29.775 --> 00:04:32.630
Personally, I use Braintrust data,
it's not something you should use for

85
00:04:32.630 --> 00:04:35.087
your personal products,
because it's super expensive.

86
00:04:35.087 --> 00:04:39.234
Yeah, I use them to evaluate and
log and do everything.

87
00:04:39.234 --> 00:04:41.960
But honestly, you don't even need.

88
00:04:41.960 --> 00:04:45.820
This is just a great place for
a dashboard and collecting your data,

89
00:04:45.820 --> 00:04:49.274
all the real value you can get
just from an open source tool and

90
00:04:49.274 --> 00:04:53.212
just logging it yourself and
keeping track of it in your own files.

91
00:04:53.212 --> 00:04:56.603
This is just like a UI for
it to be honest, but it is valuable,

92
00:04:56.603 --> 00:04:57.811
don't get me wrong.

93
00:04:57.811 --> 00:05:01.449
And there's tons of different eval
stuff and I'll cover a lot of them.

94
00:05:01.449 --> 00:05:04.801
It's just that right now a lot
of them are very Python heavy,

95
00:05:04.801 --> 00:05:08.481
it's really hard to get into
some of the JavaScript stuff.

96
00:05:08.481 --> 00:05:11.670
But I like Braintrust because
the founder is really cool, and

97
00:05:11.670 --> 00:05:13.211
they're TypeScript shops.

98
00:05:13.211 --> 00:05:16.057
They really love TypeScripts,
they always like do that.

99
00:05:16.057 --> 00:05:19.781
And then the really cool thing is,
we go to their blog they have, or

100
00:05:19.781 --> 00:05:21.923
maybe it's their docs, let's see.

101
00:05:21.923 --> 00:05:25.873
Yeah, here we go, cookbook,
this one's really cool, so

102
00:05:25.873 --> 00:05:30.245
they have a cookbook on how
to evaluate certain systems.

103
00:05:30.245 --> 00:05:34.718
And there's this one that they have in
here from Zapier, that's really cool,

104
00:05:34.718 --> 00:05:38.806
of how Zapier evaluates its tool usage
in their chat box and TypeScript.

105
00:05:38.806 --> 00:05:42.373
And it literally walks you through
how they evaluate whether or

106
00:05:42.373 --> 00:05:45.874
not the right tools are being
called in their chat system, and

107
00:05:45.874 --> 00:05:48.916
then how they can improve it,
and it's really cool.

108
00:05:48.916 --> 00:05:53.580
I actually use most of this in my eval
framework, so these cookbooks have been

109
00:05:53.580 --> 00:05:58.563
great because I'm not a data scientist,
it's really hard for me to understand.

110
00:05:58.563 --> 00:06:01.871
I don't even know what metric I should be
picking, there's so many different types

111
00:06:01.871 --> 00:06:05.197
of metrics that are very scientific, and
I don't know what any of this stuff means.

112
00:06:05.197 --> 00:06:07.747
So this stuff really is very helpful.

113
00:06:09.347 --> 00:06:14.807
&gt;&gt; Speaker 2: So back to Llama, and
one of the use case scenarios that

114
00:06:14.807 --> 00:06:21.695
I've run into is where company
doesn't necessarily want to be using

115
00:06:21.695 --> 00:06:27.277
an AI service because of
privacy of their information or

116
00:06:27.277 --> 00:06:32.527
regulatory type things,
or government agencies.

117
00:06:32.527 --> 00:06:36.470
What kind of alternatives?

118
00:06:36.470 --> 00:06:40.103
I mean, what's your take on that,
and how do you?

119
00:06:40.103 --> 00:06:42.459
&gt;&gt; Scott Moss: I don't blame them,
I don't want your OpenAI, on my stuff.

120
00:06:42.459 --> 00:06:45.850
So yeah,
I think there's a lot of levels to it.

121
00:06:45.850 --> 00:06:49.048
I think the first level is,
if you use something like OpenAI or

122
00:06:49.048 --> 00:06:52.998
anthropic, a lot of them have clauses
where you can say, a no train clause.

123
00:06:52.998 --> 00:06:54.947
Well, first of all, I think by default,

124
00:06:54.947 --> 00:06:57.813
they don't train on anything
that's sent in by the API.

125
00:06:57.813 --> 00:07:01.053
They only train on things that
are sent in through like ChatGPT or

126
00:07:01.053 --> 00:07:03.933
so they say, but
you can also work with OpenAI to be like,

127
00:07:03.933 --> 00:07:07.073
don't train any of my stuff,
and they should oblige to that.

128
00:07:07.073 --> 00:07:11.507
If you don't feel comfortable with that,
or you just want something that's like

129
00:07:11.507 --> 00:07:15.503
more underneath on your infrastructure,
OpenAI partnered with Azure.

130
00:07:15.503 --> 00:07:19.369
So you can get OpenAI, you can get
a lot of their models on Azure, or

131
00:07:19.369 --> 00:07:23.168
anthropic partner with AWS, so
you can get sonnet 3.5 which,

132
00:07:23.168 --> 00:07:27.194
in my opinion, is really great on AWS,
on your own infrastructure.

133
00:07:27.194 --> 00:07:28.984
So you can have that as well.

134
00:07:28.984 --> 00:07:31.824
&gt;&gt; Speaker 2: Still public cloud
infrastructure, though, right?

135
00:07:31.824 --> 00:07:34.323
&gt;&gt; Scott Moss: Yeah, I mean, it's still
on AWS, you could put it in a VPC,

136
00:07:34.323 --> 00:07:36.104
you're not gonna have it on your own rack.

137
00:07:36.104 --> 00:07:39.970
Yeah, they're not gonna give you the file
for it, yeah, so that's like that, and

138
00:07:39.970 --> 00:07:42.433
then you move further down,
then it's like, yeah.

139
00:07:42.433 --> 00:07:46.466
At that point, you do need to do,
a Llama or a Mistral, or

140
00:07:46.466 --> 00:07:51.393
whatever else is out there, and
you're gonna need fine tuning.

141
00:07:51.393 --> 00:07:54.702
You're gonna need a lot of fine tuning
to get it to where you want it to be,

142
00:07:54.702 --> 00:07:56.783
which means you're gonna
need lots of data.

143
00:07:56.783 --> 00:08:01.494
So that's why I think what you wanna
do is you wanna start with one of

144
00:08:01.494 --> 00:08:06.045
the other suggestions,
collect the data that you need.

145
00:08:06.045 --> 00:08:09.717
Once you reach a threshold,
use that data to go train Llama, and

146
00:08:09.717 --> 00:08:14.545
then now you can throw Llama on whatever,
hardware, whatever racks that you have.

147
00:08:14.545 --> 00:08:17.562
If you have the compute for it,
then you can do it there, but, yeah,

148
00:08:17.562 --> 00:08:19.034
that's what I would recommend.

149
00:08:20.584 --> 00:08:24.604
&gt;&gt; Mark: Have you used AI services by AWS?

150
00:08:25.634 --> 00:08:31.052
&gt;&gt; Scott Moss: I have, yeah,
we have sonnet 3.5 on AWS, and honestly,

151
00:08:31.052 --> 00:08:35.534
it's actually pretty good,
I'm not gonna lie.

152
00:08:35.534 --> 00:08:39.721
I don't really like AWS, it's kind
of confusing and weird UI, but they

153
00:08:39.721 --> 00:08:44.266
have a lot of really good tools on there,
and they have lots and lots of models.

154
00:08:44.266 --> 00:08:48.034
Pretty much every model that
isn't OpenAI is on AWS, and

155
00:08:48.034 --> 00:08:52.836
they have prompt management tools,
they have evaluation frameworks.

156
00:08:52.836 --> 00:08:57.866
They have a lot of stuff on AWS to
help you, and it's just good enough.

157
00:08:57.866 --> 00:09:01.588
So, yeah, I haven't had any issues with
it, I think it's pretty good, yep.

158
00:09:01.588 --> 00:09:06.714
&gt;&gt; Mark: There's questions
around fine tuning a model and

159
00:09:06.714 --> 00:09:10.628
then continuing to update the model.

160
00:09:10.628 --> 00:09:12.559
Do you have any experience?

161
00:09:12.559 --> 00:09:16.863
&gt;&gt; Scott Moss: I do, I have experience
fine tuning models, LLMs and

162
00:09:16.863 --> 00:09:18.498
diffusion models.

163
00:09:18.498 --> 00:09:25.588
So if you think of a model,
it's a foundational model, it's trained.

164
00:09:25.588 --> 00:09:31.893
Fine tuning is like just training
the last mile, the last 2% of that model.

165
00:09:31.893 --> 00:09:36.613
And typically, you would fine tune if you
wanted the output to look a certain way.

166
00:09:36.613 --> 00:09:40.209
It's less about accuracy,
it's more like I wanna save tokens, and

167
00:09:40.209 --> 00:09:44.363
I want the output to always look a certain
way or instance, an example would be,

168
00:09:44.363 --> 00:09:48.456
if we want our AI to respond like
an executive system would respond.

169
00:09:48.456 --> 00:09:52.012
I would go get a bunch of inputs and
example outputs, or

170
00:09:52.012 --> 00:09:56.882
I get a bunch of inputs and I would go
hire an army of executive assistants and

171
00:09:56.882 --> 00:10:00.390
say, respond to this the way
that you would respond.

172
00:10:00.390 --> 00:10:04.050
And then I would take that and I would
fine tune a model like this is how you

173
00:10:04.050 --> 00:10:07.050
respond in this vocabulary,
this type of stuff.

174
00:10:07.050 --> 00:10:10.960
So it feels and sounds a certain way, and

175
00:10:10.960 --> 00:10:17.637
that's really the only way to get to
what I call SME level of a of an AI.

176
00:10:17.637 --> 00:10:19.725
Cuz right now, OpenAI is just so generic,

177
00:10:19.725 --> 00:10:23.977
you can just tell when someone generates
something with OpenAI or any API.

178
00:10:23.977 --> 00:10:27.004
But if it's fine tuned,
then it kind of becomes really blurry,

179
00:10:27.004 --> 00:10:29.657
cuz it's very specific
if it was trained well.

180
00:10:29.657 --> 00:10:32.952
So fine tuning is mostly for
that, and then,

181
00:10:32.952 --> 00:10:37.864
updating the model as you go,
fine tuning is really expensive.

182
00:10:37.864 --> 00:10:42.164
So there's not something that you
would just be doing all the time.

183
00:10:42.164 --> 00:10:43.309
There really is no need for,

184
00:10:43.309 --> 00:10:46.044
there's just like you need to give
it access to more relevant data.

185
00:10:46.044 --> 00:10:48.764
That's what RAG is for,
that's what system prompts are for.

186
00:10:48.764 --> 00:10:51.716
If you need to update it because
the weights have changed,

187
00:10:51.716 --> 00:10:55.816
because the output that you're expecting
is different than when you originally

188
00:10:55.816 --> 00:10:58.422
fine tune it, then, yeah,
I guess that's fine.

189
00:10:58.422 --> 00:11:02.553
But if you're fine tuning constantly all
the time, I would imagine you need to

190
00:11:02.553 --> 00:11:06.246
rethink your architecture to where
you don't need to be doing that and

191
00:11:06.246 --> 00:11:09.334
probably rely on something else,
like RAG or something.

