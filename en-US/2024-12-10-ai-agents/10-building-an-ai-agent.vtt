WEBVTT

1
00:00:00.200 --> 00:00:04.994
Let's make our agent follow along with me
and we're gonna transition to pretty much

2
00:00:04.994 --> 00:00:09.130
building an abstraction on top of
the LLM function that we already made.

3
00:00:09.130 --> 00:00:12.643
We're just going to take a lot
of stuff that we already did,

4
00:00:12.643 --> 00:00:16.931
make an agent function that calls the LLM
function and add some UI to it and

5
00:00:16.931 --> 00:00:20.395
then a tool because that's what
kind of makes it an agent.

6
00:00:20.395 --> 00:00:23.568
And then we'll stop right there, and
you'll see, just like, it's really not

7
00:00:23.568 --> 00:00:27.036
complete, we kind of got to take it a step
further, but that's what we'll start with.

8
00:00:27.036 --> 00:00:31.097
So let's go into agent, should be
a blank file here, and essentially what

9
00:00:31.097 --> 00:00:35.166
we're gonna do is we're gonna create
this abstraction called runAgent.

10
00:00:35.166 --> 00:00:37.764
You can see it takes in a userMessage,
and from there,

11
00:00:37.764 --> 00:00:41.268
it kind of does the same thing we're
already doing in the index file file,

12
00:00:41.268 --> 00:00:44.169
we're just gonna abstract it even more,
so let's do that.

13
00:00:47.011 --> 00:00:51.964
&gt;&gt; Scott Moss: RunAgents = async (),
it's gonna take

14
00:00:51.964 --> 00:00:56.916
this user message here for now, and then,

15
00:00:56.916 --> 00:01:03.014
okay, I don't want you
to do any of that stuff.

16
00:01:03.014 --> 00:01:09.124
Get my AIMessage, get my addMessage,
get my runLLM, cool.

17
00:01:09.124 --> 00:01:11.012
Sometimes AI is good when it does that,

18
00:01:11.012 --> 00:01:14.444
those are some good quality
autocompletes there, I like that.

19
00:01:14.444 --> 00:01:16.699
But now what it's trying
to do is just annoying,

20
00:01:16.699 --> 00:01:18.844
it thinks it knows that I want this and
I do, but

21
00:01:18.844 --> 00:01:22.026
I don't want it to know that I do
because then it'll keep doing it.

22
00:01:22.026 --> 00:01:24.576
So, I'm just gonna ignore it, I don't
wanna think it's right all the time.

23
00:01:25.666 --> 00:01:27.199
So, the first thing we wanna do is,

24
00:01:27.199 --> 00:01:30.616
we wanna add the user message to
the database, so, let's do that.

25
00:01:30.616 --> 00:01:34.673
We can say await
addMessages{[{role: 'user',

26
00:01:34.673 --> 00:01:38.476
content: userMessage}]}, so, we got that.

27
00:01:40.116 --> 00:01:42.954
The next thing we wanna do is,
I'm gonna show this loader this,

28
00:01:42.954 --> 00:01:45.443
this is the thing in the terminal
where it's a spinner.

29
00:01:45.443 --> 00:01:48.413
It's like, I'm thinking, it's just letting
you know that, like it's doing something.

30
00:01:48.413 --> 00:01:52.521
So it feels like more alive, so

31
00:01:52.521 --> 00:02:00.445
you can import that from UI,
yeah, okay, there we go.

32
00:02:00.445 --> 00:02:03.682
Okay, can you stop doing that?

33
00:02:04.842 --> 00:02:09.461
Thank you, okay, so we got that,
so we add the message,

34
00:02:09.461 --> 00:02:13.812
we make a loader by just
saying loader = showLoader.

35
00:02:13.812 --> 00:02:16.712
Then you can put like the first message
you want, I'm just gonna put thinking.

36
00:02:16.712 --> 00:02:18.963
You can put whatever you want,
whatever you want your AI to say,

37
00:02:18.963 --> 00:02:19.682
put an emoji there.

38
00:02:19.682 --> 00:02:26.594
Actually, that might be a cool one, let
me see, it's like a thinking one, right?

39
00:02:26.594 --> 00:02:30.494
Yeah, there we go, that'll be cool,
a thinking emoji, that'd be kinda cool.

40
00:02:30.494 --> 00:02:35.122
So we could do that,
then we need to get all the messages, so

41
00:02:35.122 --> 00:02:37.264
we can feed it into our LLM.

42
00:02:37.264 --> 00:02:42.633
So we'll do that,
call this history awaits,

43
00:02:42.633 --> 00:02:48.073
getMessages like so call that history.

44
00:02:48.073 --> 00:02:52.982
So we're gonna say response,
we're gonna run the LLM, and

45
00:02:52.982 --> 00:02:59.923
we're just gonna pass in all the messages,
which, in this case, I call it history.

46
00:03:01.073 --> 00:03:04.643
So we're gonna do that pass
in all the history, and

47
00:03:04.643 --> 00:03:09.279
then the other thing we want to
do is this thing called tools.

48
00:03:09.279 --> 00:03:14.256
So tools is gonna be actually,
let's just dive into it, so essentially,

49
00:03:14.256 --> 00:03:20.109
what we're gonna do here is, if we go into
run, LLM, right now, it takes in messages.

50
00:03:20.109 --> 00:03:22.773
We're gonna go to another
argument called tools.

51
00:03:25.420 --> 00:03:28.371
&gt;&gt; Scott Moss: And we're gonna say tools.

52
00:03:28.371 --> 00:03:32.689
And for now, you can just make it
actually, I think I have a type for this,

53
00:03:32.689 --> 00:03:35.625
let me see, no,
I know I don't have a type for it.

54
00:03:35.625 --> 00:03:40.533
You could just say, if you want to do
types, you can just make it any array for

55
00:03:40.533 --> 00:03:42.704
now, so it's gonna take tools.

56
00:03:42.704 --> 00:03:49.080
In this chat completions, you're gonna
just pass those tools in like this, but

57
00:03:49.080 --> 00:03:54.236
not before formatting them in a way
that open AI wasn't the beat,

58
00:03:54.236 --> 00:03:58.962
because we're gonna define our
tools as like a Zod schema.

59
00:03:58.962 --> 00:04:02.353
So if you don't know what Zod is,
it's kind of like, I don't know,

60
00:04:02.353 --> 00:04:03.660
it's a it's a schema.

61
00:04:03.660 --> 00:04:06.053
You give it an object,
you give it a schema,

62
00:04:06.053 --> 00:04:09.700
it'll tell you if the object is
validated by the schema or not.

63
00:04:09.700 --> 00:04:13.580
It happens at runtime, same thing you'll
do with a database just in your code.

64
00:04:13.580 --> 00:04:16.897
What we wanna do is just pass the tools
in here, and then now we need to, like,

65
00:04:16.897 --> 00:04:19.807
make some tools, and we're just
gonna make some fake ones for now,

66
00:04:19.807 --> 00:04:21.718
ones that don't really do anything.

67
00:04:21.718 --> 00:04:24.342
We'll get into the real cool
stuff in a minute, but for

68
00:04:24.342 --> 00:04:26.578
now, we'll just pass in
the tools like this.

69
00:04:26.578 --> 00:04:31.394
And then down here we will
pass in those tools, but

70
00:04:31.394 --> 00:04:37.699
I also just wanna keep yeah,
we're just gonna keep l sorry,

71
00:04:37.699 --> 00:04:43.222
I don't wanna go here,
I wanna go to agent, there we go.

72
00:04:43.222 --> 00:04:48.696
I want this to take in some tools and
we're gonna keep passing them down,

73
00:04:48.696 --> 00:04:52.481
so you take in some tools,
you take in some tools.

74
00:04:54.783 --> 00:04:57.682
&gt;&gt; Scott Moss: There we go, and
now I can just pass those tools here, so

75
00:04:57.682 --> 00:05:01.340
basically I'm just making sure our
agent takes in a user message.

76
00:05:01.340 --> 00:05:04.133
Takes in some tools which
is an array right now and

77
00:05:04.133 --> 00:05:06.183
it just passes them down to the LLM.

78
00:05:13.497 --> 00:05:15.449
&gt;&gt; Scott Moss: So
we'll get to the tools in a minute.

79
00:05:17.538 --> 00:05:20.881
&gt;&gt; Scott Moss: And then while we're
still in the LLM we now have to

80
00:05:20.881 --> 00:05:25.577
add the response that we got back from
the LLM to our chat messages, and

81
00:05:25.577 --> 00:05:27.672
then we can just return them.

82
00:05:27.672 --> 00:05:32.699
So, I'm gonna say await addMessages,
add our response like that.

83
00:05:34.726 --> 00:05:40.648
&gt;&gt; Scott Moss: I now want to
stop this loader from showing,

84
00:05:40.648 --> 00:05:46.262
so I can say loader.stop() like that.

85
00:05:47.752 --> 00:05:49.416
&gt;&gt; Scott Moss: That will stop it.

86
00:05:51.346 --> 00:05:53.586
Also have this thing in
here called logMessage, so

87
00:05:53.586 --> 00:05:55.471
it'll actually log it out so
we can see it.

88
00:05:55.471 --> 00:05:58.433
&gt;&gt; Scott Moss: So we can say
logMessage with the response here,

89
00:05:58.433 --> 00:06:00.059
don't worry about that.

90
00:06:00.059 --> 00:06:02.803
That's just freak it out
because it's optionally, no,

91
00:06:02.803 --> 00:06:06.589
because I haven't finished this
function yet, that's totally fine.

92
00:06:06.589 --> 00:06:13.409
And then we're just gonna return, I guess
it doesn't really matter what returns.

93
00:06:13.409 --> 00:06:14.917
We're not we're never gonna use this, but

94
00:06:14.917 --> 00:06:16.516
I'm just gonna return
all the messages here.

95
00:06:22.445 --> 00:06:27.396
&gt;&gt; Scott Moss: So everything in the run
agent, now we just need to go create

96
00:06:27.396 --> 00:06:33.237
a fake tool, pass it in, and
then we need to make sure that this thing,

97
00:06:33.237 --> 00:06:36.921
the runLLM actually
uses the the Zod thing.

98
00:06:36.921 --> 00:06:42.098
So I'm actually going to go to OpenAI,
tool calling go here,

99
00:06:42.098 --> 00:06:47.171
function calling, tool calling,
very similar stuff.

100
00:06:47.171 --> 00:06:53.378
You'll hear me say that a lot,
Zod function, look at that,

101
00:06:53.378 --> 00:07:01.321
so we're going to import{ZodFunction}
from 'openai/helper/Zod'.

102
00:07:03.303 --> 00:07:07.789
&gt;&gt; Scott Moss: And then what we wanna do
is we just want to say, formattedTools is

103
00:07:07.789 --> 00:07:12.700
just map over all the tools and
convert them to a Zod function like that.

104
00:07:16.161 --> 00:07:20.288
&gt;&gt; Scott Moss: And now he's going to say,
formattedTools, and now we have tools.

105
00:07:27.415 --> 00:07:31.405
&gt;&gt; Scott Moss: The other thing I wanna do
is I want to put this other one here that

106
00:07:31.405 --> 00:07:33.223
says tool, choice, auto.

107
00:07:33.223 --> 00:07:36.597
What this is telling the LLM is hey,
it's up to you,

108
00:07:36.597 --> 00:07:40.532
I might give you 20 tools you
figure it out, I don't know.

109
00:07:40.532 --> 00:07:45.520
You could force it and
give it like a specific tool or

110
00:07:45.520 --> 00:07:49.123
maybe it's just one, not an array.

111
00:07:49.123 --> 00:07:52.821
Yeah, you could just give it like
a tool or maybe it's an object, yeah,

112
00:07:52.821 --> 00:07:54.714
there it's like an object like that.

113
00:07:54.714 --> 00:07:59.770
You can like, you gotta call this one,
but we want it to figure it out.

114
00:07:59.770 --> 00:08:01.734
So that's where it gets powerful.

115
00:08:01.734 --> 00:08:06.314
And then, just to be sure, I'm gonna
turn off parallel tool calls for like,

116
00:08:06.314 --> 00:08:09.349
sometimes when I don't turn this off,
it does it.

117
00:08:09.349 --> 00:08:13.087
I don't want this to run
multiple tools in parallel,

118
00:08:13.087 --> 00:08:15.670
as that is a little harder to handle.

119
00:08:15.670 --> 00:08:19.213
So we're gonna turn that off for now,
and it could still run many tools,

120
00:08:19.213 --> 00:08:21.551
just not parallel to run
them after each other.

121
00:08:21.551 --> 00:08:22.780
A lot easier to deal with.

122
00:08:22.780 --> 00:08:25.862
I don't wanna do parallel tool calls
that we'll be here for a minute.

123
00:08:25.862 --> 00:08:27.779
&gt;&gt; Student 1: Debugging
those are really fun.

124
00:08:27.779 --> 00:08:30.185
&gt;&gt; Scott Moss: Yeah, exactly.

125
00:08:30.185 --> 00:08:31.907
Yeah, I don't wanna do it.

126
00:08:31.907 --> 00:08:37.725
Order matters, I don't want to deal
with this order, so we will do that.

127
00:08:37.725 --> 00:08:42.057
And then the next thing
we want to do is Is right

128
00:08:42.057 --> 00:08:46.175
now we're returning message.content.

129
00:08:46.175 --> 00:08:50.175
If there's a tool call,
there won't be a content here.

130
00:08:50.175 --> 00:08:51.198
This content will be null.

131
00:08:51.198 --> 00:08:54.740
So let's just return the message for now.

132
00:08:54.740 --> 00:08:57.645
Because if there's a tool called,
it'll be .tool calls like this.

133
00:08:57.645 --> 00:08:59.307
It'll be like an array of tools.

134
00:08:59.307 --> 00:09:02.727
So let's just return
the message like this.

135
00:09:02.727 --> 00:09:03.687
So we'll do that.

136
00:09:06.222 --> 00:09:10.492
&gt;&gt; Scott Moss: So now, if we go back
into the agent, you can see this thing's

137
00:09:10.492 --> 00:09:15.024
freaking out because it's like,
there is no content here.

138
00:09:15.024 --> 00:09:16.101
How do you wanna do this?

139
00:09:16.101 --> 00:09:20.503
So what we can do instead of adding this
message like this, we could just put,

140
00:09:20.503 --> 00:09:23.369
like, the whole response in here so
we can see it.

141
00:09:23.369 --> 00:09:28.052
But I kind of just want to log this so
you guys can see when it calls a tool, so

142
00:09:28.052 --> 00:09:29.952
you can see what's going on.

143
00:09:29.952 --> 00:09:33.349
Because even if we say this is gonna break
because we're not responding to it yet,

144
00:09:33.349 --> 00:09:36.123
because we haven't done the loop,
so it doesn't really matter.

145
00:09:36.123 --> 00:09:40.202
So what I want is, I want to go in here.

146
00:09:40.202 --> 00:09:44.516
And I wanna say, if (response.tool_calls),
I wanna do something here.

147
00:09:44.516 --> 00:09:47.656
So for now, we will do some logging here.

148
00:09:47.656 --> 00:09:50.118
That way you can kinda see it.

149
00:09:50.118 --> 00:09:54.845
And if you really wanted to say this,
just because you really just wanted to,

150
00:09:54.845 --> 00:09:59.661
you could just throw the response in
there like that and that's totally fine.

151
00:09:59.661 --> 00:10:01.771
Let me get back to my notes.

152
00:10:03.769 --> 00:10:06.658
&gt;&gt; Scott Moss: And then,
yeah, we need to go ahead and

153
00:10:06.658 --> 00:10:11.562
replace the LLM call that's in
the index file with the agent call,

154
00:10:11.562 --> 00:10:14.906
and then we can make a fake tool and
pass it in.

155
00:10:14.906 --> 00:10:17.324
So let's do that.

156
00:10:17.324 --> 00:10:22.397
I'm gonna go into index.

157
00:10:22.397 --> 00:10:30.920
And I'm gonna get rid of all of this,
essentially.

158
00:10:30.920 --> 00:10:34.187
I'm gonna get rid of this and this, and

159
00:10:34.187 --> 00:10:38.153
now I'm just gonna import
the agent like that.

160
00:10:40.316 --> 00:10:43.773
&gt;&gt; Scott Moss: And I'm gonna say
the response is run an agent with

161
00:10:43.773 --> 00:10:47.823
this message, and
these tools that have nothing in them.

162
00:10:47.823 --> 00:10:49.509
Okay, so we have that.

163
00:10:49.509 --> 00:10:53.752
Now, let's give it a tool,
let's make it do something.

164
00:10:53.752 --> 00:10:58.700
So, we'll just hard code one right now.

165
00:10:58.700 --> 00:11:02.417
So the way that that's gonna work is,

166
00:11:02.417 --> 00:11:07.064
let's just make one called,
getWeather, and

167
00:11:07.064 --> 00:11:12.193
it's just a function, and
it returns, it's hot.

168
00:11:12.193 --> 00:11:15.478
It's 89 degrees outside,
something like that.

169
00:11:15.478 --> 00:11:18.038
That's all it's gonna do.

170
00:11:18.038 --> 00:11:22.196
That's the function, actually,
I'll make an object here.

171
00:11:27.027 --> 00:11:29.715
&gt;&gt; Scott Moss: weatherTool, or actually,
what we can do is we don't even really

172
00:11:29.715 --> 00:11:32.361
need to make the function,
we just need to make the description.

173
00:11:32.361 --> 00:11:35.946
So let's just say weatherTool.

174
00:11:35.946 --> 00:11:39.748
It's just gonna be an object that
has a name, and I'm just gonna say,

175
00:11:39.748 --> 00:11:41.489
let's just say get_weather,

176
00:11:41.489 --> 00:11:46.302
and then it's gonna have parameters,
&gt;&gt; Scott Moss: And

177
00:11:46.302 --> 00:11:50.198
I'm just gonna make it an empty
Zod object, like that.

178
00:11:50.198 --> 00:11:53.399
So we have a weatherTool, and
I can just pass in the weatherTool.

179
00:11:56.255 --> 00:11:59.449
&gt;&gt; Scott Moss: Import z from zod.

180
00:12:06.468 --> 00:12:09.210
&gt;&gt; Scott Moss: So
now we have a weatherTool,

181
00:12:09.210 --> 00:12:12.399
we're passing it into our agent.

182
00:12:12.399 --> 00:12:14.638
Our agent has these tools.

183
00:12:14.638 --> 00:12:15.380
It's gonna get the messages.

184
00:12:15.380 --> 00:12:18.641
It's gonna run an LLM with those tools.

185
00:12:18.641 --> 00:12:21.808
If the tool is called, it's gonna do this.

186
00:12:21.808 --> 00:12:26.650
It's gonna log it, and then it's gonna
save it, and then it's just gonna stop.

187
00:12:26.650 --> 00:12:29.631
We're not gonna feed it back to the LLM,
so it's not gonna loop here.

188
00:12:29.631 --> 00:12:36.411
It's just gonna be like, hey, I ran this
thing, did you go run the function or not?

189
00:12:36.411 --> 00:12:40.244
So we will try that out.

190
00:12:40.244 --> 00:12:42.887
And then in the run LLM,
let's make sure everything's good here.

191
00:12:42.887 --> 00:12:43.799
This looks good.

192
00:12:43.799 --> 00:12:45.761
Formatted tools, great.

193
00:12:45.761 --> 00:12:46.520
Let's give it a shot.

194
00:12:46.520 --> 00:12:50.809
So now, if I go here and I say,

195
00:12:50.809 --> 00:12:57.689
what is the weather,
let's see what we broke.

196
00:12:57.689 --> 00:13:01.330
Got our thinking face and cool.

197
00:13:01.330 --> 00:13:02.555
I literally logged everything.

198
00:13:02.555 --> 00:13:09.099
So as you can see right here, here's
the tool call it called get_weather.

199
00:13:09.099 --> 00:13:10.146
This was it, trying to do it.

200
00:13:10.146 --> 00:13:14.211
Let me get rid of all the noise on here so
you can not see everything.

201
00:13:14.211 --> 00:13:15.708
Where am I logging now?

202
00:13:15.708 --> 00:13:18.334
All that noise.

203
00:13:18.334 --> 00:13:19.286
Is it this?

204
00:13:19.286 --> 00:13:19.937
Response, maybe?

205
00:13:19.937 --> 00:13:23.781
I don't know where I'm logging all that.

206
00:13:23.781 --> 00:13:26.729
I think it's response, yes?

207
00:13:26.729 --> 00:13:30.670
&gt;&gt; Student 1: If you have one of these
agents that takes a really long time in

208
00:13:30.670 --> 00:13:34.245
Next.js, for instance,
how would you handle that?

209
00:13:34.245 --> 00:13:35.641
&gt;&gt; Scott Moss: I've ran into that so
many times.

210
00:13:35.641 --> 00:13:38.729
[LAUGH] Theres couple answers one,

211
00:13:38.729 --> 00:13:43.733
depending on the platform in
which you deploy your app,

212
00:13:43.733 --> 00:13:50.991
they have different execution times in
which, before your function times out.

213
00:13:50.991 --> 00:13:55.242
So with Next.js, or with vert cell,
if you're like, on their Pro Plan,

214
00:13:55.242 --> 00:13:57.882
like, you can actually
turn the function up.

215
00:13:57.882 --> 00:14:01.688
I think I forgot the exact maximum, don't
quote me, but I think it's like near or

216
00:14:01.688 --> 00:14:02.804
above 300 seconds.

217
00:14:02.804 --> 00:14:04.668
So that might help for a lot.

218
00:14:04.668 --> 00:14:09.983
Two, if you don't want to deal with that
race condition, then what you could do is

219
00:14:09.983 --> 00:14:15.165
you could run the LLMs kind of which is
what we do now, which is a background job.

220
00:14:15.165 --> 00:14:20.603
So when a user sends a message,
we don't run the LLM

221
00:14:20.603 --> 00:14:26.053
on that same request and
then send a response back.

222
00:14:26.053 --> 00:14:29.420
When they send a message, we kick
off a job in a whole new process and

223
00:14:29.420 --> 00:14:30.996
we respond back immediately.

224
00:14:30.996 --> 00:14:34.780
And then that job,
it has a queuing system.

225
00:14:34.780 --> 00:14:38.337
It is basically like using, using
a queuing system that you would have to

226
00:14:38.337 --> 00:14:42.536
make to make sure that these things aren't
timing out and they're not slowing down,

227
00:14:42.536 --> 00:14:43.606
they can be retried.

228
00:14:43.606 --> 00:14:47.738
So you're not having to run the LLM again,
which is costing you money.

229
00:14:47.738 --> 00:14:49.697
So have like a queuing system.

230
00:14:49.697 --> 00:14:51.275
You can turn on streaming.

231
00:14:51.275 --> 00:14:54.057
So we haven't talked about message
streaming, but right now we're not.

232
00:14:54.057 --> 00:14:57.866
We're waiting for the LLM to get back
everything and then send it back.

233
00:14:57.866 --> 00:15:00.450
If we just dream it as it comes,
it'll be quicker or

234
00:15:00.450 --> 00:15:03.465
at least perceived quicker
from the user's perspective.

235
00:15:03.465 --> 00:15:06.158
So that's another way to do it.

236
00:15:06.158 --> 00:15:07.844
Yeah, there really isn't one way.

237
00:15:07.844 --> 00:15:10.993
So you probably got to experiment
with a combination of those things.

238
00:15:10.993 --> 00:15:17.865
But background jobs cuz streaming
increasing your limit, it will work.

239
00:15:17.865 --> 00:15:22.126
Like right now we are using Vercel
serverless Next.js functions for

240
00:15:22.126 --> 00:15:25.883
our stuff and we don't have any issues,
so it's just fine.

241
00:15:25.883 --> 00:15:29.858
There's tons of products, I don't wanna
get into product recommendations, but

242
00:15:29.858 --> 00:15:30.946
I'm just gonna just.

243
00:15:30.946 --> 00:15:35.352
&gt;&gt; Student 2: I mean, the key fundamental
thing is like taking the AI response out

244
00:15:35.352 --> 00:15:40.221
of that immediate web request and being
able to send it back some other out of

245
00:15:40.221 --> 00:15:43.147
band of that initial request
&gt;&gt; Scott Moss: Exactly,

246
00:15:43.147 --> 00:15:47.233
if QStash workflows is a really good one,

247
00:15:47.233 --> 00:15:52.020
if you go into one that
they have specifically for

248
00:15:52.020 --> 00:15:56.364
this, which is,
let me see which one is it.

249
00:15:56.364 --> 00:16:00.257
It's basically, let me find it.

250
00:16:00.257 --> 00:16:04.859
It's one where they show you exactly how.

251
00:16:04.859 --> 00:16:07.130
No, okay, here we go, Workflow,
I'm looking at the wrong one.

252
00:16:07.130 --> 00:16:07.840
There we go.

253
00:16:07.840 --> 00:16:11.950
They show exactly how to
do what you're describing.

254
00:16:11.950 --> 00:16:16.350
So if you want to do something,
that's going to take a very, very,

255
00:16:16.350 --> 00:16:22.221
very long time, let's say a AI generation,
they have this thing called context.call,

256
00:16:22.221 --> 00:16:28.048
and you can see right here, HTTP request
with much longer timeout, two hours.

257
00:16:28.048 --> 00:16:31.378
So this one will wait two hours plus or
whatever.

258
00:16:31.378 --> 00:16:34.205
So if you have an AI call
that's taking two hours plus,

259
00:16:34.205 --> 00:16:37.298
probably should rethink your architecture,
I think.

260
00:16:37.298 --> 00:16:39.578
But yeah, background jobs,
things like that.

261
00:16:39.578 --> 00:16:43.197
There's a lot of use cases, but
you should be fine on Vercel,

262
00:16:43.197 --> 00:16:45.848
just increase your function timeout limit.

263
00:16:47.822 --> 00:16:51.621
&gt;&gt; Student 1: I mean, you need to
really get into web sockets and

264
00:16:51.621 --> 00:16:53.893
set real time capabilities.

265
00:16:53.893 --> 00:16:56.833
Then if you have some agent
that's initiating something,

266
00:16:56.833 --> 00:16:59.429
you can just send that out to the-
&gt;&gt; Scott Moss: Exactly, yeah,

267
00:16:59.429 --> 00:17:01.691
that's actually the system that we have.

268
00:17:01.691 --> 00:17:03.655
It's many different steps, and

269
00:17:03.655 --> 00:17:07.590
we use WebSockets to communicate
all the steps along the process.

270
00:17:07.590 --> 00:17:10.229
So, there never is just one
long process happening,

271
00:17:10.229 --> 00:17:13.983
because if that one process fails,
not only is it a bad user experience, but

272
00:17:13.983 --> 00:17:17.050
it'll cost you money cuz you
gotta do all those things again.

273
00:17:17.050 --> 00:17:21.142
You gotta run it through the LLM again,
when maybe every step passed except for

274
00:17:21.142 --> 00:17:22.720
the last part.

275
00:17:22.720 --> 00:17:24.870
And it's like, man,
I gotta run all this again.

276
00:17:24.870 --> 00:17:27.745
Where it's like, if you could just save
the intermediate steps and everything,

277
00:17:27.745 --> 00:17:28.950
it's probably better.

278
00:17:28.950 --> 00:17:31.510
Cool, so as you can see here,
I said, what is the weather?

279
00:17:31.510 --> 00:17:35.770
The AI thought about it, and here's the
important part, is this part right here.

280
00:17:35.770 --> 00:17:41.227
So the response that it got
back from the AI was like,

281
00:17:41.227 --> 00:17:47.697
yeah, can you run this
function called get_weather?

282
00:17:47.697 --> 00:17:53.027
It's telling my system that I need you
to run a function called get_weather.

283
00:17:54.417 --> 00:17:55.507
That's what it's doing, right?

284
00:17:55.507 --> 00:17:57.827
That's this log that I have.

285
00:17:59.487 --> 00:18:00.660
Where is it?

286
00:18:00.660 --> 00:18:06.360
This one, it's this log, this tool_calls
is an array, so it's logging this array.

287
00:18:06.360 --> 00:18:09.580
So it's like, yeah, somebody's trying
to get the weather, go do that for me.

288
00:18:09.580 --> 00:18:12.702
We're not, I'm just logging it and
ignoring it,

289
00:18:12.702 --> 00:18:15.690
and then I'm adding it,
and then I'm leaving.

290
00:18:17.820 --> 00:18:19.470
But that's what we would have to do,
right?

291
00:18:19.470 --> 00:18:24.120
We would have to go actually run this,
feed the answer back to the LLM, and

292
00:18:24.120 --> 00:18:29.376
then LLM could try to answer the question
for the user, we're just not doing that.

293
00:18:32.666 --> 00:18:35.758
And as you can see, when I said,
the content is usually null if

294
00:18:35.758 --> 00:18:40.199
there's a tool call, there's also this
other property here called refusal.

295
00:18:40.199 --> 00:18:45.484
Refusal just means,
&gt;&gt; Scott Moss: You know what?

296
00:18:45.484 --> 00:18:47.514
I'm not even gonna try to
describe what that means.

297
00:18:47.514 --> 00:18:52.220
[LAUGH] We're just gonna
look that up right here.

298
00:18:54.667 --> 00:18:56.261
&gt;&gt; Scott Moss: This has structured,
that's right, structured output,

299
00:18:56.261 --> 00:18:57.054
we're not using structure.

300
00:18:57.054 --> 00:19:00.702
So, this is basically if you want the AI
to do structured output and you say,

301
00:19:00.702 --> 00:19:03.951
here's that schema that I want you
to output, and for some reason,

302
00:19:03.951 --> 00:19:06.403
it refused to do it,
it'll say, I refuse to do it.

303
00:19:06.403 --> 00:19:08.610
[LAUGH] And here's why.

304
00:19:08.610 --> 00:19:13.399
We're not doing structured outputs ,so
yeah, structured outputs are cool, though.

305
00:19:13.399 --> 00:19:17.493
I do use them in production for
generative UI, which is really cool.

306
00:19:19.043 --> 00:19:20.813
And you can kind of follow
this conversation, right?

307
00:19:20.813 --> 00:19:23.287
So here I said, what is the weather?

308
00:19:23.287 --> 00:19:26.157
The assistant immediately was like,
call the weather function.

309
00:19:26.157 --> 00:19:30.997
It did a really good job of deciding that
it needed to call the weather function.

310
00:19:30.997 --> 00:19:36.468
And what's what's even crazier to me is
that, now that I'm thinking about it,

311
00:19:36.468 --> 00:19:41.547
I don't think I even gave it a description
of that function, let me go see.

312
00:19:42.978 --> 00:19:44.278
I didn't even give it a description.

313
00:19:44.278 --> 00:19:48.439
The only reason it knows that
this function has anything to do

314
00:19:48.439 --> 00:19:52.998
with weather is because it's in the name,
that's it.

315
00:19:52.998 --> 00:19:57.944
If I put a description here and it's like,
use this function to get shoes,

316
00:19:57.944 --> 00:20:02.122
I don't think it would call this function,
[LAUGH] let's see.

317
00:20:06.435 --> 00:20:08.071
&gt;&gt; Scott Moss: Not the weather.

318
00:20:08.071 --> 00:20:13.343
[LAUGH] The name is misleading.

319
00:20:13.343 --> 00:20:18.494
If I just described it like that, and
let me just get rid of all my history.

320
00:20:18.494 --> 00:20:20.576
I actually don't know what type of logic.

321
00:20:20.576 --> 00:20:24.072
I guess it depends on
the AI that you have.

322
00:20:24.072 --> 00:20:27.412
So, no, it still did it.

323
00:20:27.412 --> 00:20:30.302
It's like, yeah, screw you,
that's called get weather [LAUGH].

324
00:20:30.302 --> 00:20:31.962
I don't care what you say.

325
00:20:31.962 --> 00:20:33.512
It's in the name, right?

326
00:20:33.512 --> 00:20:35.602
It could just be lack of
other better options.

327
00:20:35.602 --> 00:20:37.407
Right, it could be lack
of other options as well.

328
00:20:37.407 --> 00:20:40.852
Right, it's like this is where logic and
reasoning comes in.

329
00:20:40.852 --> 00:20:43.984
And this is why it's hard to make with
with AI because it's non-determined,

330
00:20:43.984 --> 00:20:44.700
you don't know.

331
00:20:44.700 --> 00:20:46.830
I might run this again and it might do it.

332
00:20:46.830 --> 00:20:49.350
It might be like, actually,
I won't do that again.

333
00:20:49.350 --> 00:20:51.390
So it really just depends.

334
00:20:51.390 --> 00:20:53.413
And this is why you have to have evals and
stuff,

335
00:20:53.413 --> 00:20:56.940
because you don't really know what
the hell is gonna happen here.

336
00:20:56.940 --> 00:20:58.900
But yeah, that was really interesting.

337
00:20:58.900 --> 00:21:01.845
I didn't know that it was smart enough
to just look at the name of the tool.

338
00:21:01.845 --> 00:21:05.198
I've never even tried that before cuz
it's so dangerous just to do that.

339
00:21:05.198 --> 00:21:08.543
But yeah, that's pretty good,
I learned something today.

