WEBVTT

1
00:00:00.000 --> 00:00:03.519
&gt;&gt; Scott Moss: Types of LLMs, we got base
models or you'll hear foundational models,

2
00:00:03.519 --> 00:00:05.491
GPT, that's everything from OpenAI.

3
00:00:05.491 --> 00:00:09.071
That's there since ChatGPT
3 when they released that.

4
00:00:09.071 --> 00:00:12.777
Well, I guess really 3.5
is when ChatGPT came out.

5
00:00:12.777 --> 00:00:15.004
That's what took the whole world by storm.

6
00:00:15.004 --> 00:00:17.953
But they've been doing GPT for
a while, GPT 1, GPT 2.

7
00:00:17.953 --> 00:00:22.139
You can actually go make that in
Python on a couple lines of code.

8
00:00:22.139 --> 00:00:27.006
It wasn't until GPT 3.5 where they took
a bet and was like, let's train this thing

9
00:00:27.006 --> 00:00:30.741
on hundreds of millions of parameters and
see how good it can get.

10
00:00:30.741 --> 00:00:33.105
And guess what, it was pretty good.

11
00:00:33.105 --> 00:00:34.927
Claude is another one.

12
00:00:34.927 --> 00:00:38.565
That's some people left OpenAI,
made another company called Anthropic.

13
00:00:38.565 --> 00:00:41.697
They have a thing called Claude,
which is very similar to ChatGPT.

14
00:00:41.697 --> 00:00:42.663
It's actually pretty good.

15
00:00:42.663 --> 00:00:46.565
It's actually the main one that I
use when I'm interacting with AI.

16
00:00:46.565 --> 00:00:49.400
Llama is Meta's version of this.

17
00:00:49.400 --> 00:00:54.317
I hesitate to say open source, cuz it's
not open source, because you don't have

18
00:00:54.317 --> 00:00:59.031
access to the weights that make the model,
but you do have access to the model.

19
00:00:59.031 --> 00:01:02.926
You can do whatever the hell you want with
it, and that way, it is open source, and

20
00:01:02.926 --> 00:01:05.934
they have many different sizes,
many different parameters.

21
00:01:05.934 --> 00:01:08.777
Most models are shipped
in different sizes.

22
00:01:08.777 --> 00:01:10.575
So you'll have this is Llama 2B, so

23
00:01:10.575 --> 00:01:14.464
that means this is a model that was
trained with 2 billion parameters, right?

24
00:01:14.464 --> 00:01:18.296
You might have another one that's 12B,
it was trained with 12 billion parameters.

25
00:01:18.296 --> 00:01:20.417
The more parameters, the bigger the model.

26
00:01:20.417 --> 00:01:25.940
It might be gigabytes on your computer,
but that means it's probably also better.

27
00:01:25.940 --> 00:01:27.701
So it really just depends.

28
00:01:27.701 --> 00:01:30.670
Fine tuning can really mess this up and
stuff.

29
00:01:30.670 --> 00:01:33.927
But generally speaking,
the more parameters, the better it is,

30
00:01:33.927 --> 00:01:35.044
the bigger the model.

31
00:01:35.044 --> 00:01:39.128
For instance, GPT4, no one knows how
many parameters that was trained on.

32
00:01:39.128 --> 00:01:42.221
Estimates are maybe trillions, who knows?

33
00:01:42.221 --> 00:01:46.645
[LAUGH] The whole internet, it was
trained on the whole internet plus more,

34
00:01:46.645 --> 00:01:47.638
we don't know.

35
00:01:47.638 --> 00:01:50.948
And it could take months with
tons of GPU compute, hundreds and

36
00:01:50.948 --> 00:01:52.579
millions of dollars to train.

37
00:01:52.579 --> 00:01:56.642
And then most foundational models
are really just good for generic things.

38
00:01:56.642 --> 00:02:01.214
Cuz a lot of these models
are trained on generic data sets, so

39
00:02:01.214 --> 00:02:06.340
they're really good for generic things,
which can be good or bad.

40
00:02:06.340 --> 00:02:09.973
It's like you can use this thing to do
something generic, but because it's

41
00:02:09.973 --> 00:02:13.974
not specialized at one thing, it might not
get that very specialized thing right.

42
00:02:13.974 --> 00:02:17.252
So sometimes it's best just
to not use a generic tool and

43
00:02:17.252 --> 00:02:19.078
switch to a specialized tool.

44
00:02:19.078 --> 00:02:20.683
Instruction-tuned models,

45
00:02:20.683 --> 00:02:24.518
these are models that build on top
of these foundational models, right?

46
00:02:24.518 --> 00:02:27.748
So ChatGPT Claude,
those models themselves.

47
00:02:27.748 --> 00:02:30.132
ChatGPT is an app built on GPT, and

48
00:02:30.132 --> 00:02:34.420
it has instructions associated with it,
and error clauses.

49
00:02:34.420 --> 00:02:37.711
It's all these things to
help the model be good.

50
00:02:37.711 --> 00:02:40.440
Same thing with Claude,
stuff like that, yep.

51
00:02:40.440 --> 00:02:44.299
&gt;&gt; Participant: What
are the weights in AI models?

52
00:02:44.299 --> 00:02:48.441
&gt;&gt; Scott Moss: The weights are the set
of probabilities that go into the model,

53
00:02:48.441 --> 00:02:48.986
right?

54
00:02:48.986 --> 00:02:52.207
So without the weights,
the model is basically useless.

55
00:02:52.207 --> 00:02:54.483
So the weights are the output
of the training.

56
00:02:54.483 --> 00:02:59.273
So I guess the best way I could think
about it is the model itself is

57
00:02:59.273 --> 00:03:04.154
kind of the brain, and then
the weights are the thought process of

58
00:03:04.154 --> 00:03:07.207
how to actually use this brain, right?

59
00:03:07.207 --> 00:03:10.421
If without the weights,
it's pretty much useless.

60
00:03:10.421 --> 00:03:13.492
The weights are the decided probabilities

61
00:03:13.492 --> 00:03:17.839
from the outcome of the training
that you apply to the model so

62
00:03:17.839 --> 00:03:22.714
that it's actually useful or
at least that's how I understand it.

63
00:03:22.714 --> 00:03:27.553
I'm not a research scientist, and can't
break that down any lower than what I

64
00:03:27.553 --> 00:03:31.090
just explained, but
that's the way that I describe it.

65
00:03:31.090 --> 00:03:32.095
That's the way that I see it.

66
00:03:32.095 --> 00:03:36.512
So yeah, most models have
the weights already in the model.

67
00:03:36.512 --> 00:03:38.008
They're already there, they're in there.

68
00:03:38.008 --> 00:03:39.910
They're part of the model.

69
00:03:39.910 --> 00:03:42.860
But, yeah, when you're training and,
and building those things,

70
00:03:42.860 --> 00:03:44.119
they're kind of separated.

71
00:03:44.119 --> 00:03:47.498
You can have the weights out here, and
the model over here, and things like that.

72
00:03:47.498 --> 00:03:51.682
But yeah, weights are the probabilities.

73
00:03:51.682 --> 00:03:53.375
And they're called weights,

74
00:03:53.375 --> 00:03:57.999
because they're numerical representation
on how much weight to put on a decision as

75
00:03:57.999 --> 00:04:01.272
you're going through a layer
of a neural network, right?

76
00:04:01.272 --> 00:04:05.969
Neutral network is these different
decision layers where there might be

77
00:04:05.969 --> 00:04:07.745
many nodes on the network.

78
00:04:07.745 --> 00:04:11.521
And as it goes in more,
the nodes start to collapse.

79
00:04:11.521 --> 00:04:15.990
And the weights determine which direction
one node might go to another node on

80
00:04:15.990 --> 00:04:16.962
another layer.

81
00:04:16.962 --> 00:04:21.560
So those weights are the instructions on
that, which again, are trained through

82
00:04:21.560 --> 00:04:25.505
correction, and human in the loop,
and all these different things.

83
00:04:25.505 --> 00:04:28.399
So you really need the weights, otherwise,

84
00:04:28.399 --> 00:04:31.456
you just have a neural
network that has no idea.

85
00:04:31.456 --> 00:04:33.838
It's like, all right, cool,
you wanna show me a picture of a dog and

86
00:04:33.838 --> 00:04:35.043
you expect me to know this is a dog.

87
00:04:35.043 --> 00:04:37.223
I don't know what a dog is,
you haven't told me that.

88
00:04:37.223 --> 00:04:39.624
So the weights are that.

89
00:04:39.624 --> 00:04:43.771
Think about it like a kid going into
kindergarten, going through elementary

90
00:04:43.771 --> 00:04:48.257
school, the weights are the representation
of the things they learned at school.

91
00:04:48.257 --> 00:04:50.102
That's a great way to think about it, yes.

92
00:04:50.102 --> 00:04:51.283
I'm gonna steal that.

93
00:04:51.283 --> 00:04:56.017
[LAUGH] You have domain-specific models,

94
00:04:56.017 --> 00:04:59.441
code Llamas, one for coding.

95
00:04:59.441 --> 00:05:00.899
You have these medical ones.

96
00:05:00.899 --> 00:05:06.362
These are like LLMs that were
trained either from scratch or

97
00:05:06.362 --> 00:05:13.914
fine tuned later on a very specific set of
data to do a very specific thing, right?

98
00:05:13.914 --> 00:05:18.084
So if you want something that's really
good at writing code, then sure you can

99
00:05:18.084 --> 00:05:22.379
train it on like human language and stuff,
but you probably also wanna train it on

100
00:05:22.379 --> 00:05:25.812
a bunch of code and understanding that and
intricacies of that.

101
00:05:25.812 --> 00:05:28.215
So those are domain-specific models and

102
00:05:28.215 --> 00:05:31.105
you're starting to see
a lot more of these pop up.

103
00:05:31.105 --> 00:05:33.501
So talk a little bit
about context windows,

104
00:05:33.501 --> 00:05:38.183
some other limitations you might find is
hallucinations and just errors in general.

105
00:05:38.183 --> 00:05:41.363
If you've never heard of hallucinations,
it's just when the AI is lying.

106
00:05:41.363 --> 00:05:43.312
Or I guess not that it's lying,

107
00:05:43.312 --> 00:05:47.877
cuz I think lying is you are lying,
you know you're not telling the truth.

108
00:05:47.877 --> 00:05:50.252
The AI literally thinks
it's telling the truth.

109
00:05:50.252 --> 00:05:51.207
It just doesn't know it.

110
00:05:51.207 --> 00:05:54.975
And again, because it can't think,
it can only predict the next thing.

111
00:05:54.975 --> 00:05:58.419
So when you look at the hallucination,
you look at what it output,

112
00:05:58.419 --> 00:06:01.077
you think to yourself,
I can see why you said that.

113
00:06:01.077 --> 00:06:04.101
That grammatically makes sense.

114
00:06:04.101 --> 00:06:08.733
And if I didn't know the fact myself,
I might think it was right.

115
00:06:08.733 --> 00:06:10.101
But I do know the fact and

116
00:06:10.101 --> 00:06:15.256
I do know that this team didn't win the
NBA championship in 1992, so that's wrong.

117
00:06:15.256 --> 00:06:17.727
But there's nothing wrong
with that sentence, so

118
00:06:17.727 --> 00:06:19.396
you can see why it predicted that.

119
00:06:19.396 --> 00:06:23.674
So you'll get a lot of that, and that's
probably the number one problem with LLMs

120
00:06:23.674 --> 00:06:28.138
today, is that they hallucinate and every
company that's building a product to help

121
00:06:28.138 --> 00:06:31.056
you with AI is probably trying
to solve this in some way.

122
00:06:31.056 --> 00:06:32.665
So it's a massive problem.

123
00:06:32.665 --> 00:06:35.510
Also, context window constraints,
we'll talk about that.

124
00:06:35.510 --> 00:06:40.616
Computation is huge, I mean GPUs,
they're gold, right?

125
00:06:40.616 --> 00:06:44.413
If you thought crypto made GPUs valuable,

126
00:06:44.413 --> 00:06:48.846
it pales in comparison to
what AI is doing to GPUs.

127
00:06:48.846 --> 00:06:50.187
It's not even close.

128
00:06:50.187 --> 00:06:54.700
So I would imagine OpenAI and
Anthropic probably harness and

129
00:06:54.700 --> 00:06:59.401
use more electricity than some
countries just for their GPUs.

130
00:06:59.401 --> 00:07:03.099
So I think I forgot what
this quote that I saw.

131
00:07:03.099 --> 00:07:07.488
But it was like, every time you use,
or every 100 calls to OpenAI

132
00:07:07.488 --> 00:07:12.199
is a sip of water or something like that,
all the resources that it used.

133
00:07:12.199 --> 00:07:14.374
So it's like, is it really good for
the environment?

134
00:07:14.374 --> 00:07:15.528
Who knows?

135
00:07:15.528 --> 00:07:18.851
But there's a lot of
compute that is needed.

136
00:07:18.851 --> 00:07:22.736
And really, the only other last thing
you need to know here is that we

137
00:07:22.736 --> 00:07:26.566
don't need to know anything about
setting any of these models up.

138
00:07:26.566 --> 00:07:31.384
Cuz we're just gonna use a suite API that
allows us to get some AI in the cloud

139
00:07:31.384 --> 00:07:36.143
across the world somewhere, and
it's gonna come back with our results.

140
00:07:36.143 --> 00:07:37.475
And that's all we need to know, right?

141
00:07:37.475 --> 00:07:39.159
So we're gonna be good there.

