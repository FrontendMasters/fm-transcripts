WEBVTT

1
00:00:00.318 --> 00:00:03.682
So in a real world instance,
you have thousands and thousands and

2
00:00:03.682 --> 00:00:06.802
thousands of containers,
these thousands of instances and

3
00:00:06.802 --> 00:00:08.894
you see how easy they are to come around.

4
00:00:08.894 --> 00:00:11.719
We need some way of bringing them online,

5
00:00:11.719 --> 00:00:16.575
upgrading them taking offline,
it's a whole job of people to do this.

6
00:00:16.575 --> 00:00:21.540
So we call this orchestration, bringing
the servers online freaking offline.

7
00:00:21.540 --> 00:00:24.425
The one you've probably most
heard of is Kubernetes, so

8
00:00:24.425 --> 00:00:27.622
you may have heard this term and
you're like, what does it do?

9
00:00:27.622 --> 00:00:30.782
That's what Kubernetes does,
it manages your containers for you.

10
00:00:30.782 --> 00:00:34.836
As you can tell by us just creating
one and then we created another one,

11
00:00:34.836 --> 00:00:36.291
that's pretty manual.

12
00:00:36.291 --> 00:00:40.815
But we've used something like Kubernetes
or even Docker Swarm or Apache Mesos,

13
00:00:40.815 --> 00:00:43.622
we can create hundreds of
containers at one time.

14
00:00:43.622 --> 00:00:46.839
They're all running the exact
same instance of our application,

15
00:00:46.839 --> 00:00:48.858
and that's what we call orchestration.

16
00:00:48.858 --> 00:00:51.664
Obviously, we're not going to
create hundreds of containers,

17
00:00:51.664 --> 00:00:53.662
it's not super practical for
this instance.

18
00:00:53.662 --> 00:00:56.133
And Kubernetes is probably its own lesson,

19
00:00:56.133 --> 00:00:59.289
is there a front-end master's
course on Kubernetes?

20
00:00:59.289 --> 00:01:03.716
&gt;&gt; Brian holds course, complete intro to
containers gets deeper into Docker and

21
00:01:03.716 --> 00:01:05.941
has a section that covers Kubernetes.

22
00:01:05.941 --> 00:01:10.207
&gt;&gt; Yeah, so if you want to dive more into
containers, there's a course for that for

23
00:01:10.207 --> 00:01:12.744
any masters,
it's a whole whole thing going.

24
00:01:12.744 --> 00:01:15.176
So I talked about earlier
we have two containers now,

25
00:01:15.176 --> 00:01:16.643
how do we get Nginx to talk to it?

26
00:01:16.643 --> 00:01:18.907
We're going to use a load balancer, and

27
00:01:18.907 --> 00:01:22.981
we need a load balancer because what
if one of our servers is overloaded?

28
00:01:22.981 --> 00:01:25.108
But we have two servers, but
all the traffic is going to one,

29
00:01:25.108 --> 00:01:26.085
that doesn't make sense.

30
00:01:26.085 --> 00:01:27.334
Why do we have multiple servers?

31
00:01:27.334 --> 00:01:32.275
So we can tell Nginx, if there's
servers running at 90% of CPU time,

32
00:01:32.275 --> 00:01:36.731
why don't we route the request
to the one that's running lower?

33
00:01:36.731 --> 00:01:41.261
In that way we balance the load across all
of our servers this is a really important

34
00:01:41.261 --> 00:01:47.543
concepts if you're talking about
multi container deployments.

35
00:01:47.543 --> 00:01:50.253
There's different types of algorithms, and

36
00:01:50.253 --> 00:01:54.908
they're called scheduling algorithms for
deciding which one is should go to,

37
00:01:54.908 --> 00:01:59.791
round robin is default, so that just
means 1, 2, 3, start again 1, 2, 3.

38
00:01:59.791 --> 00:02:04.698
There's IP hashing, so you're saying
all IPs from this particular block or

39
00:02:04.698 --> 00:02:07.652
domain go here,
all IPs from this one go here.

40
00:02:07.652 --> 00:02:10.210
There's random, implied, random,

41
00:02:10.210 --> 00:02:14.015
don't really know what that one's good for
but you know.

42
00:02:14.015 --> 00:02:16.700
&gt;&gt; [LAUGH]
&gt;&gt; Yeah?

43
00:02:16.700 --> 00:02:21.431
&gt;&gt; There's a really cool article on
two random choice choose the least

44
00:02:21.431 --> 00:02:26.255
connections and least load you have
to sample over all your servers.

45
00:02:26.255 --> 00:02:31.362
If you have a lot of them that might take
a while, if you do true random choices and

46
00:02:31.362 --> 00:02:36.801
choose the loss of the two, and you get
that surprisingly even distribution.

47
00:02:36.801 --> 00:02:37.386
&gt;&gt; Yeah.

48
00:02:37.386 --> 00:02:40.394
&gt;&gt; And you only have to look
at two instead of 1000.

49
00:02:40.394 --> 00:02:44.012
&gt;&gt; Yeah, that's good thinking, you take
random choices and then you parse those

50
00:02:44.012 --> 00:02:47.416
down the least connections versus
getting those server load for each one.

51
00:02:47.416 --> 00:02:49.857
Yeah, that's realy sharp, yeah.

52
00:02:49.857 --> 00:02:53.828
And one of the scheduling algorithms we
can use is least connections, but again

53
00:02:53.828 --> 00:02:57.933
there's a cost too, you have to know how
many connections are on current server.

54
00:02:57.933 --> 00:03:03.034
Then there's least load, again, you
have to measure the load of each server.

55
00:03:04.792 --> 00:03:08.381
And I've talked about load but we haven't
actually looked at how to see that.

56
00:03:08.381 --> 00:03:12.586
So run the htop command in your terminal,

57
00:03:12.586 --> 00:03:16.438
I'll go ahead and do that too, htop.

58
00:03:19.370 --> 00:03:24.150
And that shows a kind of nice breakdown
of what exactly are servers doing.

59
00:03:24.150 --> 00:03:30.979
I wanna say a Mac equivalent would
be like, application monitor?

60
00:03:30.979 --> 00:03:33.791
No, someone remind me.

61
00:03:33.791 --> 00:03:34.487
&gt;&gt; Activity monitor.

62
00:03:34.487 --> 00:03:39.793
&gt;&gt; Activity monitor, I never remember
activity monitor, I don't know why.

63
00:03:42.382 --> 00:03:47.800
And pretty similar concepts, this one's
much shinier because it's actually GUI.

64
00:03:47.800 --> 00:03:52.177
But this is useful if you're like,
why is my server so slow?

65
00:03:52.177 --> 00:03:55.687
We can look at and see there's
a node process that's running, so

66
00:03:55.687 --> 00:03:58.779
we can just peek kill that node and
watch our server load drop,

67
00:03:58.779 --> 00:04:00.935
so right now our server loads pretty good.

68
00:04:00.935 --> 00:04:05.511
We're only using point 0.1, 0.7% and

69
00:04:05.511 --> 00:04:11.972
we have all our a lot of free memory
full So I just Ctrl C out of that.

70
00:04:11.972 --> 00:04:18.109
So htop is nice to know,
see what your servers actually doing.

71
00:04:18.109 --> 00:04:23.050
So now let's implement a load balancer in
Nginx because we now have two servers so

72
00:04:23.050 --> 00:04:26.185
we can actually load balance
from one to the other.

73
00:04:26.185 --> 00:04:30.819
So what we're going to do is we're going
to create an upstream something called

74
00:04:30.819 --> 00:04:32.825
an upstream, and that just says,

75
00:04:32.825 --> 00:04:36.924
hey, these are actually my server
clusters that I want to connect to.

76
00:04:36.924 --> 00:04:40.449
We give it a name, we'll probably call
this node back end or something like that.

77
00:04:40.449 --> 00:04:43.671
We give it a list of the servers
that belong to that server cluster.

78
00:04:43.671 --> 00:04:47.586
And then in our server block we just
proxy path to that server cluster and

79
00:04:47.586 --> 00:04:49.750
Nginx will take care of the rest for us.

80
00:04:53.060 --> 00:04:56.848
And if we want to implement a different
algorithm in that upstream,

81
00:04:56.848 --> 00:05:00.711
we can say at least con, but
we'll keep it at default as round robin.

82
00:05:00.711 --> 00:05:04.767
So let's go ahead and do that now.

