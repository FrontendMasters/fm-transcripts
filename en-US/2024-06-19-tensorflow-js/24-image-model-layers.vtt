WEBVTT

1
00:00:00.000 --> 00:00:04.667
So at this point, cool,
we're loading our data, but

2
00:00:04.667 --> 00:00:08.430
now we need to actually
feed it to the model.

3
00:00:08.430 --> 00:00:13.926
So we're going to go to our create
model file, we are going to,

4
00:00:13.926 --> 00:00:20.150
yes, create our model, and
the next step after that will actually be,

5
00:00:20.150 --> 00:00:25.372
we'll be do doing the whole
training in the separate file.

6
00:00:25.372 --> 00:00:31.662
But to create our model, we're going
to start by requiring TensorFlow.js.

7
00:00:31.662 --> 00:00:38.080
So const tf = require('@tensorflow/tfjs,

8
00:00:38.080 --> 00:00:44.506
and after that we have to
declare a few variables.

9
00:00:44.506 --> 00:00:49.996
So first of all, we're going to
declare a variable called kernelSize.

10
00:00:49.996 --> 00:00:55.616
Kernel_, or, actually, we can just write
it like this to keep it consistent.

11
00:00:55.616 --> 00:01:00.872
And the kernelSize can either be
an integer or an array of integers.

12
00:01:00.872 --> 00:01:04.590
And to try to explain what
the kernelSize actually does,

13
00:01:04.590 --> 00:01:08.548
I found a resource that probably
explains it better than me.

14
00:01:08.548 --> 00:01:13.532
So when we have a tensor that has
a certain shape with like a lot of data,

15
00:01:13.532 --> 00:01:15.998
the kernel is basically an array.

16
00:01:15.998 --> 00:01:19.784
So here we have like an array of 3 by 3,
so it's looking at the data and

17
00:01:19.784 --> 00:01:24.172
it's transforming it into like a smaller
output for the next layer of the model.

18
00:01:24.172 --> 00:01:26.934
So instead of doing always
like 28 by 28 data and

19
00:01:26.934 --> 00:01:30.982
then pass the same amount to each layer,
the kernel is actually kind of like

20
00:01:30.982 --> 00:01:35.182
a window, and you look at the data and
you kind of represent it in another way.

21
00:01:35.182 --> 00:01:39.987
So it's okay if you don't necessarily
know why or how, or things like that, but

22
00:01:39.987 --> 00:01:44.242
it's just a way to transform the data
from one layer pass it to the next.

23
00:01:44.242 --> 00:01:48.815
And we are gonna have a couple of layers
that will have filters to transform

24
00:01:48.815 --> 00:01:51.117
the data from one way to another way.

25
00:01:51.117 --> 00:01:55.347
And then it's like a way to find
patterns into data later on.

26
00:01:55.347 --> 00:02:00.195
So here our kernelSize, I personally
worked with an array of 3 by 3, so

27
00:02:00.195 --> 00:02:05.368
as I was showing you in the example,
it's gonna be kind of a matrix of 3 by 3.

28
00:02:05.368 --> 00:02:09.714
So it's gonna look at our input tensor and
just kinda do like,

29
00:02:09.714 --> 00:02:15.668
kinda reducing that 3 by 3 into another,
maybe a single number for the next layer.

30
00:02:15.668 --> 00:02:18.687
So that's our kernelSize,

31
00:02:18.687 --> 00:02:23.475
then we can create
a variable called filter.

32
00:02:23.475 --> 00:02:25.405
So call it filter.

33
00:02:25.405 --> 00:02:29.451
We're gonna set it to 32, and
this thing is that, when we're gonna

34
00:02:29.451 --> 00:02:33.855
create two of our layers in, our model
is going to be a layer called Comp 2D.

35
00:02:33.855 --> 00:02:37.406
And filter is actually
a required variable.

36
00:02:37.406 --> 00:02:43.864
So when you're going through the steps in
your machine learning training process and

37
00:02:43.864 --> 00:02:48.974
each layer, a filter variable here
is gonna be the amount of filters

38
00:02:48.974 --> 00:02:54.197
that the data go through until
the next step of the training process.

39
00:02:54.197 --> 00:02:58.774
So I don't know exactly internally what it
does, but this number can be any number,

40
00:02:58.774 --> 00:03:01.812
I also tried the number 20 or
10 or things like that.

41
00:03:01.812 --> 00:03:05.056
So sometimes you can play with
these values to see if it

42
00:03:05.056 --> 00:03:07.302
affects the accuracy of your model.

43
00:03:07.302 --> 00:03:13.363
So if you want to write something else
than 32, it should also work as well,

44
00:03:13.363 --> 00:03:18.027
but it means that this is going
to apply 32 types of layer or

45
00:03:18.027 --> 00:03:23.473
filters in one layer when it takes
the data from one layer to another.

46
00:03:23.473 --> 00:03:28.621
And then we can have,
we're just gonna have,

47
00:03:28.621 --> 00:03:34.833
just to make it easier,
a variable called numClasses.

48
00:03:34.833 --> 00:03:38.610
And just we're gonna set it to true,
so that if later on,

49
00:03:38.610 --> 00:03:43.886
you decide to take the end result of this
project and add more types of drawings,

50
00:03:43.886 --> 00:03:49.164
you can just change the variable here
instead of figuring out where it's used,

51
00:03:49.164 --> 00:03:50.613
we'll use it later.

52
00:03:50.613 --> 00:03:55.187
Okay, so we're just starting with these
variables, we might add a few later on,

53
00:03:55.187 --> 00:03:58.457
depending on the layers that
we decide to add to our model.

54
00:03:58.457 --> 00:04:00.337
But here we can start
to declare the model.

55
00:04:00.337 --> 00:04:05.077
So we can say const model, we're going
to use a sequential model again.

56
00:04:05.077 --> 00:04:06.785
So tf.sequential, and

57
00:04:06.785 --> 00:04:12.153
sequential just means that you have layers
that are added one after the other,

58
00:04:12.153 --> 00:04:17.293
and the data and the process of processing
the data goes one layer by layer.

59
00:04:17.293 --> 00:04:22.176
So you can declare it like this, and
then we can add individual layers.

60
00:04:22.176 --> 00:04:26.027
So yesterday, if you remember, we declared
an object like this with layers and

61
00:04:26.027 --> 00:04:29.302
an array of layers, but
there's another way to write this as well,

62
00:04:29.302 --> 00:04:32.936
because sometimes if you add a lot of
layers, it can start to be confusing.

63
00:04:32.936 --> 00:04:37.716
So what you can do is
just call tf.sequential,

64
00:04:37.716 --> 00:04:41.309
and then you can call an add method.

65
00:04:41.309 --> 00:04:44.885
So we're gonna add a first layer,

66
00:04:44.885 --> 00:04:49.089
I'm gonna call tf.layers.conv2d.

67
00:04:49.089 --> 00:04:53.259
And inside we can pass
different parameters.

68
00:04:53.259 --> 00:04:57.542
And what I was talking about yesterday
with the shape ,when we resized from 200

69
00:04:57.542 --> 00:05:00.719
to 28 by 28, this is what we're
going to do here as well.

70
00:05:00.719 --> 00:05:03.997
So the parameter for
this is called inputShape, and

71
00:05:03.997 --> 00:05:09.079
it's gonna be an array of 28 by 28,
cuz this is what we resized our images as.

72
00:05:09.079 --> 00:05:12.779
But there's going to be a third number,
that's going to be 4.

73
00:05:12.779 --> 00:05:18.373
And I believe that this is to represent
the fact that the, when we're analyzing

74
00:05:18.373 --> 00:05:23.729
images, you have a value for R, G, B,
and A, and this is like four values.

75
00:05:23.729 --> 00:05:27.966
So here we're basically having
an image by 28 by 28 and

76
00:05:27.966 --> 00:05:32.380
another dimension of four values,
one for our R, G, B, A.

77
00:05:32.380 --> 00:05:35.530
So our model is going to be
expecting that kind of data.

78
00:05:35.530 --> 00:05:39.446
And then we have the parameter filters,
that is required.

79
00:05:39.446 --> 00:05:42.178
And this is going to be
our filter variable.

80
00:05:46.707 --> 00:05:51.587
And then we have also our kernelSize,
that is also required here.

81
00:05:51.587 --> 00:05:53.817
So we can just pass it like this,
cuz we already had the variable.

82
00:05:53.817 --> 00:05:57.490
And then we have something that we
talked about yesterday as well,

83
00:05:57.490 --> 00:05:59.466
that's the Activation function.

84
00:05:59.466 --> 00:06:03.046
And here we're going to use the same
thing that we used yesterday.

85
00:06:03.046 --> 00:06:07.773
And one thing that I actually researched
this morning is that you do have two

86
00:06:07.773 --> 00:06:11.836
other types of activation layers
that are Sigmoid and Softmax.

87
00:06:11.836 --> 00:06:12.867
But apparently,

88
00:06:12.867 --> 00:06:17.616
if you want to use, Softmax, it's usually
used at the last layer in your bottle.

89
00:06:17.616 --> 00:06:20.167
So here because we're in the first,

90
00:06:20.167 --> 00:06:24.050
it's apparently recommended
to use ReLU activation.

91
00:06:24.050 --> 00:06:25.760
So we're going to use that one.

92
00:06:25.760 --> 00:06:28.992
And at this point we have our
first layer of our model.

93
00:06:28.992 --> 00:06:32.714
It usually wouldn't just stop your
your model with one layer, so

94
00:06:32.714 --> 00:06:34.480
we're gonna add another one.

95
00:06:34.480 --> 00:06:41.654
So what we can do is,
we're going to add a new layer.

96
00:06:41.654 --> 00:06:48.697
And I'm actually gonna have to read
the docs, cuz I forget what this one does,

97
00:06:48.697 --> 00:06:54.474
but we can do model.add, and
then again, we have tf.layers.

98
00:06:54.474 --> 00:07:00.431
And here we're gonna do maxPooling2d,
which I think is used as well for

99
00:07:00.431 --> 00:07:04.435
images, and
that's probably why I used it, but

100
00:07:04.435 --> 00:07:10.217
I forgot exactly what it's used for,
so I can look at the docs in a bit.

101
00:07:10.217 --> 00:07:15.010
But the parameter that we're gonna
use in here is called poolSize,

102
00:07:15.010 --> 00:07:18.624
and that can be,
I think I had an array of 2 by 2, so

103
00:07:18.624 --> 00:07:22.091
it might be doing just
another windowing thing.

104
00:07:22.091 --> 00:07:24.975
So we can do 2 by 2 or
you can declare it as a variable,

105
00:07:24.975 --> 00:07:26.849
where we had the kernelSize here.

106
00:07:26.849 --> 00:07:30.934
So we have a first layer,
that's a conv2d layer,

107
00:07:30.934 --> 00:07:37.113
that is expecting the shape of our images,
and then we have a maxPooling2d.

108
00:07:37.113 --> 00:07:41.912
And then what we're going to do is
we're going to flatten this data

109
00:07:41.912 --> 00:07:45.683
because the next layer is
going to be a dense layer and

110
00:07:45.683 --> 00:07:49.814
it's expecting more of like
a one-dimensional input.

111
00:07:49.814 --> 00:07:54.116
So we can do model.add, and

112
00:07:54.116 --> 00:08:00.212
we have tf.layers again, .flatten,

113
00:08:00.212 --> 00:08:05.244
that is a function that you call.

114
00:08:05.244 --> 00:08:09.982
So this is going to basically
take the input, no, yeah,

115
00:08:09.982 --> 00:08:15.117
it's going to take the output of
the previous layer into it and

116
00:08:15.117 --> 00:08:18.700
it's going to output a flattened tensor.

117
00:08:18.700 --> 00:08:23.596
And the reason we do that is because
I think that in the next layer, we're

118
00:08:23.596 --> 00:08:28.750
going to create a dense layer and this
is going to expect a flattened tensor.

119
00:08:28.750 --> 00:08:33.785
So if it doesn't make a lot of sense,
it's okay, this thing is,

120
00:08:33.785 --> 00:08:38.106
if you're not doing it every day,
and I do it every day.

121
00:08:38.106 --> 00:08:40.768
Sometimes, I even forget
what these things do, but

122
00:08:40.768 --> 00:08:43.016
you can play around with different things.

123
00:08:43.016 --> 00:08:45.586
So for example,
if the next layer was not a dense layer,

124
00:08:45.586 --> 00:08:47.236
I don't think you would need this.

125
00:08:47.236 --> 00:08:52.127
But let's try and see what the accuracy
of our model is going to be later on, and

126
00:08:52.127 --> 00:08:55.167
then we can play and
even maybe look at the docs and

127
00:08:55.167 --> 00:08:58.080
maybe try a layer I've
haven't used before.

128
00:08:58.080 --> 00:09:03.004
So as I was saying, we're flattening
the output of the previous layer,

129
00:09:03.004 --> 00:09:05.840
and we're going to create a dense layer.

130
00:09:05.840 --> 00:09:11.564
So we're gonna add model.add,
and here we have tf.layers.dense.

131
00:09:11.564 --> 00:09:18.402
In terms of unit, again, yesterday
I mentioned what these units here,

132
00:09:18.402 --> 00:09:24.467
units means the number of outputs
that are going to be returned.

133
00:09:24.467 --> 00:09:26.391
And here I have the number 10, but

134
00:09:26.391 --> 00:09:29.934
I can see from my notes that before
I tried another number as well.

135
00:09:29.934 --> 00:09:33.138
So again, depending on what we pick here,

136
00:09:33.138 --> 00:09:37.858
it will affect the result, but
we can play around a little bit.

137
00:09:37.858 --> 00:09:41.873
I`m gonna have another activation
that's gonna be a review as well,

138
00:09:41.873 --> 00:09:43.720
cuz we are not at the last layer.

139
00:09:43.720 --> 00:09:47.712
And maybe you're gonna add one more and
that's going to be our last layer, so

140
00:09:47.712 --> 00:09:49.688
we're gonna change our activation.

141
00:09:49.688 --> 00:09:55.488
So if you copy the previous line,
so still, model.add and then dense.

142
00:09:55.488 --> 00:09:57.832
But because it is our last layer,
if you remember,

143
00:09:57.832 --> 00:10:01.497
the units needs to be the number of labels
that we're expecting to be returned.

144
00:10:01.497 --> 00:10:04.791
So here I had created
before my numClasses, so

145
00:10:04.791 --> 00:10:08.600
here instead of 10,
we need to return the classes.

146
00:10:08.600 --> 00:10:13.652
So if you had kept 10 here on your last
layer, it would complain, because it would

147
00:10:13.652 --> 00:10:18.869
be like, well, you're expecting it to
have like 2 layers but you return 10.

148
00:10:18.869 --> 00:10:24.149
And the activate, oops, that typo,
activation, activation.

149
00:10:24.149 --> 00:10:29.344
And because we're at the last layer,
you could leave ReLU if you want,

150
00:10:29.344 --> 00:10:33.249
but let's spice it up a little bit and
write Softmax.

151
00:10:33.249 --> 00:10:38.102
Okay, so at this point, I did one, two,
three, four, five, all right, five layers.

152
00:10:38.102 --> 00:10:39.852
I think it's pretty good.

153
00:10:39.852 --> 00:10:43.236
There could be a lot more,
you could have less, but

154
00:10:43.236 --> 00:10:47.432
then it means that the data,
it goes through less processing.

155
00:10:47.432 --> 00:10:49.752
So we're gonna pause here.

156
00:10:49.752 --> 00:10:51.645
We're not done,
there's gonna be a few lines, but

157
00:10:51.645 --> 00:10:52.989
we can get back to it in a few minutes.

158
00:10:52.989 --> 00:10:57.282
Cool, so now that we have
created our sequential layer and

159
00:10:57.282 --> 00:11:03.060
we have added a few layers to it, we need
to do a step that we also did yesterday,

160
00:11:03.060 --> 00:11:07.901
where we need to actually compile
it with an optimizer function.

161
00:11:07.901 --> 00:11:12.016
So first,
we can declare an optimizer variable.

162
00:11:12.016 --> 00:11:19.795
And we're gonna use the same one that
we used yesterday, so tf.train.add.

163
00:11:19.795 --> 00:11:23.709
And what we pass into it,
if you remember, is the learning rate.

164
00:11:23.709 --> 00:11:28.204
And here, I'm gonna use the same
value as I used yesterday, but

165
00:11:28.204 --> 00:11:32.631
you can tinker with it as well later,
and it was 0.0001.

166
00:11:32.631 --> 00:11:35.169
I made some changes this morning,

167
00:11:35.169 --> 00:11:41.133
trying to play around with a few different
values, and we can do that a bit later.

168
00:11:41.133 --> 00:11:43.557
First of all we are going to
finish what we're doing and

169
00:11:43.557 --> 00:11:47.258
then if the accuracy is not good, we can
play around with different values in here.

170
00:11:47.258 --> 00:11:50.055
But once we have the optimizer,
we need to call the Compile function.

171
00:11:50.055 --> 00:11:55.942
So, model.compile, and
it requires an object as parameter,

172
00:11:55.942 --> 00:11:59.982
and we have our work
passing out optimizer.

173
00:11:59.982 --> 00:12:03.722
And then if you remember, we had like,
oops, loss parameter as well.

174
00:12:03.722 --> 00:12:09.183
We're gonna use the same one as yesterday,
categoricalCrossentropy.

175
00:12:09.183 --> 00:12:15.104
Another parameter that we're going
to have as well this time is matrix,

176
00:12:15.104 --> 00:12:17.734
and we want to get the accuracy.

177
00:12:17.734 --> 00:12:23.464
So whenever we run our model,
it usually kinda returns just the loss.

178
00:12:23.464 --> 00:12:26.621
So you can see if it goes down then you
know that your accuracy should go up, but

179
00:12:26.621 --> 00:12:28.870
here we're also going to
expose the accuracy as well.

180
00:12:28.870 --> 00:12:34.095
So that then you'll see when we're gonna
run this all together in a terminal.

181
00:12:34.095 --> 00:12:39.308
I'll show you your CLC, you'll see,
we'll see the value of the accuracy

182
00:12:39.308 --> 00:12:44.212
that will change over time as the model
goes through different steps.

183
00:12:44.212 --> 00:12:47.502
And it will make sense when we
actually run the whole thing.

184
00:12:47.502 --> 00:12:50.770
But here at this point also,
again, we're in node.js, so

185
00:12:50.770 --> 00:12:54.887
we're going to do module.exports,
and we're going to export our model.

186
00:12:54.887 --> 00:12:57.205
So in this file,
we're not training it yet,

187
00:12:57.205 --> 00:12:59.592
we're just declaring our model separately.

188
00:12:59.592 --> 00:13:04.254
I thought that it would be a bit easier
to make changes to the model if it's like

189
00:13:04.254 --> 00:13:07.937
separate from having everything
as different files, yep.

190
00:13:07.937 --> 00:13:10.953
&gt;&gt; Kind of a general
question about layers,

191
00:13:10.953 --> 00:13:15.517
are there any strategy to how you
kind of select an add layers for

192
00:13:15.517 --> 00:13:19.152
your own project or
is it more of a trial and error?

193
00:13:19.152 --> 00:13:22.969
&gt;&gt; So to me, in general, it's more
of a trial and error that I've seen,

194
00:13:22.969 --> 00:13:26.612
I think there's a few things that
I know need to work a certain way.

195
00:13:26.612 --> 00:13:30.561
So for example, I just know that in terms
of units, the number of units in the last

196
00:13:30.561 --> 00:13:34.592
layer needs to be the number of classes
that you're using when you're training.

197
00:13:34.592 --> 00:13:38.101
So that I know, but instead,
like then for example, Max Cooling 2D,

198
00:13:38.101 --> 00:13:42.268
I don't think I've used it before, but
I thought, let's try and see what it does.

199
00:13:42.268 --> 00:13:48.204
And here we're working with images, so,
I thought a conv2d layer would be good.

200
00:13:48.204 --> 00:13:50.856
But to me, it's more of a trial and error,

201
00:13:50.856 --> 00:13:54.986
I think that if I was doing machine
learning really like every day,

202
00:13:54.986 --> 00:13:59.227
there was probably strategies,
I just don't know them right now.

203
00:13:59.227 --> 00:14:02.511
So if people, after this workshop,
end up doing a lot more machining,

204
00:14:02.511 --> 00:14:04.702
maybe they'll realize that, actually,

205
00:14:04.702 --> 00:14:08.127
we should have used another activation
function or something like that.

206
00:14:08.127 --> 00:14:12.484
So, I do believe that even for people who
do it every day, sometimes there's a bit

207
00:14:12.484 --> 00:14:16.801
of tweaking anyway, in terms of, for
example, the units here, I picked 10.

208
00:14:16.801 --> 00:14:21.137
But I think you could definitely just
pick 20 or 5, and then see what happens.

209
00:14:21.137 --> 00:14:25.564
So you'll tweak it and see how it
affects the accuracy of your model.

210
00:14:25.564 --> 00:14:29.907
So here, even in terms of numbers
of layers, here we have five, but

211
00:14:29.907 --> 00:14:34.098
I think in my original example,
I think I had seven but I was like,

212
00:14:34.098 --> 00:14:37.706
let's try to mix it up a little bit and
see what happens.

213
00:14:37.706 --> 00:14:40.306
So we're gonna do that and
we're gonna run it.

214
00:14:40.306 --> 00:14:43.723
And then, if it actually comes back with
a good accuracy, we could stop there,

215
00:14:43.723 --> 00:14:45.206
we don't have to add more layers.

216
00:14:45.206 --> 00:14:49.053
And if we realize that,
the accuracy is less than 0.5,

217
00:14:49.053 --> 00:14:52.479
then there's multiple things
that we can change then.

218
00:14:52.479 --> 00:14:55.701
It could be the number of layers, it could
be the type of layers, it could just be

219
00:14:55.701 --> 00:14:58.556
that there's not enough data,
it could be a lot of different things.

220
00:14:58.556 --> 00:15:04.609
So that's why there's not a set kind
of recipe on how to get great accuracy.

221
00:15:04.609 --> 00:15:09.076
Or maybe there is, but
I just wouldn't know it then.

222
00:15:09.076 --> 00:15:11.256
So at this point,
I'm just gonna save my file.

223
00:15:11.256 --> 00:15:14.122
So here in this file,
we created a sequential layer,

224
00:15:14.122 --> 00:15:17.944
we added a sequential model,
we created layers and we compiled it, and

225
00:15:17.944 --> 00:15:20.961
we're exporting it to be able
to use it somewhere else.

