WEBVTT

1
00:00:00.182 --> 00:00:06.468
Okay, so for this particular project,
that's pretty much it.

2
00:00:06.468 --> 00:00:10.034
And hopefully, now you kind of understand
a little bit more of the process that you

3
00:00:10.034 --> 00:00:10.921
have to go through.

4
00:00:10.921 --> 00:00:14.074
If you wanted to use data that
was not made on your own, and

5
00:00:14.074 --> 00:00:18.744
it was a dataset that you downloaded from
somewhere, usually, you would still have

6
00:00:18.744 --> 00:00:23.256
to resize some images and pick the shape
of your tensor that you would want to use.

7
00:00:23.256 --> 00:00:29.075
It's more of also a trial-and-error
kind of process, but that's it.

8
00:00:29.075 --> 00:00:31.806
So we created a custom
machine learning model.

9
00:00:31.806 --> 00:00:38.747
And what I want to show you next before
we finish is a few extra resources.

10
00:00:38.747 --> 00:00:42.513
So first of all, when building
your own machine learning model,

11
00:00:42.513 --> 00:00:46.827
I think I talked about it yesterday,
but my project of air street fighter,

12
00:00:46.827 --> 00:00:49.240
I built it using the same kind of concept.

13
00:00:49.240 --> 00:00:53.825
The model is not exactly the same
with the same amount of layers, but

14
00:00:53.825 --> 00:00:58.098
in terms of figuring out what worked,
I used the same process.

15
00:00:58.098 --> 00:01:02.928
I recorded a lot of live data from
gesture, so it was not images,

16
00:01:02.928 --> 00:01:05.616
but it was live data from gestures.

17
00:01:05.616 --> 00:01:10.460
Instead of tensor 1D, I just experimented
with tensor 2D, and I tried that,

18
00:01:10.460 --> 00:01:12.646
and I tried different parameters.

19
00:01:12.646 --> 00:01:16.615
And when I got to a point where the
accuracy was pretty good, then I kind of,

20
00:01:16.615 --> 00:01:21.165
like, okay, I'm keeping that model and
then I'm building an interface around it.

21
00:01:21.165 --> 00:01:24.803
So even though we've used images here,

22
00:01:24.803 --> 00:01:29.083
you can use live data
coming from other places.

23
00:01:29.083 --> 00:01:32.922
For example, yesterday when
we did transfer learning and

24
00:01:32.922 --> 00:01:37.700
I showed you how you can use sound,
and then they transform the sound data

25
00:01:37.700 --> 00:01:42.347
into spectrograms, you could use
a similar code that we wrote today.

26
00:01:42.347 --> 00:01:46.225
But first, you would also have to create
the part that transforms the live data

27
00:01:46.225 --> 00:01:49.632
from the getUserMedia or the Web Audio API
to then spectrograms, and

28
00:01:49.632 --> 00:01:51.359
then you could save these images.

29
00:01:51.359 --> 00:01:56.065
So it means that if you wanted to create
a machine learning model that recognizes

30
00:01:56.065 --> 00:01:59.295
words that you spoke to a computer for
hello or thanks,

31
00:01:59.295 --> 00:02:02.829
then you could represent that
audio data as a spectrogram.

32
00:02:02.829 --> 00:02:05.106
And then just download these images and

33
00:02:05.106 --> 00:02:08.424
create an audio recognition
model out of images as well.

34
00:02:08.424 --> 00:02:14.459
So I haven't done that,
but I would be quite cool.

35
00:02:14.459 --> 00:02:19.435
So I kinda had to pause this project,
but it's actually to use brain

36
00:02:19.435 --> 00:02:24.688
sensor data to actually build a custom
model to recognize me blinking.

37
00:02:24.688 --> 00:02:28.426
So it's not using the webcam,
it's not taking pictures,

38
00:02:28.426 --> 00:02:30.237
I don't know if you can see.

39
00:02:30.237 --> 00:02:34.768
I realized recently that I actually
never finished that project, but

40
00:02:34.768 --> 00:02:37.312
I'm wearing a brain sensor on my head.

41
00:02:37.312 --> 00:02:42.684
It's basically getting live
data from my neural activity.

42
00:02:42.684 --> 00:02:44.022
And using the live data,

43
00:02:44.022 --> 00:02:47.921
I can isolate the data coming from
the sensors that are closer to my head.

44
00:02:47.921 --> 00:02:52.215
And when I'm blinking,
I'm basically gathering that data.

45
00:02:52.215 --> 00:02:56.682
And what I had is I also visualized
it just to see what if I'm

46
00:02:56.682 --> 00:03:01.437
getting raw data from the brain sensor,
do I even see a spike?

47
00:03:01.437 --> 00:03:03.347
It can visualize the spike
when I'm blinking.

48
00:03:03.347 --> 00:03:05.559
And as I see that it could,
then I'm like, okay,

49
00:03:05.559 --> 00:03:08.962
if there's a difference between my
normal brain activity and when I blink.

50
00:03:08.962 --> 00:03:13.552
There's a little spike, because all
of a sudden, I guess your visual,

51
00:03:13.552 --> 00:03:15.294
I don't know how to say it.

52
00:03:15.294 --> 00:03:19.296
But you're not seeing any more or you're
really using the muscles of your face and

53
00:03:19.296 --> 00:03:22.428
that changes a little bit the data
that you get from the sensors,

54
00:03:22.428 --> 00:03:23.994
you can then use that raw data and

55
00:03:23.994 --> 00:03:28.018
train a machine learning model as well
to recognize certain facial expressions.

56
00:03:28.018 --> 00:03:32.959
So what I wanted to do then is to create
a machine learning model to recognize more

57
00:03:32.959 --> 00:03:37.414
thoughts patterns, or at least
different areas of my brain activity.

58
00:03:37.414 --> 00:03:40.359
I wanted to double-check that at
least with blinking, it worked.

59
00:03:40.359 --> 00:03:42.134
And then what I wanted to do is,

60
00:03:42.134 --> 00:03:45.653
can I do a difference between blinking
with my right eye or my left eye?

61
00:03:45.653 --> 00:03:50.045
So just to show you that you can play
with a lot of different types of data,

62
00:03:50.045 --> 00:03:54.869
here we did images cuz it's a little bit,
let's say, faster to work with, but

63
00:03:54.869 --> 00:03:58.757
you can find different ways to
experiment with data as well, and

64
00:03:58.757 --> 00:04:02.729
different sensors and where you
get it from and things like that.

65
00:04:02.729 --> 00:04:04.868
But basically, before we finish,

66
00:04:04.868 --> 00:04:08.232
I just have some resources
that I wanted to show as well.

67
00:04:08.232 --> 00:04:10.544
Yesterday, there was a question about,

68
00:04:10.544 --> 00:04:13.990
do you know an example of
TensorFlow.js used in production?

69
00:04:13.990 --> 00:04:18.181
So I did some research and there's
actually in the TensorFlow.js blog,

70
00:04:18.181 --> 00:04:22.306
there's actually a few examples of
companies using TensorFlow.js for

71
00:04:22.306 --> 00:04:23.943
production applications.

72
00:04:23.943 --> 00:04:26.802
For example, LinkedIn, use TensorFlow.js.

73
00:04:26.802 --> 00:04:31.905
I didn't read the article totally,
but I thought it was cool that your

74
00:04:31.905 --> 00:04:37.718
company like LinkedIn, I think it was for
performance, used certain things.

75
00:04:37.718 --> 00:04:40.371
I can let you read it, but
I just wanted to give you some examples.

76
00:04:40.371 --> 00:04:41.682
If you feel like,

77
00:04:41.682 --> 00:04:46.392
now that you know TensorFlow.js maybe you
can pitch some ideas in your company.

78
00:04:46.392 --> 00:04:50.473
What I really liked is that Adobe used
a TensorFlow.js to enhance a Photoshop

79
00:04:50.473 --> 00:04:50.988
for web.

80
00:04:50.988 --> 00:04:54.938
So I think recently, they made
a Photoshop accessible in the browser and

81
00:04:54.938 --> 00:04:57.412
they used TensorFlow.js to make it better.

82
00:04:57.412 --> 00:04:59.291
So I thought that was pretty cool as well.

83
00:04:59.291 --> 00:05:03.270
Photoshop is a tool that maybe
a lot of people use, and

84
00:05:03.270 --> 00:05:08.586
I thought it was really cool that
a company like Adobe uses that as well.

85
00:05:08.586 --> 00:05:13.205
And I mentioned briefly, maybe in
the conversation yesterday, predictive

86
00:05:13.205 --> 00:05:18.259
prefetching, and that is basically using
navigation data from Google Analytics.

87
00:05:18.259 --> 00:05:21.913
If you have Google Analytics
hooked up to your website, and

88
00:05:21.913 --> 00:05:24.907
you can get the navigation
data from your users,

89
00:05:24.907 --> 00:05:28.723
you can use all of that data to
train a machine learning model.

90
00:05:28.723 --> 00:05:33.827
And then kind of have an idea of how
many times certain links or certain

91
00:05:33.827 --> 00:05:39.994
pages are visited by your users, and you
can prefetch the assets for performance.

92
00:05:39.994 --> 00:05:44.404
To improve performance, instead of
prefetching things only when people

93
00:05:44.404 --> 00:05:49.533
are clicking, you can prefetch in advance
and that is optimizing your performance.

94
00:05:49.533 --> 00:05:52.637
So that's another example.

95
00:05:52.637 --> 00:05:57.785
But in general, if you want to keep in
touch with what's going on in WebML,

96
00:05:57.785 --> 00:06:01.874
then there's a newsletter that
is written by Jason Mayes,

97
00:06:01.874 --> 00:06:07.878
who is actually working at Google on the
developer advocacy team for TensorFlow.js.

98
00:06:07.878 --> 00:06:11.817
So I think it's maybe monthly, yeah,
monthly, he sends a newsletter with

99
00:06:11.817 --> 00:06:15.274
the updates and projects from
the community and things like that.

100
00:06:15.274 --> 00:06:18.171
So if you want to keep in touch,
that's a good resource.

101
00:06:18.171 --> 00:06:22.955
And there's another one as well on
YouTube where you have a series.

102
00:06:22.955 --> 00:06:25.584
It looks like the last one was a year ago.

103
00:06:25.584 --> 00:06:30.390
So maybe they stopped doing it, but it's
still a good resource to see how far you

104
00:06:30.390 --> 00:06:33.349
can really get what can
be done with TensorFlow.

105
00:06:33.349 --> 00:06:37.669
Obviously, here, radiological image
segmentation, that looks pretty cool,

106
00:06:37.669 --> 00:06:40.308
very different from what
we've been doing here.

107
00:06:40.308 --> 00:06:44.579
But here, reinforcement learning,
we didn't do this here, but obviously,

108
00:06:44.579 --> 00:06:47.271
apparently, you can do
it with TensorFlow.js.

109
00:06:47.271 --> 00:06:51.111
And this, yes, all of these
are examples from the community.

110
00:06:51.111 --> 00:06:54.940
Either people who've done things maybe on
their job and they're talking about it, or

111
00:06:54.940 --> 00:06:57.471
really just open source projects
that you can check out.

112
00:06:57.471 --> 00:06:59.747
So there's quite a few videos in there and

113
00:06:59.747 --> 00:07:04.170
I can give you an idea of how you can get
the knowledge that you've had, hopefully

114
00:07:04.170 --> 00:07:08.093
that you've gained in this workshop and
then go a step further as well.

115
00:07:08.093 --> 00:07:13.872
So all these resources are available
on the course website and

116
00:07:13.872 --> 00:07:17.218
there's probably more out there.

117
00:07:17.218 --> 00:07:21.674
But again, now that you've learned at
least the basics, the three main things

118
00:07:21.674 --> 00:07:26.264
that you can do with TensorFlow, then it's
up to you to experiment with different

119
00:07:26.264 --> 00:07:29.589
ideas or look at the docs and
try different types of layers or

120
00:07:29.589 --> 00:07:32.536
architectures and
see really where you can take it.

121
00:07:32.536 --> 00:07:35.786
But thank you for
joining me in this workshop.

122
00:07:35.786 --> 00:07:39.251
Hope you've had fun and I really hope
that you take what you've learned and

123
00:07:39.251 --> 00:07:40.575
that you really take it far.

124
00:07:40.575 --> 00:07:44.008
And maybe I'll see in the wild some
of your projects that you build.

125
00:07:44.008 --> 00:07:45.308
But yeah, thank you very much.

126
00:07:45.308 --> 00:07:49.984
&gt;&gt; [APPLAUSE]

