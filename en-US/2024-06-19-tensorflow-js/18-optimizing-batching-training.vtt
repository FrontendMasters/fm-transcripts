WEBVTT

1
00:00:00.239 --> 00:00:03.539
Okay, so I know that it's probably
confusing if you're doing the first time.

2
00:00:03.539 --> 00:00:09.043
Even me as someone who's done it a few
times, sometimes I forget exactly what,

3
00:00:09.043 --> 00:00:13.640
for example, Kernelizer does, but
we can try to see if it works and

4
00:00:13.640 --> 00:00:17.280
then change the values if
we have an issue with that.

5
00:00:17.280 --> 00:00:20.672
But at this point, we created,
technically, we kind of created

6
00:00:20.672 --> 00:00:24.820
the structure of our sequential model
that is stored in our new model variable.

7
00:00:24.820 --> 00:00:28.516
But it's not yet created,
we just created the structure, but

8
00:00:28.516 --> 00:00:33.327
we have not given our training data to the
model, so it's not really available yet

9
00:00:33.327 --> 00:00:34.670
to use in the browser.

10
00:00:34.670 --> 00:00:41.948
So, then we are going, there's like
two more steps before we fit the data,

11
00:00:41.948 --> 00:00:47.749
is we are going to create an optimizer,
so const optimizer,

12
00:00:47.749 --> 00:00:52.754
and this is going to be
an algorithm that's called,

13
00:00:52.754 --> 00:00:58.713
so tf.train.adam, and
we're passing the learning rate.

14
00:00:58.713 --> 00:01:03.391
And as I was talking about previously,
the learning rate is how often

15
00:01:03.391 --> 00:01:07.192
the model is updated with
when the weights are changed.

16
00:01:07.192 --> 00:01:13.146
And this again, that's a type of
optimizer that uses the Adam algorithm,

17
00:01:13.146 --> 00:01:19.307
but you can see here in the optimizer
function of the docs that there are more.

18
00:01:19.307 --> 00:01:24.686
So if we want, we could change
it to either grad or momentum.

19
00:01:24.686 --> 00:01:29.139
They don't really tell you which one
is best, because then we would have

20
00:01:29.139 --> 00:01:33.306
to go into a deeper thing of,
what's a momentum gradient descent?

21
00:01:33.306 --> 00:01:37.413
And for what we're building,
it's not necessary to know right now,

22
00:01:37.413 --> 00:01:41.953
what this does, it's more that you can
try different ones, see the output.

23
00:01:41.953 --> 00:01:45.164
And then if you want to get deeper
into maybe be more performant or

24
00:01:45.164 --> 00:01:47.781
try different types of output and
see what happens,

25
00:01:47.781 --> 00:01:51.008
then you could dive into the docs and
get a lot more information.

26
00:01:51.008 --> 00:01:54.357
But here,
once we've created our optimizer,

27
00:01:54.357 --> 00:01:57.381
we need to call a function called compile.

28
00:01:57.381 --> 00:01:59.969
So before we are able to train the model,

29
00:01:59.969 --> 00:02:04.549
we need to call the compile method and
we need to pass it that optimizer.

30
00:02:04.549 --> 00:02:09.791
So, I think we might be
able to shorten it but

31
00:02:09.791 --> 00:02:14.752
just in case, gonna pass the optimizer,

32
00:02:14.752 --> 00:02:19.854
and we have a loss
function that is called,

33
00:02:19.854 --> 00:02:24.275
okay, categoricalCrossentropy.

34
00:02:24.275 --> 00:02:28.080
And here in terms of losses, you want your

35
00:02:28.080 --> 00:02:33.078
loss to be as low as possible
by the end of the training,

36
00:02:33.078 --> 00:02:37.339
because it means that
the accuracy is better.

37
00:02:37.339 --> 00:02:40.844
If you end up with a loss
parameter that's too high,

38
00:02:40.844 --> 00:02:43.641
it means that it's not accurate enough.

39
00:02:43.641 --> 00:02:48.612
So here in terms of the loss
that we're setting here,

40
00:02:48.612 --> 00:02:53.805
categoricalCrossentropy might
be actually a function

41
00:02:53.805 --> 00:02:59.116
to calculate the loss that you get or
a type of algorithm.

42
00:02:59.116 --> 00:03:08.736
Actually, let me Or
the type of matrix that you get.

43
00:03:08.736 --> 00:03:11.851
Okay, so I think that whenever you're
getting the loss is calculated in

44
00:03:11.851 --> 00:03:12.626
different ways.

45
00:03:12.626 --> 00:03:16.497
And here I'm using
categoricalCrossentropy, but I feel like

46
00:03:16.497 --> 00:03:21.474
we might be able to use different ones as
well, but we can tinker with that later.

47
00:03:21.474 --> 00:03:27.949
So when you create a model, just a quick
recap, you choose your type of algorithm,

48
00:03:27.949 --> 00:03:32.679
like sequential or else, and
then you add some layers in it.

49
00:03:32.679 --> 00:03:37.515
Then you optimize it when you
compile it with an optimizer, and

50
00:03:37.515 --> 00:03:42.532
the next step will be able to
actually train that model with data,

51
00:03:42.532 --> 00:03:46.563
but first we're going to
split our data by batches.

52
00:03:46.563 --> 00:03:51.247
So the first thing that we do,
have the batchSize variable, and

53
00:03:51.247 --> 00:03:56.733
we are going to use our bySize fraction
variable that we use before as well.

54
00:03:56.733 --> 00:04:00.916
But first we do Math.Flaoa,
to have a round number, and

55
00:04:00.916 --> 00:04:05.971
if you remember, our example data
is stored in a variable called xs,

56
00:04:05.971 --> 00:04:09.567
and you can call the shape
function on it as well.

57
00:04:09.567 --> 00:04:15.656
So here we're going to kind of get
the first input of our examples,

58
00:04:15.656 --> 00:04:20.660
and we're gonna multiply
by the batch size fraction,

59
00:04:20.660 --> 00:04:26.676
and that's going to give us the batch
size based on our examples.

60
00:04:26.676 --> 00:04:31.484
And here we could have like an if
statement that says if the batch

61
00:04:31.484 --> 00:04:35.757
size is zero, then,
kind of like stop the logic there.

62
00:04:35.757 --> 00:04:40.828
But for our example, well, let's forget
about error handling just for a second,

63
00:04:40.828 --> 00:04:45.620
and we should be fine because we are going
to add examples and things like that.

64
00:04:45.620 --> 00:04:49.387
So at this point, when we've have
created the structure of our model and

65
00:04:49.387 --> 00:04:52.782
we've compiled it and
we have a batch size, we can call finally,

66
00:04:52.782 --> 00:04:54.600
the function that's called fit.

67
00:04:54.600 --> 00:04:57.489
And fit is going to actually
be training the model.

68
00:04:57.489 --> 00:05:02.711
And what you pass into fit is that you
pass your data, so your examples xs,

69
00:05:02.711 --> 00:05:07.778
and you also pass your, sorry,
actually xs, and what did I have?

70
00:05:07.778 --> 00:05:09.878
I think it was xy.

71
00:05:09.878 --> 00:05:11.176
Xy, so that should be it.

72
00:05:11.176 --> 00:05:15.257
So we're giving it the data,
we're giving it the labels, and

73
00:05:15.257 --> 00:05:19.262
then we need to pass it some parameters,
like our batch size.

74
00:05:19.262 --> 00:05:23.652
So, because at first, xs contains like
everything, it's not split in two batches.

75
00:05:23.652 --> 00:05:27.981
So first we're passing it our batch size
that we declared just above, and then we

76
00:05:27.981 --> 00:05:32.260
also need to tell the model how many steps
are you gonna go through to be trained?

77
00:05:32.260 --> 00:05:36.680
So we need to pass the epochs,
that I defined at the beginning as well.

78
00:05:36.680 --> 00:05:41.221
So the number of steps, I think we
probably should be able to just do that,

79
00:05:41.221 --> 00:05:43.440
cuz we defined it at the beginning.

80
00:05:43.440 --> 00:05:48.216
And at the end of each epoch,
so at the end of each process,

81
00:05:48.216 --> 00:05:52.603
we can have access to a callback
that kind of can give us

82
00:05:52.603 --> 00:05:57.502
an output every time that
the model has finished one epoch.

83
00:05:57.502 --> 00:05:58.956
So it's kind of called callback.

84
00:05:58.956 --> 00:06:02.086
It's weird that they're not calling it
callback, but it's called callbacks.

85
00:06:02.086 --> 00:06:06.236
And we can have a kind of
sub event of onBatch end.

86
00:06:06.236 --> 00:06:08.380
So every time that you've finished or

87
00:06:08.380 --> 00:06:12.735
that the model has finished kind of like
doing things with one of the batches,

88
00:06:12.735 --> 00:06:16.622
we can have an async function where
we're passing it like the batch.

89
00:06:16.622 --> 00:06:21.720
And we get some logs out of it,
and we can then update our status

90
00:06:21.720 --> 00:06:26.231
element on the page so
that you can see where we're at,

91
00:06:26.231 --> 00:06:29.981
at least in the loss function,
for example.

92
00:06:29.981 --> 00:06:35.828
So we have status element, we are going
to update the innerHTML element,

93
00:06:35.828 --> 00:06:40.540
and we can have a message like,
so what I had worked with is,

94
00:06:40.540 --> 00:06:45.945
you are telling the user that the loss
is we're gonna take the logs.

95
00:06:45.945 --> 00:06:51.878
And we're gonna extract of it, the loss,
and we're just gonna be, I think toFixed5

96
00:06:51.878 --> 00:06:56.853
because I think it's a flow number and
there's a lot of our use in there.

97
00:06:56.853 --> 00:07:00.994
So what this is going to do is that it's
going to train the model with our data,

98
00:07:00.994 --> 00:07:01.896
with our label.

99
00:07:01.896 --> 00:07:03.608
It's gonna split it into batches.

100
00:07:03.608 --> 00:07:06.395
It's gonna run the number
of epochs that there is.

101
00:07:06.395 --> 00:07:10.511
And every time that an epoch
is done on each batch,

102
00:07:10.511 --> 00:07:15.518
it's going to tell us the loss
result of that training step.

103
00:07:15.518 --> 00:07:18.025
So then we'll see that it
will hopefully decrease

104
00:07:18.025 --> 00:07:20.054
as the model gets more performance, and

105
00:07:20.054 --> 00:07:24.424
in the end, when it kinda stops deleting,
we'll know that the training is finished.

106
00:07:24.424 --> 00:07:28.634
And because the training
should be finished after this,

107
00:07:28.634 --> 00:07:32.064
we should be able to set
R is training to false.

108
00:07:32.064 --> 00:07:37.494
So as I was saying, it is more complicated
than the things that are magic,

109
00:07:37.494 --> 00:07:41.085
but for now we're going to
take a bit of a break and

110
00:07:41.085 --> 00:07:45.920
after that we'll be able to
actually run the actual prediction.

111
00:07:45.920 --> 00:07:48.344
Cuz now our model should be
ready to almost being used,

112
00:07:48.344 --> 00:07:50.515
we just haven't done
the prediction part of it.

