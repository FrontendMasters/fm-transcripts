WEBVTT

1
00:00:00.000 --> 00:00:04.760
So for this part 1 and part 2,
we use the same model, so cocoSsd, but

2
00:00:04.760 --> 00:00:06.470
with different inputs.

3
00:00:06.470 --> 00:00:11.657
And now, I want to move on to part 3,
where the piece of the code that

4
00:00:11.657 --> 00:00:18.393
we're going to keep is we're going to keep
the webcam, but we're gonna change model.

5
00:00:18.393 --> 00:00:21.098
And instead of an object detection model,

6
00:00:21.098 --> 00:00:23.803
we are going to use
a face detection model so

7
00:00:23.803 --> 00:00:28.763
that then you can see that with the same
input of an image, you can actually also

8
00:00:28.763 --> 00:00:34.046
start to build applications around seeing
where a person's face is on the screen.

9
00:00:34.046 --> 00:00:37.278
I don't know if it does
multi-people detection, but

10
00:00:37.278 --> 00:00:39.183
we're going to do at least one.

11
00:00:39.183 --> 00:00:44.201
And I use face detection, because actually
I think it can detect multiple faces.

12
00:00:44.201 --> 00:00:49.150
And we're gonna see what comes
out of this model as well and

13
00:00:49.150 --> 00:00:51.579
see what we can do on screen.

14
00:00:51.579 --> 00:00:53.166
But in terms of HTML,

15
00:00:53.166 --> 00:00:58.294
the only thing we're going to change
is move from part two to part 3.

16
00:00:58.294 --> 00:01:00.778
So in terms of what we import,
we start again.

17
00:01:00.778 --> 00:01:04.715
We have TensorFlow.js, but then the rest
is going to be a bit different, so

18
00:01:04.715 --> 00:01:06.696
@ensorflow, and tensorglow.js.

19
00:01:06.696 --> 00:01:09.721
And now, because we're going
to use a different model,

20
00:01:09.721 --> 00:01:11.864
it's already in the package.json, so

21
00:01:11.864 --> 00:01:15.914
don't worry about installing it,
it should have already been installed.

22
00:01:15.914 --> 00:01:20.502
That is going to be called
mediapipe/face_detection.

23
00:01:20.502 --> 00:01:23.998
So mediapipe is a kind of suite of tools.

24
00:01:23.998 --> 00:01:28.648
It's still part of TensorFlow.js and
it's still developed by Google,

25
00:01:28.648 --> 00:01:32.703
but they do a lot of detection of
body parts, so it could be face.

26
00:01:32.703 --> 00:01:38.058
They also do the pose detection model for
more of key points around your body.

27
00:01:38.058 --> 00:01:41.570
I think one is specifically for
hand movement.

28
00:01:41.570 --> 00:01:44.534
So I think when I built the plugin for
a Figma,

29
00:01:44.534 --> 00:01:48.671
it was using the hand detection model,
and it's pretty good.

30
00:01:48.671 --> 00:01:51.692
You have a lot of key points about
different parts of your hands and stuff,

31
00:01:51.692 --> 00:01:53.165
and then you can build cool things.

32
00:01:53.165 --> 00:01:56.953
But for now, we're just gonna keep
it simple, just use face detection.

33
00:01:56.953 --> 00:02:03.262
And when you're using the models in
the MediaPipe kinda bundle of tools,

34
00:02:03.262 --> 00:02:07.750
you also need to import
more TensorFlow.js stuff.

35
00:02:07.750 --> 00:02:12.710
So we have tensorflow and
then tensorflowjs-core, and

36
00:02:12.710 --> 00:02:16.443
we also needed to import backend for that.

37
00:02:16.443 --> 00:02:21.716
So we have tensorflow.js, and
I think this is backend webgl.

38
00:02:21.716 --> 00:02:28.755
So there's a lot of imports going on,
ooh, actually give me a second.

39
00:02:28.755 --> 00:02:30.915
I might have been wrong about that one.

40
00:02:30.915 --> 00:02:35.088
Let's comment it up for now, because I
think I tried a couple of different ways.

41
00:02:35.088 --> 00:02:40.880
The way we're going to import the model
instead is everything as faceDetection

42
00:02:40.880 --> 00:02:46.248
cuz if changed, oops, from, and
then that's TensorFlow.js models.

43
00:02:46.248 --> 00:02:50.108
They've kind of changed a little
bit sometimes the way that they

44
00:02:50.108 --> 00:02:54.194
put things together,
it's under MediaPipe or TensorFlow model.

45
00:02:54.194 --> 00:02:56.669
We're gonna see which one works, I forgot.

46
00:02:56.669 --> 00:02:59.620
I think it might be that one instead.

47
00:02:59.620 --> 00:03:04.004
But it's the same thing, Tensorflow and

48
00:03:04.004 --> 00:03:11.554
MediaPipes are kind of,
Did just work the same way.

49
00:03:11.554 --> 00:03:14.980
And then to make it easier,
we can go to part 2 and

50
00:03:14.980 --> 00:03:17.990
copy our Webcam Capture and Video button,

51
00:03:17.990 --> 00:03:22.934
because in terms of interaction
from the user, it's the same thing.

52
00:03:22.934 --> 00:03:25.917
We're gonna still click on the Webcam
button to turn on the webcam.

53
00:03:25.917 --> 00:03:32.360
We're gonna still have the Capture
button to take a still picture,

54
00:03:32.360 --> 00:03:39.495
and we still need our video element to
pass it to the start webcam function.

55
00:03:39.495 --> 00:03:44.279
Okay, so again, to keep it consistent,

56
00:03:44.279 --> 00:03:48.654
we can have s init async function, and

57
00:03:48.654 --> 00:03:53.046
just gonna declare a model variable.

58
00:03:53.046 --> 00:03:56.971
And in here,
we are going to essentiate our model.

59
00:03:56.971 --> 00:04:00.025
And you'll see here that
the way we instantiate or

60
00:04:00.025 --> 00:04:04.912
we load the face detection model is a bit
different from the syntax for coco Ssd.

61
00:04:04.912 --> 00:04:07.460
So for this one, it's

62
00:04:07.460 --> 00:04:14.851
faceDetection.SupportedModels.MediaPipeFa-
ceDetector.

63
00:04:14.851 --> 00:04:17.446
So I think that's probably why
they've changed the syntax.

64
00:04:17.446 --> 00:04:20.725
Maybe before I was importing it like this,
and now they change,

65
00:04:20.725 --> 00:04:22.557
they move the [INAUDIBLE] js models.

66
00:04:22.557 --> 00:04:29.381
So the way to start, oops, yes, so
at first, we haven't really loaded yet

67
00:04:29.381 --> 00:04:34.407
because there's a few parameters
that we need to pass.

68
00:04:34.407 --> 00:04:41.670
So here, we can do, okay, I'm gonna
declare a variable called detector.

69
00:04:41.670 --> 00:04:44.944
So they just have a different syntax for
this model.

70
00:04:44.944 --> 00:04:48.321
And here, we're going to do detector and

71
00:04:48.321 --> 00:04:53.727
we're going to await the faceDetection
createDetector to detect

72
00:04:53.727 --> 00:04:58.759
what's in the face, and
then we're gonna pass it the model.

73
00:04:58.759 --> 00:05:04.573
And then we have some configs, and
it's just going to be an object

74
00:05:04.573 --> 00:05:09.662
with the runtime is going to be,
yeah, tfjs as a string.

75
00:05:09.662 --> 00:05:14.600
So I know that in terms of the syntax,
this is a little bit different.

76
00:05:14.600 --> 00:05:17.369
So depending on the model
that you're using,

77
00:05:17.369 --> 00:05:20.572
make sure to check the docs
are what's in the README.

78
00:05:20.572 --> 00:05:23.789
I wish that you could just
do faceDetection.load,

79
00:05:23.789 --> 00:05:28.030
but I guess that maybe it's just
because it's like a MediaPipe model

80
00:05:28.030 --> 00:05:31.628
that they do a little bit
differently from the other ones.

81
00:05:31.628 --> 00:05:35.023
And if you start to use maybe
the post-detection model,

82
00:05:35.023 --> 00:05:37.569
I think that's also a MediaPipe model, so

83
00:05:37.569 --> 00:05:41.265
you might have to also create
detectors and things like that.

84
00:05:41.265 --> 00:05:42.745
And do the syntax is
a little bit different,

85
00:05:42.745 --> 00:05:44.186
but it's not that much more lines of code.

86
00:05:44.186 --> 00:05:47.320
So we have the buttons and
we have our init function, but

87
00:05:47.320 --> 00:05:50.533
we did not actually start the webcam or
take the picture.

88
00:05:50.533 --> 00:05:56.698
So as we already wrote these functions,
we can copy them here.

89
00:05:56.698 --> 00:06:01.180
Okay, so we have our onClick
to start the video again,

90
00:06:01.180 --> 00:06:06.159
the webcam, and then we have
our function to take a picture.

91
00:06:06.159 --> 00:06:10.306
And here, again, we're passing
the predict as the callback, but

92
00:06:10.306 --> 00:06:12.434
we did not declare that function.

93
00:06:12.434 --> 00:06:15.526
So this 20 is going to be a little bit
different, so we're gonna write it.

94
00:06:15.526 --> 00:06:19.699
So we have a predict function, predict.

95
00:06:19.699 --> 00:06:23.197
It's going to be async function, and

96
00:06:23.197 --> 00:06:28.005
we're going to pass it
an argument called photo, or

97
00:06:28.005 --> 00:06:31.737
image, or whatever you want to call it.

98
00:06:31.737 --> 00:06:36.684
And as I was telling you before, that once
we're actually calling either predict, or

99
00:06:36.684 --> 00:06:41.313
detect, or anything that is detected from
the model, we get back a prediction.

100
00:06:41.313 --> 00:06:42.667
So here I called it faces,

101
00:06:42.667 --> 00:06:46.683
because that's what we're predicting
when you could call it predictions.

102
00:06:46.683 --> 00:06:51.910
And we're going to await,
oops, the detector,

103
00:06:51.910 --> 00:06:57.396
and we're going to call
the method estimateFaces.

104
00:06:57.396 --> 00:07:02.534
So instead of being more general as
predict or detect as this model is

105
00:07:02.534 --> 00:07:08.588
particularly to detect faces, then they
have a method called estimateFaces.

106
00:07:08.588 --> 00:07:11.095
And we're passing the argument that
we passed into the predict function.

107
00:07:11.095 --> 00:07:14.942
So here, it was photo, but
there's a little bit more.

108
00:07:14.942 --> 00:07:21.101
I think that by default it might
flip the image horizontally,

109
00:07:21.101 --> 00:07:28.339
so we just need to pass it or
flipHorizontal, false, just to make sure.

110
00:07:28.339 --> 00:07:32.267
And at this point, you can either,
oops, log the faces.

111
00:07:32.267 --> 00:07:37.396
Or one util function that is probably
gonna be, okay, let's start with

112
00:07:37.396 --> 00:07:43.113
that just to show you what's actually in
what's returned from the model and then

113
00:07:43.113 --> 00:07:48.856
we'll actually look at the util function
to draw around your face on the screen.

114
00:07:48.856 --> 00:07:53.944
So the last step that we need is to
actually call the init function.

115
00:07:53.944 --> 00:07:58.042
So if I recap, we imported the tools
that are gonna be necessary.

116
00:07:58.042 --> 00:08:03.461
Then we have our elements from
the UI to be able to click on them.

117
00:08:03.461 --> 00:08:07.576
So we have a Webcam button that's
gonna start the webcam feed,

118
00:08:07.576 --> 00:08:11.148
then we have a Capture button
that takes a picture, and

119
00:08:11.148 --> 00:08:14.817
then here we have an init
function that loads the model.

120
00:08:14.817 --> 00:08:19.424
And whenever we capture the webcam,
we have a predict callback that is

121
00:08:19.424 --> 00:08:23.640
going to call estimateFaces that's
gonna return some faces and

122
00:08:23.640 --> 00:08:27.013
we can look at what's gonna
be shown on the screen.

123
00:08:27.013 --> 00:08:32.909
So if I go back here, so
don't worry about the pink border.

124
00:08:32.909 --> 00:08:37.916
So if I enable the webcam,
start, I forgot to

125
00:08:37.916 --> 00:08:43.663
actually import from the utils,
so apologies for that.

126
00:08:43.663 --> 00:08:48.713
If you have picked up on that before,
then it should work for you.

127
00:08:48.713 --> 00:08:52.308
But we are importing startWebcam and
take picture,

128
00:08:52.308 --> 00:08:57.222
cuz that's the two functions from
the details that we're gonna need.

129
00:08:57.222 --> 00:09:00.741
So now, if I refresh, that should work.

130
00:09:00.741 --> 00:09:05.683
So if I click on Webcam, cool,
and then I'm going to capture.

131
00:09:05.683 --> 00:09:08.929
Okay, so [LAUGH] not a good capture.

132
00:09:08.929 --> 00:09:14.067
Anyway, so this is resized, because in
general, I mean, I could look at this.

133
00:09:14.067 --> 00:09:18.647
But I think the model might have been
trained with pictures that were more like

134
00:09:18.647 --> 00:09:20.562
square, so you just resize it.

135
00:09:20.562 --> 00:09:22.972
In the UI, it doesn't look great, but

136
00:09:22.972 --> 00:09:26.998
the model doesn't really care if
the proportions are not right.

137
00:09:26.998 --> 00:09:32.421
Then if I predict, yes, okay,
it took a little bit of time.

138
00:09:32.421 --> 00:09:37.422
But what we get back here is an array of
faces and it has recognized only one face.

139
00:09:37.422 --> 00:09:41.756
I haven't actually tried it with more
people, but if you are building things

140
00:09:41.756 --> 00:09:46.168
with more people, It would be kind of fun
to see if it detects multiple people.

141
00:09:46.168 --> 00:09:52.575
But what you get inside here in the output
of the model, only if I can show, so

142
00:09:52.575 --> 00:09:59.201
you have a box here that is basically
the coordinates of the face on the screen.

143
00:09:59.201 --> 00:10:02.879
So if you want to draw a box
that we're gonna do right after,

144
00:10:02.879 --> 00:10:07.516
it basically tells you that the height and
width and the xMax in terms of x and

145
00:10:07.516 --> 00:10:11.073
y coordinates of where your
face is in the picture taken.

146
00:10:11.073 --> 00:10:14.252
And then in terms of key points, you also
have access to different key points.

147
00:10:14.252 --> 00:10:18.807
So here in the following part of the
exercise, I'm just gonna draw a box that

148
00:10:18.807 --> 00:10:23.574
if you wanted to put things specifically
where the rightEye or where the leftEye

149
00:10:23.574 --> 00:10:27.866
is, then you also have access to the x and
y coordinates of this as well.

