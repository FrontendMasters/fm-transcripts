WEBVTT

1
00:00:00.000 --> 00:00:04.726
And at the beginning I talked about
when you look at teachable machine on

2
00:00:04.726 --> 00:00:10.422
the homepage, at least, we worked with
this one, but there's also an audio model.

3
00:00:10.422 --> 00:00:11.646
And I can show that quickly.

4
00:00:11.646 --> 00:00:15.541
We don't need to implement it because
the second part of the project is going to

5
00:00:15.541 --> 00:00:16.377
be a bit longer.

6
00:00:16.377 --> 00:00:17.398
A question?

7
00:00:17.398 --> 00:00:21.934
&gt;&gt; Is there a rule of thumb when training
these models like how many samples.

8
00:00:21.934 --> 00:00:26.099
For instance,
someone said they did 120 images, and

9
00:00:26.099 --> 00:00:30.101
it seemed like the result was
roughly the same as what?

10
00:00:30.101 --> 00:00:30.663
&gt;&gt; Sure.

11
00:00:30.663 --> 00:00:31.628
&gt;&gt; You did in 30.

12
00:00:31.628 --> 00:00:35.470
&gt;&gt; So the thing is because
we're using the camera as well,

13
00:00:35.470 --> 00:00:39.899
it also depends on what's in your
backgrounds or the lighting.

14
00:00:39.899 --> 00:00:43.630
For example, if you try to teach a model
and let's say you're in like a dark room

15
00:00:43.630 --> 00:00:47.269
or something because it's making
predictions based on what it is in pixels.

16
00:00:47.269 --> 00:00:52.175
If you can't really make the difference
between your background or your movements

17
00:00:52.175 --> 00:00:57.024
or something, sometimes you will have
more troubles not predicting something.

18
00:00:57.024 --> 00:00:58.920
But in terms of number of images,

19
00:00:58.920 --> 00:01:02.191
it will depend on the difference
between your gestures.

20
00:01:02.191 --> 00:01:05.872
So as I said before,
let's say, for example,

21
00:01:05.872 --> 00:01:10.300
like I put my hand here, and
then I just go a little bit up.

22
00:01:10.300 --> 00:01:13.140
There's not that much a difference
between the position of my hand.

23
00:01:13.140 --> 00:01:17.377
So you will need more samples for
the model to be able to really see

24
00:01:17.377 --> 00:01:22.886
a correlation between like, well pixels
are and what label is associated with it.

25
00:01:22.886 --> 00:01:27.281
So I don't know necessarily
the context in which other people

26
00:01:27.281 --> 00:01:29.741
are training their samples here.

27
00:01:29.741 --> 00:01:32.388
But it could be a bunch
of different things.

28
00:01:32.388 --> 00:01:36.308
And usually I would say
more images is better but

29
00:01:36.308 --> 00:01:39.936
it will depend if there's a ton happening.

30
00:01:39.936 --> 00:01:42.606
Maybe in the background is
like colors everywhere and

31
00:01:42.606 --> 00:01:44.445
things you might need more samples so

32
00:01:44.445 --> 00:01:48.201
you can really isolate you from the rest,
but I would say it depends again.

33
00:01:48.201 --> 00:01:52.297
[LAUGH] Sorry, you had another question.

34
00:01:52.297 --> 00:01:53.626
&gt;&gt; To that point,

35
00:01:53.626 --> 00:01:59.365
I ran into a little anomaly where
I did winking left and right eyes.

36
00:01:59.365 --> 00:02:03.228
And I didn't realize that when I recorded
the right eye my head was closer to

37
00:02:03.228 --> 00:02:04.468
the right of the frame.

38
00:02:04.468 --> 00:02:07.611
So then when I would go to test it I
didn't blink my left eye my head was close

39
00:02:07.611 --> 00:02:11.113
to the right of the frame you would think
I was doing the right because I'm guessing

40
00:02:11.113 --> 00:02:12.316
more pixels were changed.

41
00:02:12.316 --> 00:02:17.737
&gt;&gt; Yes, exactly, so that's also the kind
of tricky thing maybe with transfer

42
00:02:17.737 --> 00:02:23.072
learning is or at least with images is
that sometimes if you want it to work,

43
00:02:23.072 --> 00:02:25.716
you have to be in the exact same spot.

44
00:02:25.716 --> 00:02:29.422
Or if I'm training it on a white
background, but then I go and

45
00:02:29.422 --> 00:02:31.527
I train it in a kitchen or in a park,

46
00:02:31.527 --> 00:02:36.420
it will be a little bit different because
it's not isolating you necessarily.

47
00:02:36.420 --> 00:02:40.479
There's no real body recognition because
it could just be my hand without

48
00:02:40.479 --> 00:02:41.068
my body and

49
00:02:41.068 --> 00:02:45.726
it would still have to learn what's going
on there in terms of patterns with pixels.

50
00:02:45.726 --> 00:02:49.291
So yeah, it depends.

51
00:02:49.291 --> 00:02:51.143
Sometimes you could
also be training it and

52
00:02:51.143 --> 00:02:53.208
you're closer to the screen
than other times.

53
00:02:53.208 --> 00:02:56.510
So the best thing to do in that case is,
first of all,

54
00:02:56.510 --> 00:03:01.635
you need to probably understand in which
context you want people to use your app.

55
00:03:01.635 --> 00:03:05.062
If you know that they are gonna
be outdoors in the park,

56
00:03:05.062 --> 00:03:10.071
then maybe you want to train your model
while being in that environment as well.

57
00:03:10.071 --> 00:03:13.141
Or you also have a lot
of different samples.

58
00:03:13.141 --> 00:03:16.143
So you could have,
you could record it like here, and

59
00:03:16.143 --> 00:03:19.358
then you go somewhere else and
you record the same thing.

60
00:03:19.358 --> 00:03:24.127
So that you give it enough samples so that
the model can try to figure out what's

61
00:03:24.127 --> 00:03:27.605
the thing that's like similar
from all of these images.

62
00:03:27.605 --> 00:03:30.491
If I go in a park and I'm like this,
and then I go in the kitchen and

63
00:03:30.491 --> 00:03:31.843
I'm like this, same thing.

64
00:03:31.843 --> 00:03:34.467
It's easier to see
the pixels that are similar,

65
00:03:34.467 --> 00:03:36.963
rather than like the ones
that are different.

66
00:03:36.963 --> 00:03:41.463
So yes, in this workshop here we started
with 30, wasn't quite right, then 100,

67
00:03:41.463 --> 00:03:42.373
it worked for me.

68
00:03:42.373 --> 00:03:46.378
But depending on the use case and
the environment,

69
00:03:46.378 --> 00:03:48.812
you might have to do a lot more.

70
00:03:48.812 --> 00:03:52.626
So if you think about production
application of like machine learning

71
00:03:52.626 --> 00:03:55.803
models, they are trained with
like millions of samples.

72
00:03:55.803 --> 00:03:59.661
So if you were maybe recording yourself,
like a million times,

73
00:03:59.661 --> 00:04:01.453
then that would work better.

74
00:04:01.453 --> 00:04:03.880
But just to show you how it works,

75
00:04:03.880 --> 00:04:08.063
we maybe don't have the time
to train a million samples.

76
00:04:08.063 --> 00:04:13.160
But in general, more samples is better
than the quality of it matters.

77
00:04:13.160 --> 00:04:20.097
There's another tool called Tiny Motion
Trainer, and that is using Arduino boards.

78
00:04:20.097 --> 00:04:22.218
So if you don't know what Arduino is like,
that's a drawing of it.

79
00:04:22.218 --> 00:04:24.688
But it's like a mini macro controller, and

80
00:04:24.688 --> 00:04:28.441
there's one specifically that
works well with TensorFlow.js.

81
00:04:28.441 --> 00:04:31.082
So it was kind of like, I think,
a collaboration between the two.

82
00:04:31.082 --> 00:04:35.547
Where now instead of normal TensorFlow.js
models or TensorFlow models, you have

83
00:04:35.547 --> 00:04:40.023
TensorFlow Lite, so they're even lighter
to be able to run on macro controllers.

84
00:04:40.023 --> 00:04:41.762
And what that allows you to do?

85
00:04:41.762 --> 00:04:45.889
I don't have my device,
right now, but it's also a UI.

86
00:04:45.889 --> 00:04:48.739
I'm not gonna be able to actually
show you what's behind or

87
00:04:48.739 --> 00:04:50.686
actually proceed without committing.

88
00:04:50.686 --> 00:04:56.363
So what's happening in that UI is that
if you do have this little device,

89
00:04:56.363 --> 00:05:02.706
I think that's an Arduino BLE Sense or
Arduino IoT Sense, there's two of them.

90
00:05:02.706 --> 00:05:04.449
And what you can do is just that.

91
00:05:04.449 --> 00:05:09.769
So you connect your Arduino, you connect
it to the browser using the web USBAPI.

92
00:05:09.769 --> 00:05:14.181
And you pick certain settings,
the threshold of difference of movement,

93
00:05:14.181 --> 00:05:17.541
the number of samples,
the delay between captures, and

94
00:05:17.541 --> 00:05:21.268
you start doing the same thing
that we do in teachable machine.

95
00:05:21.268 --> 00:05:25.122
But with data here, so
if I wanted to do a punch gesture,

96
00:05:25.122 --> 00:05:30.050
then I would create punch and
then I would start recording the same way.

97
00:05:30.050 --> 00:05:33.871
And it would actually instead of taking
snapshots of webcam, it would take live

98
00:05:33.871 --> 00:05:37.363
feed from this little device that has
an accelerometer and a gyroscope.

99
00:05:37.363 --> 00:05:41.699
And while I do a punch gesture in the air,
it would record all that live data and

100
00:05:41.699 --> 00:05:44.987
you will be able to do that
with multiple gestures as well.

101
00:05:44.987 --> 00:05:47.789
So that's kind of what I
was talking about, where,

102
00:05:47.789 --> 00:05:50.274
if you're depending what you're building,

103
00:05:50.274 --> 00:05:53.920
you might not want to work with
transfer learning from like images.

104
00:05:53.920 --> 00:05:58.355
You might want to do transfer
learning from motion data.

105
00:05:58.355 --> 00:06:03.225
And that's actually this tiny motion
trainer application is what I use.

106
00:06:03.225 --> 00:06:05.586
No, not that.

107
00:06:05.586 --> 00:06:10.358
Is what I used to do the street fighter,
like the air street fighter thing.

108
00:06:10.358 --> 00:06:12.645
I built this thing in multiple ways.

109
00:06:12.645 --> 00:06:15.975
No, that's brain sensors, not that one.

110
00:06:15.975 --> 00:06:17.592
This one.

111
00:06:17.592 --> 00:06:20.550
So the way that I built this
was like four different ways.

112
00:06:20.550 --> 00:06:22.853
So using four different sensors.

113
00:06:22.853 --> 00:06:26.033
But you could rebuild that using
tiny motion trainer where you have

114
00:06:26.033 --> 00:06:26.894
little Arduino.

115
00:06:26.894 --> 00:06:30.709
And then I record it a few
times like punch, punch and

116
00:06:30.709 --> 00:06:32.896
then hadouken the other one.

117
00:06:32.896 --> 00:06:38.232
And using training relatively quickly
a machine learning model in the browser.

118
00:06:38.232 --> 00:06:44.020
Then it is uploaded onto the Arduino via
Bluetooth, which is really cool as well.

119
00:06:44.020 --> 00:06:47.449
And then it means that once you're
actually connecting your Arduino and

120
00:06:47.449 --> 00:06:50.708
your custom JavaScript application,
you're able to get a label and

121
00:06:50.708 --> 00:06:52.463
also have that on the screen as well.

122
00:06:52.463 --> 00:06:53.298
So a little bit different.

123
00:06:53.298 --> 00:06:58.230
We're not gonna do it in this workshop
because it requires a microcontroller that

124
00:06:58.230 --> 00:06:59.953
you might or might not have.

125
00:06:59.953 --> 00:07:03.700
But it's definitely something
that works as well and

126
00:07:03.700 --> 00:07:07.371
they also have a very nice
UI to help you get started.

127
00:07:07.371 --> 00:07:10.802
Hopefully that helps if
you're building something and

128
00:07:10.802 --> 00:07:13.431
the webcam isn't the right media for you.

129
00:07:13.431 --> 00:07:14.787
This?

130
00:07:14.787 --> 00:07:21.503
&gt;&gt; Arduino has its own framework, right,
too for programming those devices?

131
00:07:21.503 --> 00:07:26.279
&gt;&gt; So usually It's written in like C or
like the Arduino language.

132
00:07:26.279 --> 00:07:27.888
That's kinda like a little bit different.

133
00:07:27.888 --> 00:07:30.553
You can do Arduino stuff in JavaScript,
but

134
00:07:30.553 --> 00:07:33.585
the JavaScript part doesn't
run on the Arduino.

135
00:07:33.585 --> 00:07:38.331
Usually you have to install another
thing called Firmata that then kinda

136
00:07:38.331 --> 00:07:42.936
like transcribe your JavaScript code
into what can run on the device.

137
00:07:42.936 --> 00:07:46.225
So it's usually a little bit.

138
00:07:46.225 --> 00:07:49.103
There's more delay because if
you're writing write in C,

139
00:07:49.103 --> 00:07:51.650
it runs on the board and
it's like super optimized.

140
00:07:51.650 --> 00:07:54.810
If you do Arduino stuff in JavaScript,
it works.

141
00:07:54.810 --> 00:07:57.406
I've done it multiple times, but

142
00:07:57.406 --> 00:08:03.234
it's you can't do applications that
are necessarily very time sensitive.

143
00:08:03.234 --> 00:08:08.560
But for, again for [SOUND] for
this project I built a version with

144
00:08:08.560 --> 00:08:13.903
an Arduino that was not the one
to use with Tiny Motion Trainer.

145
00:08:13.903 --> 00:08:17.192
I think that was a new maker 1,000.

146
00:08:17.192 --> 00:08:21.768
And the way that I did that is I created
my own gesture recognition model.

147
00:08:21.768 --> 00:08:25.681
And hopefully you'll understand
a bit more in part three or

148
00:08:25.681 --> 00:08:27.802
project three, how to do this.

149
00:08:27.802 --> 00:08:32.752
But I recorded raw data a lot of
times doing different gestures.

150
00:08:32.752 --> 00:08:37.764
And then I built a model in using Node.js,
and I plugged in,

151
00:08:37.764 --> 00:08:43.282
it was Johnny 5 is the library to
use with Arduino and JavaScript.

152
00:08:43.282 --> 00:08:47.491
And then sort of getting live data
using 25 in JavaScript, recording it,

153
00:08:47.491 --> 00:08:50.846
everything, all the files
locally creating the model, and

154
00:08:50.846 --> 00:08:53.893
then having WebSockets to
stream it back to the right.

155
00:08:53.893 --> 00:08:56.213
There was a lot involved but
it's possible.

156
00:08:56.213 --> 00:08:56.934
&gt;&gt; Yes.

157
00:08:56.934 --> 00:09:01.745
&gt;&gt; We do have a course on Arduino.

158
00:09:01.745 --> 00:09:02.442
&gt;&gt; Perfect.

159
00:09:02.442 --> 00:09:03.632
&gt;&gt; And it's fairly recent too.

160
00:09:03.632 --> 00:09:05.413
&gt;&gt; Okay.
&gt;&gt; Today and everything so

161
00:09:05.413 --> 00:09:08.659
using the default kinda kit
you can buy off Amazon and

162
00:09:08.659 --> 00:09:09.855
&gt;&gt; Yeah.

163
00:09:09.855 --> 00:09:12.830
&gt;&gt; He programs sensors and stuff.

164
00:09:12.830 --> 00:09:16.885
&gt;&gt; Yeah.
&gt;&gt; Like one that does light mode and

165
00:09:16.885 --> 00:09:21.155
dark mode on a web page with turning off

166
00:09:21.155 --> 00:09:26.273
the lights with a when you turn the
&gt;&gt; Yes no, that's that's super cool.

167
00:09:26.273 --> 00:09:31.237
There's, again, here in this workshop
I'm talking about machine learning,

168
00:09:31.237 --> 00:09:35.555
but you can definitely mix and
match a lot of different technologies.

169
00:09:35.555 --> 00:09:40.947
I mean with hardware, you can also learn
about the web USBAPI or web Bluetooth.

170
00:09:40.947 --> 00:09:44.606
And you can get to learn more about
these web APIs that are available and

171
00:09:44.606 --> 00:09:46.851
they've been available for like a while.

172
00:09:46.851 --> 00:09:53.176
But usually, I guess they're more used in
more kind of creative experimental stuff.

173
00:09:53.176 --> 00:09:54.803
But I really like to mix and match.

