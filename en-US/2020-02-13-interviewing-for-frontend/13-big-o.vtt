WEBVTT

1
00:00:00.160 --> 00:00:01.860
Now we get to some knowledge and

2
00:00:01.860 --> 00:00:05.620
we're gonna have different
knowledge portions of this course.

3
00:00:05.620 --> 00:00:09.460
I want to start with Big-O because it's
something that is a little misunderstood.

4
00:00:09.460 --> 00:00:11.990
I don't know if it should be hyphenated or
not.

5
00:00:13.830 --> 00:00:15.200
I believe it hyphenated
cuz it looks cooler.

6
00:00:16.430 --> 00:00:18.433
Big-O, I mean, what is Big-O?

7
00:00:18.433 --> 00:00:21.090
Someone tell me, and
this will come up a lot.

8
00:00:21.090 --> 00:00:22.500
Some of the questions I'm gonna ask you,

9
00:00:22.500 --> 00:00:26.330
I'm going to say, what's the Big-O
of this function you just wrote?

10
00:00:27.670 --> 00:00:28.880
Yeah.

11
00:00:28.880 --> 00:00:30.725
&gt;&gt; I can try.

12
00:00:30.725 --> 00:00:34.470
Big-O notation is kinda of like the
formula that you use to determine how long

13
00:00:34.470 --> 00:00:37.300
something's gonna take, like a problem.

14
00:00:37.300 --> 00:00:38.880
Or actually I've only
used it as a problem,

15
00:00:38.880 --> 00:00:41.750
I've never used it for anything else,
I don't know if you can.

16
00:00:41.750 --> 00:00:47.840
So,
&gt;&gt; Yeah, well but it's the longest

17
00:00:47.840 --> 00:00:51.740
amount any function is going to take,
given the worst case scenario of input.

18
00:00:53.640 --> 00:01:00.040
It's important to know as engineers, as
computer scientists, we should know Big-O.

19
00:01:00.040 --> 00:01:04.050
Because you're thinking, my function that,

20
00:01:05.240 --> 00:01:09.760
I don't know, my function that I wrote,
that it has for-loops in it.

21
00:01:09.760 --> 00:01:12.320
I only have 100 elements to iterate over,
fine.

22
00:01:12.320 --> 00:01:15.650
But if you write things that
take billions of elements,

23
00:01:15.650 --> 00:01:17.830
then you see that time creep up,
and creep up, and creep up.

24
00:01:17.830 --> 00:01:20.220
Knowing Big-O is really important.

25
00:01:20.220 --> 00:01:25.520
But because it's trying to masters,
I want you to master all the Bigs.

26
00:01:25.520 --> 00:01:28.640
Big Omega, Big Theta, Big O.

27
00:01:28.640 --> 00:01:30.460
You don't have to know Big Omega and
Big Theta.

28
00:01:30.460 --> 00:01:33.440
I think it's just called knowledge to
have because most people say Big O,

29
00:01:34.450 --> 00:01:35.150
we know it's a thing.

30
00:01:35.150 --> 00:01:36.630
But there's Big Omega.

31
00:01:36.630 --> 00:01:39.250
Big Omega describes
the best case scenario,

32
00:01:39.250 --> 00:01:44.800
the operational complexity of
the ideal case of your input.

33
00:01:46.060 --> 00:01:48.500
If all things go well,
it's gonna run really, really quickly, and

34
00:01:48.500 --> 00:01:49.840
that's the best case.

35
00:01:49.840 --> 00:01:52.290
Big theta is the average case.

36
00:01:54.430 --> 00:01:56.590
Well, on average, or

37
00:01:56.590 --> 00:02:00.770
the precise case, it's gonna run
at this complexity, this time.

38
00:02:00.770 --> 00:02:02.840
And big O is the worst case.

39
00:02:02.840 --> 00:02:06.140
And you're thinking, why are computer
scientists so pessimistic,

40
00:02:06.140 --> 00:02:07.710
they only care about Big O?

41
00:02:07.710 --> 00:02:10.640
Yes, it's awesome when
things run on Big omega.

42
00:02:10.640 --> 00:02:12.540
But generally,
we care about the Big O time,

43
00:02:12.540 --> 00:02:14.240
because we wanna know about
the worst-case scenario.

44
00:02:14.240 --> 00:02:17.320
What is the slowest possible thing
that can happen in this function?

45
00:02:19.860 --> 00:02:25.060
So in this case, this search function
which just iterates over an array, and

46
00:02:25.060 --> 00:02:27.380
if it finds whatever
number it's looking for,

47
00:02:27.380 --> 00:02:30.920
it's gonna return true, if it doesn't
find it's gonna return false.

48
00:02:30.920 --> 00:02:34.410
So the best case with this search
function because I have one for

49
00:02:34.410 --> 00:02:38.790
loop, which means I'm iterating over
the elements one time, but I return early.

50
00:02:38.790 --> 00:02:42.090
So the best case is element zero.

51
00:02:42.090 --> 00:02:46.430
Element at index zero is the thing I'm
looking for, we're going to turn true.

52
00:02:46.430 --> 00:02:51.610
So, we're gonna say that's Big O one or
Big omega one, which is constant time.

53
00:02:51.610 --> 00:02:53.110
It's gonna return instantly.

54
00:02:53.110 --> 00:02:55.654
And it doesn't matter how many
elements are in our data set,

55
00:02:55.654 --> 00:03:01.080
it doesn't matter if it's a billion or
a trillion or a gazillion.

56
00:03:01.080 --> 00:03:05.450
I'm not sure if gazillion is a number, but
it doesn't matter if the first element is

57
00:03:05.450 --> 00:03:07.800
the thing we're looking for, it's going to
return immediately so it's constant time.

58
00:03:09.030 --> 00:03:09.590
Big omega.

59
00:03:11.090 --> 00:03:13.290
Big theta is the average time.

60
00:03:13.290 --> 00:03:17.470
On average, we'll probably have to
iterate through the entire array or

61
00:03:17.470 --> 00:03:19.770
we'll say on average we're gonna have
to iterate through half the array.

62
00:03:21.150 --> 00:03:21.980
You follow?

63
00:03:21.980 --> 00:03:24.822
Like if we had a giant data set on
average, we're gonna have to go through

64
00:03:24.822 --> 00:03:28.610
half of it to find the thing we're looking
for in an evenly distributed data set.

65
00:03:28.610 --> 00:03:33.838
So that's actually Big data n over 2 cuz
that's half of the elements we're going

66
00:03:33.838 --> 00:03:39.860
to iterate over, but we always drop those
extra numbers so it's just big data n.

67
00:03:39.860 --> 00:03:43.050
And n is of course the number of
elements you're traversing, or

68
00:03:43.050 --> 00:03:43.970
the size of data set.

69
00:03:43.970 --> 00:03:48.310
It can mean a lot of things, but
it's generally time or number of elements.

70
00:03:48.310 --> 00:03:52.880
So big data on average n over
2 which we just could call n.

71
00:03:52.880 --> 00:03:54.960
The Big O case is also n.

72
00:03:54.960 --> 00:03:57.790
So let's say that's the return false case.

73
00:03:57.790 --> 00:04:02.117
You didn't find the element all of you
are looking for, at the worst case,

74
00:04:02.117 --> 00:04:06.600
we're gonna have to iterate through
the entire data set, the entire array.

75
00:04:06.600 --> 00:04:09.810
And it's like the thing we're looking for
isn't there, cool.

76
00:04:09.810 --> 00:04:11.490
So in the very worst
case of this function,

77
00:04:11.490 --> 00:04:16.950
we're going to say it's Big-O n, hopefully
that helps with Big-O a little bit.

78
00:04:16.950 --> 00:04:21.200
I know it's intimidating cuz people be
like, a tree to access its login or

79
00:04:21.200 --> 00:04:23.230
they'll just throw out numbers
that you expect to memorize.

80
00:04:23.230 --> 00:04:26.565
But instead of memorizing it's better
to really take these to heart.

81
00:04:26.565 --> 00:04:30.051
I'll show you the trick,
I'll show you the Jem trick at the end for

82
00:04:30.051 --> 00:04:33.840
figuring out big-O and when you
say you're like, that makes sense.

83
00:04:33.840 --> 00:04:35.110
Hopefully if I'm doing things right.

84
00:04:37.170 --> 00:04:41.192
And we care about big O because I can say,
it's big-O 2n or

85
00:04:41.192 --> 00:04:44.120
big O n squared,
that doesn't necessarily mean anything,

86
00:04:44.120 --> 00:04:48.800
when you look at a graph of how long
things take to run, so don't think 10 or

87
00:04:48.800 --> 00:04:52.790
100, think a billion elements,
that timing complexity adds up.

88
00:04:53.810 --> 00:04:58.210
So big -O n factorial is
the worst possible time.

89
00:04:58.210 --> 00:05:02.310
Factorial is say, so that's the bang sign,

90
00:05:02.310 --> 00:05:04.450
or exclamation point,
whatever you wanna call it.

91
00:05:04.450 --> 00:05:09.190
So factorial 4 would be 4 times 3
times 2 times 1, that's factorial.

92
00:05:09.190 --> 00:05:10.740
That's the worst possible time.

93
00:05:10.740 --> 00:05:15.550
It's very unusual, if you write something,
to get big O factorial time.

94
00:05:15.550 --> 00:05:19.040
But just don't, your function
will pretty much never finish,

95
00:05:19.040 --> 00:05:20.585
it'll just get worse and worse.

96
00:05:20.585 --> 00:05:23.050
2n, n squared,

97
00:05:23.050 --> 00:05:25.870
those are all terrible running times
you want to avoid things like that.

98
00:05:26.910 --> 00:05:31.710
We get to a little bit reasonable times,
we get to n log n.

99
00:05:31.710 --> 00:05:35.940
So the trick and I won't go into
what logarithmic means, but

100
00:05:35.940 --> 00:05:39.810
the trick is think if you're
iterating over a data set, and

101
00:05:39.810 --> 00:05:42.520
every time you iterate,
you're reducing the number of elements

102
00:05:42.520 --> 00:05:46.630
as in you're filtering the number of
elements you need to iterate over.

103
00:05:46.630 --> 00:05:48.080
That's going to be log time every time.

104
00:05:49.820 --> 00:05:53.810
So giving a good example of log time.

105
00:05:53.810 --> 00:05:57.760
We'll get some examples that are log time,
trees,

106
00:05:57.760 --> 00:06:01.020
funny element as a tree is log
time because trees are ordered.

107
00:06:01.020 --> 00:06:05.400
So I look here and I say,
on let's say a binary search tree.

108
00:06:05.400 --> 00:06:07.970
Is this element bigger or
smaller, I'm gonna go left.

109
00:06:07.970 --> 00:06:10.470
Bigger or smaller,
go right, go left or right.

110
00:06:10.470 --> 00:06:14.370
So, and no time iterating over the entire
tree to find the thing I'm looking for,

111
00:06:14.370 --> 00:06:15.390
that's why it's log time,

112
00:06:15.390 --> 00:06:19.358
because every time I'm reducing
the number of possible matches.

113
00:06:19.358 --> 00:06:24.340
Big O n, that is kind of the base case,
that means we

114
00:06:24.340 --> 00:06:28.200
iterated through the entire data set
to do whatever we're trying to do.

115
00:06:29.460 --> 00:06:30.390
That's gonna happen a lot.

116
00:06:31.894 --> 00:06:35.990
Generally, you see it as a for
loop reiterating over the entire dataset.

117
00:06:35.990 --> 00:06:38.589
Worst case scenario,
that's probably the most common case.

118
00:06:39.798 --> 00:06:44.850
Log n is, like I said before you're
reducing the number of elements that

119
00:06:44.850 --> 00:06:49.560
are between log n and n is, I'm going to
have to iterate over the entire data set.

120
00:06:49.560 --> 00:06:53.100
But as I do every single iteration, I'm
going to narrow down what I'm looking for.

121
00:06:54.560 --> 00:06:56.630
And log n, it's not great.

122
00:06:56.630 --> 00:06:58.330
It's not terrible soccer.

123
00:06:58.330 --> 00:06:59.269
It's somewhere in the middle there.

124
00:07:00.370 --> 00:07:03.570
What you're always striving whenever
you're writing any sort of algorithm

125
00:07:03.570 --> 00:07:06.596
you want log n time cuz that's fast,

126
00:07:06.596 --> 00:07:09.920
that's short of constant time which is
bigger than 1, which means it doesn't

127
00:07:09.920 --> 00:07:11.770
matter what we're doing it's always
gonna return to the same time.

128
00:07:11.770 --> 00:07:13.250
It doesn't matter if they're billion or
trillion,

129
00:07:13.250 --> 00:07:15.030
quadrillion it's always
gonna return the same time.

130
00:07:15.030 --> 00:07:18.870
But it's pretty hard to write
things in constant time, so log n.

131
00:07:19.890 --> 00:07:22.264
I know I'm saying a lot of like
computer sciency terms, but

132
00:07:22.264 --> 00:07:24.508
a lot of companies will ask you,
what's the big O time?

133
00:07:26.883 --> 00:07:30.570
The cheap is looking at something
like this and understanding, okay,

134
00:07:30.570 --> 00:07:33.380
forget big omega, big data,
they're fun to know.

135
00:07:33.380 --> 00:07:36.560
And if you throw that out, I'll be like,
woah, this person is pretty smart.

136
00:07:36.560 --> 00:07:37.370
They're smarter than me.

137
00:07:37.370 --> 00:07:38.870
I wanna hear what they have to say.

138
00:07:40.370 --> 00:07:45.410
But the trick I use is looking at number
four loops looking at number of loops, and

139
00:07:45.410 --> 00:07:47.500
that's like the cheating way.

140
00:07:47.500 --> 00:07:51.647
So, if one for loop is big-O n,

141
00:07:51.647 --> 00:07:55.350
what's the big-O of this?

142
00:07:55.350 --> 00:07:56.180
&gt;&gt; n to the third.

143
00:07:56.180 --> 00:07:58.650
&gt;&gt; Yes, n cubed, n to the third.

144
00:07:58.650 --> 00:08:02.010
Because we're iterating over and then
every iteration we have to iterate over

145
00:08:02.010 --> 00:08:05.670
again, and then every one of those
iterations we iterate over again.

146
00:08:05.670 --> 00:08:10.756
So going back, Yeah,

147
00:08:10.756 --> 00:08:15.127
we're out here, n squared, n to the third,
as you get n to the 10 whatever,

148
00:08:15.127 --> 00:08:18.640
you kind of get here but
you'll never get to this line really.

149
00:08:18.640 --> 00:08:22.240
And that's the trick, it's a simple trick.

150
00:08:22.240 --> 00:08:26.290
So apply that to things that are loops
that don't necessarily look like it.

151
00:08:26.290 --> 00:08:27.251
What's the big-O of this?

152
00:08:33.140 --> 00:08:34.200
&gt;&gt; n cubed?

153
00:08:34.200 --> 00:08:36.208
&gt;&gt; Yes, n to the third again,
it's count the loops,

154
00:08:36.208 --> 00:08:38.710
every loop you're iterating over,
you're iterating over.

155
00:08:39.790 --> 00:08:44.930
There's no special, well actually,
that's my Jem tip, count the loops.

156
00:08:44.930 --> 00:08:48.050
It's a little tricky sometimes and
I make it seem straight forward in these

157
00:08:48.050 --> 00:08:51.711
examples, but generally have
some idea of the Big-O time.

