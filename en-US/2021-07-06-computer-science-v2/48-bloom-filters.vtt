WEBVTT

1
00:00:00.300 --> 00:00:05.085
Let's do the last exercise in this
course before I release you and

2
00:00:05.085 --> 00:00:09.871
your brain from this vise of
knowledge that I'm throwing at you,

3
00:00:09.871 --> 00:00:11.800
this curse of knowledge.

4
00:00:11.800 --> 00:00:17.064
Let's talk about Bloom filters,
which is just like a fun thing to say,

5
00:00:17.064 --> 00:00:20.926
it sounds like it's a really
happy data structure,

6
00:00:20.926 --> 00:00:25.340
actually think it's named after
the guy that invented it.

7
00:00:26.390 --> 00:00:30.934
They normally are, I can't remember why
it's actually called a Bloom filter, but

8
00:00:30.934 --> 00:00:31.639
here we are.

9
00:00:31.639 --> 00:00:35.588
Bloom filters are something that you
actually do see quite a bit out in

10
00:00:35.588 --> 00:00:36.620
the wild today.

11
00:00:36.620 --> 00:00:40.491
There's actually a really great article
here on medium about medium itself that I

12
00:00:40.491 --> 00:00:42.630
dropped in here if you
wanna take a look at it.

13
00:00:44.350 --> 00:00:47.682
And I also put the Wikipedia
article in here so you can see,

14
00:00:47.682 --> 00:00:51.360
here's a bunch of places where
people are using Bloom filters.

15
00:00:53.150 --> 00:00:54.576
So what is a Bloom filter?

16
00:00:54.576 --> 00:00:58.680
The Bloom filter is a data
structure that was designed so

17
00:00:58.680 --> 00:01:03.484
that we could ask it questions
about extremely large data sets and

18
00:01:03.484 --> 00:01:07.088
it could give us an answer of no or
maybe, right?

19
00:01:07.088 --> 00:01:09.931
So it's a way that you design a set.

20
00:01:09.931 --> 00:01:15.105
And so when I say a set I mean
I can add things to a set and

21
00:01:15.105 --> 00:01:21.960
then I can later go back and
ask the set do you have this in your set?

22
00:01:21.960 --> 00:01:26.986
And we actually did this already
with graphs, so let's just hop

23
00:01:26.986 --> 00:01:32.681
back over to graphs so I can show you
concretely what I'm talking about.

24
00:01:32.681 --> 00:01:36.290
Here I have this set called seen, right?

25
00:01:36.290 --> 00:01:36.822
And this is a set.

26
00:01:36.822 --> 00:01:41.637
And then later I ask scene hey,
have you seen this connection before, and

27
00:01:41.637 --> 00:01:43.356
it says yes or no, right?

28
00:01:43.356 --> 00:01:46.690
This has been added to my set, or
this has not been added to my set.

29
00:01:46.690 --> 00:01:50.518
This is 100% accurate, right?

30
00:01:50.518 --> 00:01:55.716
If you ask it have you seen number 32, it
will give you the definitive answer of yes

31
00:01:55.716 --> 00:02:01.310
I have seen 32, or no I have not seen 32
before and it will be 100% correct always.

32
00:02:01.310 --> 00:02:06.065
A Bloom filter, well, so the downside
of this particular set implementation

33
00:02:06.065 --> 00:02:09.750
though is that you must store
everything in the set, right?

34
00:02:09.750 --> 00:02:13.705
Everything that I add to the set
has to be stored somewhere, so

35
00:02:13.705 --> 00:02:19.426
if I add 100 million things to the set,
it has to store 100 million things, right?

36
00:02:19.426 --> 00:02:22.698
This is where Bloom filters
can be kind of interesting,

37
00:02:22.698 --> 00:02:26.041
storing 100 million things
doesn't sound like fun,

38
00:02:26.041 --> 00:02:30.236
what if I could store way less things and
get most of the same advantage?

39
00:02:30.236 --> 00:02:32.451
That's what a bloom filter is.

40
00:02:32.451 --> 00:02:35.130
A Bloom filter,
you can add things to it and

41
00:02:35.130 --> 00:02:40.192
it does kind of a little bit of a trick to
say, hey, I might have seen this before,

42
00:02:40.192 --> 00:02:43.330
or no,
I have definitely never seen this before.

43
00:02:44.370 --> 00:02:49.573
So by trading off that accuracy,
you can drastically reduce how much

44
00:02:49.573 --> 00:02:54.520
storage you're using, so
let's talk about when that would be.

45
00:02:54.520 --> 00:02:57.738
Normally we don't want a margin of
error with our data structures, right?

46
00:02:57.738 --> 00:03:00.905
That's not something that's desirable
about a data structure, however,

47
00:03:00.905 --> 00:03:02.130
using a lot less space can be.

48
00:03:03.780 --> 00:03:06.748
So, in this medium article right
here that I'd linked you to,

49
00:03:06.748 --> 00:03:09.800
they talk about their
recommendation engine.

50
00:03:09.800 --> 00:03:14.006
In the recommendation engine, you scroll
down to the bottom of your article and

51
00:03:14.006 --> 00:03:17.420
they recommend you 3 more articles
that you might like to read.

52
00:03:19.220 --> 00:03:22.598
Medium doesn't want to ask you,
hey, you should read this for

53
00:03:22.598 --> 00:03:25.101
something that you've already read before.

54
00:03:25.101 --> 00:03:29.747
However, it's impractical for medium to
store for every user every single time

55
00:03:29.747 --> 00:03:33.583
how many things that they have read
that are related to something.

56
00:03:33.583 --> 00:03:37.740
That would be an enormous data set
given how many users medium has.

57
00:03:37.740 --> 00:03:41.862
So, what they have done instead is
they use a Bloom filter to say, hey,

58
00:03:41.862 --> 00:03:45.070
has this user ever read
this article before?

59
00:03:45.070 --> 00:03:48.836
And the answer is either no, they have
never read this article before, or maybe,

60
00:03:48.836 --> 00:03:51.730
they might have read this article before.

61
00:03:51.730 --> 00:03:53.351
And the advantage here is that,

62
00:03:53.351 --> 00:03:57.983
if they don't recommend something that you
haven't read before, that's okay, right?

63
00:03:57.983 --> 00:03:58.596
It's okay for

64
00:03:58.596 --> 00:04:02.570
them to just say, here's 3 things that
we definitely know you have never read.

65
00:04:02.570 --> 00:04:05.272
And so
they're okay with that trade off of,

66
00:04:05.272 --> 00:04:09.874
we're willing to not recommend articles
that they haven't read before to

67
00:04:09.874 --> 00:04:12.890
trade off in favor of
much smaller storage.

68
00:04:12.890 --> 00:04:17.678
And we can always show them things
that they had never read before.

69
00:04:17.678 --> 00:04:19.607
Is that kind of make sense?

70
00:04:19.607 --> 00:04:24.015
It's just,
they have this acceptable margin of error.

71
00:04:24.015 --> 00:04:26.658
And you might ask how big
is the margin of error?

72
00:04:26.658 --> 00:04:29.765
And then, it depends on how much
space you're willing to take.

73
00:04:29.765 --> 00:04:34.652
If you're willing to take a lot of space
you can get that margin of error down

74
00:04:34.652 --> 00:04:37.847
to like 1% or 0.1% or something like that.

75
00:04:37.847 --> 00:04:41.574
If you're not willing to do that,
it'll get up to 50%

76
00:04:41.574 --> 00:04:45.862
margin of error it depends on how
big you make the data structure.

77
00:04:45.862 --> 00:04:53.429
So, let's say that we have a Bloom
filter with 10 elements in it, right?

78
00:04:53.429 --> 00:04:58.210
The way that we're gonna model a Bloom
filter is we're gonna have an array of

79
00:04:58.210 --> 00:04:59.040
0s and 1s.

80
00:05:00.850 --> 00:05:03.678
A 1 means that it has been seen before,
and

81
00:05:03.678 --> 00:05:06.515
a 0 means that it has
not been seen before.

82
00:05:06.515 --> 00:05:11.409
What I'm gonna do is I'm gonna run
whatever string that I'm trying to put in

83
00:05:11.409 --> 00:05:12.180
the array.

84
00:05:12.180 --> 00:05:15.874
So let's say I'm trying to
put Brian in my set, right?

85
00:05:15.874 --> 00:05:17.187
In my Bloom filter.

86
00:05:17.187 --> 00:05:21.459
I'm gonna run it through 2 to 5
hashing functions, all right?

87
00:05:21.459 --> 00:05:22.810
Even 8, right?

88
00:05:22.810 --> 00:05:26.757
You can as many hashing
functions as you wanna do.

89
00:05:26.757 --> 00:05:32.787
The more hashing functions you use,
the more space you're gonna use, but

90
00:05:32.787 --> 00:05:39.309
also yeah, it just depends on kind of the
margin of error that you wanna do there.

91
00:05:41.350 --> 00:05:42.741
So let's say I'm doing it through 3,

92
00:05:42.741 --> 00:05:44.990
I'm gonna run Brian through 3
different hashing functions.

93
00:05:44.990 --> 00:05:47.946
And a hashing function if you're
not familiar with something,

94
00:05:47.946 --> 00:05:51.299
just takes a number or takes a string and
converts it to a number, right?

95
00:05:51.299 --> 00:05:55.340
So in this particular case, I'll run it
through 3 different hashing functions.

96
00:05:55.340 --> 00:05:58.380
Let's say one of them gives me 2,
one gives me 5, and one gives me 8.

97
00:05:59.780 --> 00:06:05.970
I'll go back to my data structure here and
I will flip the two elements to 1,

98
00:06:05.970 --> 00:06:10.927
the five elements to 1, and
the eight element to 1, okay?

99
00:06:10.927 --> 00:06:15.727
Now if I go back later and say hey,
have you seen Brian before?

100
00:06:15.727 --> 00:06:18.458
I will run it through
the same hashing functions,

101
00:06:18.458 --> 00:06:22.630
which will definitely give me back 2,
5, and 8 again, right?

102
00:06:22.630 --> 00:06:26.430
And I'll look at 2, 5, and
8 here and it will say maybe,

103
00:06:26.430 --> 00:06:31.154
I might have seen Brian before,
in which case I wouldn't use it, right?

104
00:06:31.154 --> 00:06:35.310
Now, if I go back and
ask later have I seen Sarah before?

105
00:06:35.310 --> 00:06:38.868
Let's say that it gives me 2,
2, and 4, so I go to 0, 1, 2,

106
00:06:38.868 --> 00:06:40.978
this one has been seen before, right?

107
00:06:40.978 --> 00:06:45.832
So there's that, and it goes to 4
which is this one, which is a 0,

108
00:06:45.832 --> 00:06:48.816
because 0 hasn't been flipped before,

109
00:06:48.816 --> 00:06:53.357
it definitely has never seen Sarah before,
for sure, right?

110
00:06:53.357 --> 00:06:57.880
So if it encounters any 0s
here you can go ahead and say,

111
00:06:57.880 --> 00:07:02.607
I've never seen Sarah before,
you can safely use that one

112
00:07:04.504 --> 00:07:09.880
Okay, so let's say that we add one more
item to the array which is Simona,

113
00:07:09.880 --> 00:07:15.798
which gives me back back 0, 4, and 5,
if we went through hashing functions.

114
00:07:15.798 --> 00:07:17.164
So now I erase this.

115
00:07:17.164 --> 00:07:21.899
Now if I go back and ask Sarah again,
if you remember that was 2,

116
00:07:21.899 --> 00:07:24.111
2, 4, so 2, 2, and 4.

117
00:07:24.111 --> 00:07:27.851
Despite the fact that it has never seen
Sarah before, it's gonna say yeah, maybe I

118
00:07:27.851 --> 00:07:31.557
might have seen Sarah before and in this
case it would be a false positive, right?

119
00:07:31.557 --> 00:07:33.895
It would be coming back to you and say,

120
00:07:33.895 --> 00:07:38.293
I might have seen this before when
in reality it hadn't seen it before.

121
00:07:38.293 --> 00:07:41.660
That's the entire gist of
Bloom filters right there.

122
00:07:42.660 --> 00:07:46.865
Instead I could be storing Sarah,
Simona, and Brian in the set, and

123
00:07:46.865 --> 00:07:48.906
I could get a 100% accuracy.

124
00:07:48.906 --> 00:07:51.372
But with this, even if I add 100 elements,

125
00:07:51.372 --> 00:07:55.164
I still need to take 10 items in
the array, if I had a 100 elements,

126
00:07:55.164 --> 00:07:59.233
then I'm definitely gonna get false
positives every single time, right?

127
00:07:59.233 --> 00:08:03.286
But I would still only take
10 items in the array, right?

128
00:08:03.286 --> 00:08:04.518
I wouldn't have to store 100 things.

129
00:08:09.672 --> 00:08:11.019
Okay.

130
00:08:11.019 --> 00:08:13.869
So, the more items you
add to a Bloom filter,

131
00:08:13.869 --> 00:08:18.758
the more you increase the false positive
rate that should be pretty obvious.

132
00:08:18.758 --> 00:08:21.338
You can mitigate this by
having a larger array.

133
00:08:21.338 --> 00:08:26.120
You can also have more or less hashing
functions depending on how much memory

134
00:08:26.120 --> 00:08:31.137
you're willing to fill up versus how
many false positive you wanna be, right?

135
00:08:31.137 --> 00:08:35.011
So let's say I added one
more hashing function, and

136
00:08:35.011 --> 00:08:37.748
that returned 9 for Sarah, right?

137
00:08:37.748 --> 00:08:41.870
In that case,
I wouldn't have gotten a false positive.

138
00:08:41.870 --> 00:08:45.425
So the more hashing functions
you're willing to add as well

139
00:08:45.425 --> 00:08:48.709
means you're likely gonna
get less false positives.

140
00:08:48.709 --> 00:08:53.821
Not totally true, because whenever you add
things as well, yeah, so you kind of have

141
00:08:53.821 --> 00:08:58.737
to mess around with it to kind of really
figure out what your false positives rate.

142
00:08:58.737 --> 00:09:02.070
You get really mathematical with
the false positive rate, so yeah,

143
00:09:02.070 --> 00:09:05.760
definitely just check out the Wikipedia
they explain it pretty well there.

