[00:00:00]
>> Brian Holt: So we're gonna get into setting up our McP client. So two different things today that I'm gonna ask you to really burn into your brain of what are separate. There is an MCP client, something that uses MCP servers and then there are MCP servers. So Claude Desktop, Tome, you can see here, there's a ton of them.

[00:00:21]
VS code, anything that's consuming an MCP server would be a client, right? I'm gonna be using mostly Claude desktop, which is just like the desktop way of interacting with it. The reason why I'm not using the website is because we're gonna be using MCP servers locally, which you can't do on the website.

[00:00:46]
So in order to use Claude Desktop you do have to have an account with them and they have a pretty generous free tier. I would imagine you should be able to get through this course without paying for anything. But I just kinda have a rule that I always want people to be able to use open source stuff and stuff for free.

[00:01:04]
So I did put a link here for Tome, which I have up here as well. So as you can see, it's obviously very inspired by Claude desktop, very similar idea. The big difference is here, if we look at this is you can see I'm actually running everything here off of Ollama, which is like a tool for hosting these models locally.

[00:01:28]
You can see actually I have it running up here in my taskbar. But the other thing that's cool about Tome is that it also can run basically like a Claude desktop environment for ChatGPT which doesn't otherwise exist from OpenAI directly or from Gemini for that matter. So feel free to use Tome.

[00:01:54]
Most of the stuff here will work out of the box for either one of them. There's some stuff that does not work on Tome, but tools do work, which is the most important part of this course. I will show you how to set it up probably once the nice thing about this one, so you can see these are all the models I have installed on Ollama.

[00:02:13]
And again, installing stuff on Ollama is very simple. It's like brew install Ollama, ollama.com, ignore all their turbo stuff. That's their hosted stuff which we're not talking about today. Yeah, it's just ollama run or ollama pull or something like that to install new models. It's really simple. Yeah, and then installing this.

[00:02:49]
Yeah, download here. All you have to do is here is click Download for Mac OS and you can download Ollama. Ollama, all it does is manage all of your models for you and then you can just say ollama --help. And so ollama list. You can see these are all the models that I have installed right now.

[00:03:09]
I have some fairly small models installed. I have 24 gigs of RAM on this laptop, which is not bad for running good sized models. So you do need to be extremely cognizant of how much RAM you have on your computer and specifically GPU VRAM, cuz that'll make a huge difference on what models you can run and how fast they're going to run, right?

[00:03:31]
If you try and run a huge model on a tiny GPU, it's either just not gonna work straight up or it's gonna take a decade to run. So just be very cautious of that. I have these linked somewhere else as well of some good models for you to run on smaller computers.

[00:03:48]
I was mostly using where I could this Qwen 3.8 billion and where I couldn't the quin 3.6 billion. We'll talk about what that means here just in a little bit. But once you have those installed, you can. You can see here that Tome is very smart enough to pull this directly out of Ollama what I have.

[00:04:12]
Tome only works with tools and does not support prompts or resources. We are gonna talk about both prompts and resources today and Tome does not support them. I'm gonna say it's not really that big of a deal because prompts and resources, I'll show you and we'll do them, but I don't think they're that impactful yet.

[00:04:30]
It's tools, right now that are just like everything. So if you're going forward with Tome, I think that's totally fine. Just be aware that once you get to that point, you will not be able to use prompts or resources. Be sure you're using Ollama. That's what everything integrates with.

[00:04:47]
Yeah, I was gonna say Ollama last week introduced Turbo, which is basically the ability to run these models not locally on your computer, which is the entire point of Ollama. But you can run them in the Claude, on Ollama's Claude. If that's what you wanna do, by all means, I didn't try it.

[00:05:04]
It's a paid service, obviously, right? For that kind of paid hosted model, I really like open router, which is another way of doing kind of the same thing. Yeah. Keep in mind that most of us are on, I'm on 24 gigs right here, and that's a pretty beefy laptop in terms of just RAM, and Apple computers have joint VRAM and RAM together.

[00:05:25]
At least the Apple Silicon ones do. But most laptops are gonna really struggle running some of these beefy models. If you're on a laptop you either probably just wanna use Claude, it's gonna be the fastest and easiest way to take this course. Or if you are gonna run Ollama, I'm gonna suggest Qwen 3.8 billion if your computer can handle it okay.

[00:05:49]
And if not, then Qwen 3.6 billion. Just be prepare for some pretty wacky answers from the 0.6 billion. Most importantly, you do need to choose a model that handles tools calling. So you can't just choose any model. Some models don't support it and others do. Make sure that you are choosing one.

[00:06:10]
In fact, I left you the link here. These ones, if it's on this list, it does support tools calling. Yeah.
>> Male Student: You said tool calling, is that specific the MCP tool calling? Are there other types or is it pretty much all like there? I assume these models are trained to know how to call tools around MCP.

[00:06:31]

>> Brian Holt: Yeah, I do think that it's MCP specific. I don't know of another protocol that people are adopting. MCP is kind of one at this point, I think. So by the time that people watch this course recorded online, I guarantee you these are gonna be not the best models for them to run.

[00:06:50]
So you might have to do just a tiny bit of research cuz they just come out at a breakneck pace. But it's really not that important. Some of you might be wondering about these numbers cuz they feel kind of arbitrary. The 1.5 billion, 0.6 billion, 325 million, blah blah blah.

[00:07:10]
Do you remember, some of you I'm probably dating myself, back in when there was Pentium IVs out and there was Athlon XPS. And the Athlons had way slower clock speeds in terms of megahertz, but were just as fast as the Intels. But they had to name their models very similar to the Intel ones cuz everyone was judging everything based on Intel's clock speeds.

[00:07:32]
So they say, this is 3 GHz and this is the 3000 series Athlon. This is the same thing here, right? These billions of parameters just roughly translate to how smart the model is, but it really doesn't, right? So please don't say that this DeepSeek one is dumber than this Phi one, because you're comparing apples and cars, right?

[00:07:56]
It just doesn't make any sense to compare these two things together. But you can use them to say I have this Qwen 3 here and you can see this has many different ones. You can say that roughly this Qwen 0.6 billion, I'm not gonna say it's three times less capable than this 1.7 billion, but you can say this one's gonna be much more capable because they are in the same series, right, the same model.

[00:08:20]
Does that make sense? You can very roughly translate to that to how smart the model is, just with a huge caveat that it might not match up one to one there. So generally when I'm at home I have a gaming computer because I degenerate that plays computer games and I run Ollama on my gaming computer when I'm not playing it cuz it has a 4090 which has.

[00:08:45]
Is that 24 gigs of RAM as well? I don't remember how much it has, enough is the answer to your question. And it can run some of the bigger models and that's all that computer does when I'm not playing games on it. So if you have a gaming computer, that is a very good use case for it.

[00:08:59]
It also uses a lot of electricity. I'm just throwing that out there, beware. I used to have my gaming computer processing all of my home assistant events from my house and that did burn quite a bit of power. So I stopped doing that. I asked the creators of Tome if it was okay if I talk about their stuff on this.

[00:09:16]
They were very happy about that. Their call to action for you is join them on Discord if you think it's a cool piece of technology. And then yeah, I did talk about Open Router here. Yeah, Open Router. Just a slight shout out for that is, if you wanna try a bunch of models in a row, I wanna see if this works well with this model, and then I wanna try this model, and then this model, and this model.

[00:09:36]
Open Router makes it really easy to just swap models really, really quickly. That's why I like it. There's lots of other clients as well. Cursor, Windsurf, VS Code, Claude, Code are kinda coding specific ones. We're gonna be using Claude code today cuz I've tried all of them at this point and it's one that fit my brain the best, and also gave me really good results.

[00:09:58]
I think this is where MCP is really gonna shine the most, cuz we're gonna start introducing things like database MCP servers and GitHub MCP servers. A bunch of really cool stuff. But you should definitely try these. In fact, if you haven't tried all of these, at least Cursor, VS Code, Agent mode, Claude code.

[00:10:18]
If you haven't tried all these, that's your homework, just go try all of these. Go build something just inane and dumb using one of these and see which one you like.

