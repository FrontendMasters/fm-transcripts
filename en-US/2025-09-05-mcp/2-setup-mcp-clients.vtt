WEBVTT

1
00:00:00.640 --> 00:00:03.920
&gt;&gt; Brian Holt: So we're gonna get
into setting up our McP client.

2
00:00:03.920 --> 00:00:07.440
So two different things today that I'm
gonna ask you to really burn into your

3
00:00:07.440 --> 00:00:08.760
brain of what are separate.

4
00:00:08.760 --> 00:00:12.944
There is an MCP client,
something that uses MCP servers and

5
00:00:12.944 --> 00:00:15.680
then there are MCP servers.

6
00:00:15.680 --> 00:00:21.144
So Claude Desktop, Tome,
you can see here, there's a ton of them.

7
00:00:21.144 --> 00:00:27.810
VS code, anything that's consuming
an MCP server would be a client, right?

8
00:00:27.810 --> 00:00:31.317
I'm gonna be using mostly Claude desktop,

9
00:00:31.317 --> 00:00:35.970
which is just like the desktop
way of interacting with it.

10
00:00:37.090 --> 00:00:40.482
The reason why I'm not using the website
is because we're gonna be using MCP

11
00:00:40.482 --> 00:00:42.930
servers locally,
which you can't do on the website.

12
00:00:46.450 --> 00:00:50.331
So in order to use Claude Desktop you do
have to have an account with them and

13
00:00:50.331 --> 00:00:52.410
they have a pretty generous free tier.

14
00:00:52.410 --> 00:00:55.354
I would imagine you should be able to
get through this course without paying

15
00:00:55.354 --> 00:00:55.970
for anything.

16
00:00:57.690 --> 00:01:01.990
But I just kinda have a rule that I
always want people to be able to use open

17
00:01:01.990 --> 00:01:04.330
source stuff and stuff for free.

18
00:01:04.330 --> 00:01:09.290
So I did put a link here for Tome,
which I have up here as well.

19
00:01:09.290 --> 00:01:14.976
So as you can see, it's obviously
very inspired by Claude desktop,

20
00:01:14.976 --> 00:01:16.890
very similar idea.

21
00:01:16.890 --> 00:01:21.218
The big difference is here,
if we look at this is you can see I'm

22
00:01:21.218 --> 00:01:26.200
actually running everything here off
of Ollama, which is like a tool for

23
00:01:26.200 --> 00:01:28.580
hosting these models locally.

24
00:01:28.580 --> 00:01:31.940
You can see actually I have it
running up here in my taskbar.

25
00:01:34.500 --> 00:01:39.393
But the other thing that's cool about
Tome is that it also can run basically

26
00:01:39.393 --> 00:01:44.132
like a Claude desktop environment for
ChatGPT which doesn't otherwise

27
00:01:44.132 --> 00:01:48.030
exist from OpenAI directly or
from Gemini for that matter.

28
00:01:51.150 --> 00:01:52.590
So feel free to use Tome.

29
00:01:54.190 --> 00:01:58.030
Most of the stuff here will work out
of the box for either one of them.

30
00:01:58.030 --> 00:02:01.232
There's some stuff that does not
work on Tome, but tools do work,

31
00:02:01.232 --> 00:02:03.590
which is the most important
part of this course.

32
00:02:03.590 --> 00:02:07.942
I will show you how to set it up probably
once the nice thing about this one, so

33
00:02:07.942 --> 00:02:11.550
you can see these are all the models
I have installed on Ollama.

34
00:02:13.820 --> 00:02:16.220
And again,
installing stuff on Ollama is very simple.

35
00:02:17.660 --> 00:02:22.154
It's like brew install Ollama,

36
00:02:22.154 --> 00:02:28.420
ollama.com, ignore all their turbo stuff.

37
00:02:28.420 --> 00:02:33.020
That's their hosted stuff which
we're not talking about today.

38
00:02:35.500 --> 00:02:39.462
Yeah, it's just ollama run or
ollama pull or

39
00:02:39.462 --> 00:02:43.330
something like that to install new models.

40
00:02:43.330 --> 00:02:44.210
It's really simple.

41
00:02:47.810 --> 00:02:49.090
Yeah, and then installing this.

42
00:02:49.090 --> 00:02:50.610
Yeah, download here.

43
00:02:50.610 --> 00:02:53.172
All you have to do is here is
click Download for Mac OS and

44
00:02:53.172 --> 00:02:54.410
you can download Ollama.

45
00:02:54.410 --> 00:02:58.490
Ollama, all it does is manage
all of your models for you and

46
00:02:58.490 --> 00:03:01.250
then you can just say ollama --help.

47
00:03:03.010 --> 00:03:07.100
And so ollama list.

48
00:03:07.100 --> 00:03:09.935
You can see these are all the models
that I have installed right now.

49
00:03:09.935 --> 00:03:11.900
I have some fairly small models installed.

50
00:03:11.900 --> 00:03:15.147
I have 24 gigs of RAM on this laptop,
which is not bad for

51
00:03:15.147 --> 00:03:16.780
running good sized models.

52
00:03:18.940 --> 00:03:22.668
So you do need to be extremely
cognizant of how much RAM you have on

53
00:03:22.668 --> 00:03:27.642
your computer and specifically GPU VRAM,
cuz that'll make a huge difference on what

54
00:03:27.642 --> 00:03:31.260
models you can run and
how fast they're going to run, right?

55
00:03:31.260 --> 00:03:33.434
If you try and
run a huge model on a tiny GPU,

56
00:03:33.434 --> 00:03:37.501
it's either just not gonna work straight
up or it's gonna take a decade to run.

57
00:03:37.501 --> 00:03:40.200
So just be very cautious of that.

58
00:03:42.040 --> 00:03:45.722
I have these linked somewhere else
as well of some good models for

59
00:03:45.722 --> 00:03:48.120
you to run on smaller computers.

60
00:03:48.120 --> 00:03:53.633
I was mostly using where I
could this Qwen 3.8 billion and

61
00:03:53.633 --> 00:03:57.800
where I couldn't the quin 3.6 billion.

62
00:03:57.800 --> 00:03:59.960
We'll talk about what that means
here just in a little bit.

63
00:04:01.720 --> 00:04:03.400
But once you have those installed,
you can.

64
00:04:05.200 --> 00:04:08.806
You can see here that Tome
is very smart enough to

65
00:04:08.806 --> 00:04:12.320
pull this directly out
of Ollama what I have.

66
00:04:12.320 --> 00:04:16.120
Tome only works with tools and
does not support prompts or resources.

67
00:04:16.120 --> 00:04:18.628
We are gonna talk about both prompts and
resources today and

68
00:04:18.628 --> 00:04:20.480
Tome does not support them.

69
00:04:20.480 --> 00:04:24.374
I'm gonna say it's not really that big
of a deal because prompts and resources,

70
00:04:24.374 --> 00:04:28.400
I'll show you and we'll do them, but
I don't think they're that impactful yet.

71
00:04:30.090 --> 00:04:33.050
It's tools,
right now that are just like everything.

72
00:04:33.050 --> 00:04:35.810
So if you're going forward with Tome,
I think that's totally fine.

73
00:04:35.810 --> 00:04:38.877
Just be aware that once
you get to that point,

74
00:04:38.877 --> 00:04:42.197
you will not be able to use prompts or
resources.

75
00:04:42.197 --> 00:04:44.250
Be sure you're using Ollama.

76
00:04:44.250 --> 00:04:45.770
That's what everything integrates with.

77
00:04:47.450 --> 00:04:50.595
Yeah, I was gonna say Ollama
last week introduced Turbo,

78
00:04:50.595 --> 00:04:54.959
which is basically the ability to run
these models not locally on your computer,

79
00:04:54.959 --> 00:04:56.970
which is the entire point of Ollama.

80
00:04:56.970 --> 00:05:00.750
But you can run them in the Claude,
on Ollama's Claude.

81
00:05:00.750 --> 00:05:04.270
If that's what you wanna do,
by all means, I didn't try it.

82
00:05:04.270 --> 00:05:07.310
It's a paid service, obviously, right?

83
00:05:07.310 --> 00:05:11.020
For that kind of paid hosted model,
I really like open router,

84
00:05:11.020 --> 00:05:14.270
which is another way of doing
kind of the same thing.

85
00:05:14.270 --> 00:05:16.882
Yeah.
Keep in mind that most of us are on,

86
00:05:16.882 --> 00:05:22.395
I'm on 24 gigs right here, and that's a
pretty beefy laptop in terms of just RAM,

87
00:05:22.395 --> 00:05:25.982
and Apple computers have joint VRAM and
RAM together.

88
00:05:25.982 --> 00:05:29.500
At least the Apple Silicon ones do.

89
00:05:29.500 --> 00:05:33.836
But most laptops are gonna really struggle
running some of these beefy models.

90
00:05:33.836 --> 00:05:37.975
If you're on a laptop you either
probably just wanna use Claude,

91
00:05:37.975 --> 00:05:41.975
it's gonna be the fastest and
easiest way to take this course.

92
00:05:41.975 --> 00:05:44.123
Or if you are gonna run Ollama,

93
00:05:44.123 --> 00:05:49.420
I'm gonna suggest Qwen 3.8 billion
if your computer can handle it okay.

94
00:05:49.420 --> 00:05:52.020
And if not, then Qwen 3.6 billion.

95
00:05:52.020 --> 00:05:56.120
Just be prepare for some pretty
wacky answers from the 0.6 billion.

96
00:05:58.680 --> 00:06:02.760
Most importantly, you do need to choose
a model that handles tools calling.

97
00:06:02.760 --> 00:06:04.680
So you can't just choose any model.

98
00:06:04.680 --> 00:06:08.600
Some models don't support it and
others do.

99
00:06:08.600 --> 00:06:10.760
Make sure that you are choosing one.

100
00:06:10.760 --> 00:06:12.760
In fact, I left you the link here.

101
00:06:12.760 --> 00:06:16.600
These ones, if it's on this list,
it does support tools calling.

102
00:06:16.600 --> 00:06:17.117
Yeah.

103
00:06:17.117 --> 00:06:21.808
&gt;&gt; Male Student: You said tool calling,
is that specific the MCP tool calling?

104
00:06:21.808 --> 00:06:25.200
Are there other types or
is it pretty much all like there?

105
00:06:25.200 --> 00:06:31.120
I assume these models are trained to
know how to call tools around MCP.

106
00:06:31.120 --> 00:06:34.800
&gt;&gt; Brian Holt: Yeah,
I do think that it's MCP specific.

107
00:06:34.800 --> 00:06:38.880
I don't know of another protocol
that people are adopting.

108
00:06:38.880 --> 00:06:42.160
MCP is kind of one at this point, I think.

109
00:06:42.160 --> 00:06:45.752
So by the time that people watch
this course recorded online,

110
00:06:45.752 --> 00:06:49.770
I guarantee you these are gonna be
not the best models for them to run.

111
00:06:50.810 --> 00:06:54.272
So you might have to do just a tiny bit
of research cuz they just come out at

112
00:06:54.272 --> 00:06:55.210
a breakneck pace.

113
00:06:57.050 --> 00:06:58.330
But it's really not that important.

114
00:07:00.410 --> 00:07:03.649
Some of you might be wondering about these
numbers cuz they feel kind of arbitrary.

115
00:07:03.649 --> 00:07:08.570
The 1.5 billion, 0.6 billion,
325 million, blah blah blah.

116
00:07:10.970 --> 00:07:15.694
Do you remember,
some of you I'm probably dating myself,

117
00:07:15.694 --> 00:07:20.711
back in when there was Pentium IVs out and
there was Athlon XPS.

118
00:07:20.711 --> 00:07:25.059
And the Athlons had way slower clock
speeds in terms of megahertz, but

119
00:07:25.059 --> 00:07:27.060
were just as fast as the Intels.

120
00:07:27.060 --> 00:07:30.381
But they had to name their models very
similar to the Intel ones cuz everyone was

121
00:07:30.381 --> 00:07:32.980
judging everything based
on Intel's clock speeds.

122
00:07:32.980 --> 00:07:38.660
So they say, this is 3 GHz and
this is the 3000 series Athlon.

123
00:07:38.660 --> 00:07:40.900
This is the same thing here, right?

124
00:07:40.900 --> 00:07:45.791
These billions of parameters just roughly
translate to how smart the model is,

125
00:07:45.791 --> 00:07:48.240
but it really doesn't, right?

126
00:07:48.240 --> 00:07:52.665
So please don't say that this DeepSeek
one is dumber than this Phi one,

127
00:07:52.665 --> 00:07:56.160
because you're comparing apples and
cars, right?

128
00:07:56.160 --> 00:07:58.560
It just doesn't make any sense to
compare these two things together.

129
00:08:00.800 --> 00:08:03.613
But you can use them to say
I have this Qwen 3 here and

130
00:08:03.613 --> 00:08:06.320
you can see this has many different ones.

131
00:08:06.320 --> 00:08:10.860
You can say that roughly this Qwen 0.6
billion, I'm not gonna say it's three

132
00:08:10.860 --> 00:08:15.667
times less capable than this 1.7 billion,
but you can say this one's gonna be much

133
00:08:15.667 --> 00:08:20.460
more capable because they are in
the same series, right, the same model.

134
00:08:20.460 --> 00:08:21.900
Does that make sense?

135
00:08:21.900 --> 00:08:27.408
You can very roughly translate to
that to how smart the model is,

136
00:08:27.408 --> 00:08:33.020
just with a huge caveat that it
might not match up one to one there.

137
00:08:33.020 --> 00:08:37.402
So generally when I'm at home I have
a gaming computer because I degenerate

138
00:08:37.402 --> 00:08:40.534
that plays computer games and
I run Ollama on my gaming

139
00:08:40.534 --> 00:08:44.380
computer when I'm not playing
it cuz it has a 4090 which has.

140
00:08:45.440 --> 00:08:47.080
Is that 24 gigs of RAM as well?

141
00:08:47.080 --> 00:08:49.760
I don't remember how much it has,
enough is the answer to your question.

142
00:08:51.040 --> 00:08:52.625
And it can run some of
the bigger models and

143
00:08:52.625 --> 00:08:55.120
that's all that computer does
when I'm not playing games on it.

144
00:08:55.120 --> 00:08:59.240
So if you have a gaming computer,
that is a very good use case for it.

145
00:08:59.240 --> 00:09:00.960
It also uses a lot of electricity.

146
00:09:00.960 --> 00:09:02.720
I'm just throwing that out there, beware.

147
00:09:04.320 --> 00:09:08.401
I used to have my gaming computer
processing all of my home assistant events

148
00:09:08.401 --> 00:09:11.200
from my house and
that did burn quite a bit of power.

149
00:09:11.200 --> 00:09:13.000
So I stopped doing that.

150
00:09:13.000 --> 00:09:16.950
I asked the creators of Tome if it was
okay if I talk about their stuff on this.

151
00:09:16.950 --> 00:09:19.110
They were very happy about that.

152
00:09:19.110 --> 00:09:20.135
Their call to action for

153
00:09:20.135 --> 00:09:23.270
you is join them on Discord if you
think it's a cool piece of technology.

154
00:09:25.270 --> 00:09:27.430
And then yeah,
I did talk about Open Router here.

155
00:09:27.430 --> 00:09:28.590
Yeah, Open Router.

156
00:09:28.590 --> 00:09:31.879
Just a slight shout out for that is, if
you wanna try a bunch of models in a row,

157
00:09:31.879 --> 00:09:35.221
I wanna see if this works well with this
model, and then I wanna try this model,

158
00:09:35.221 --> 00:09:36.950
and then this model, and this model.

159
00:09:36.950 --> 00:09:40.130
Open Router makes it really easy to
just swap models really, really quickly.

160
00:09:40.130 --> 00:09:40.970
That's why I like it.

161
00:09:40.970 --> 00:09:43.065
There's lots of other clients as well.

162
00:09:43.065 --> 00:09:47.170
Cursor, Windsurf, VS Code, Claude,
Code are kinda coding specific ones.

163
00:09:47.170 --> 00:09:52.949
We're gonna be using Claude code today cuz
I've tried all of them at this point and

164
00:09:52.949 --> 00:09:58.251
it's one that fit my brain the best,
and also gave me really good results.

165
00:09:58.251 --> 00:10:02.160
I think this is where MCP is really gonna
shine the most, cuz we're gonna start

166
00:10:02.160 --> 00:10:05.730
introducing things like database
MCP servers and GitHub MCP servers.

167
00:10:05.730 --> 00:10:06.690
A bunch of really cool stuff.

168
00:10:08.800 --> 00:10:10.160
But you should definitely try these.

169
00:10:11.920 --> 00:10:16.622
In fact, if you haven't tried all
of these, at least Cursor, VS Code,

170
00:10:16.622 --> 00:10:18.405
Agent mode, Claude code.

171
00:10:18.405 --> 00:10:22.320
If you haven't tried all these, that's
your homework, just go try all of these.

172
00:10:22.320 --> 00:10:26.257
Go build something just inane and
dumb using one of these and

173
00:10:26.257 --> 00:10:27.840
see which one you like.

