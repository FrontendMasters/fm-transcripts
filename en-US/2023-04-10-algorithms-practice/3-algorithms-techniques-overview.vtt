WEBVTT

1
00:00:00.080 --> 00:00:03.560
So let's talk about
some common algorithms.

2
00:00:03.560 --> 00:00:05.850
Not all of these are ones that
we're gonna tackle again.

3
00:00:05.850 --> 00:00:09.650
These are just things that you'll hear,
that you'll Google, you'll find.

4
00:00:10.920 --> 00:00:12.714
There are sorting algorithms.

5
00:00:12.714 --> 00:00:16.563
A couple of the most famous ones
are BubbleSort and QuickSort,

6
00:00:16.563 --> 00:00:18.970
there are dozens of other ones.

7
00:00:18.970 --> 00:00:21.515
There's all kinds of general and
specialized sorts for

8
00:00:21.515 --> 00:00:22.660
different conditions.

9
00:00:23.720 --> 00:00:27.330
I'm gonna be straight up honest with you,
I don't care about sorting at all.

10
00:00:27.330 --> 00:00:31.030
It's the least interesting
DSA topic to me.

11
00:00:31.030 --> 00:00:36.500
People have figured that out, they've
written volumes of information about them.

12
00:00:36.500 --> 00:00:41.781
And the most that I've ever cared about
the sorting algorithm is I love the little

13
00:00:41.781 --> 00:00:46.740
fancy animations where they show how
it works conceptually or whatever.

14
00:00:46.740 --> 00:00:50.869
That's about as much as I'm gonna care, so
we're not gonna deal with sorting in this.

15
00:00:50.869 --> 00:00:58.401
And sorting is one of those key common
topics that are covered in a DSA course.

16
00:00:58.401 --> 00:01:01.341
I'm just gonna skip it cuz I
think it's not as interesting.

17
00:01:01.341 --> 00:01:03.296
If I need to sort something,

18
00:01:03.296 --> 00:01:07.620
I'm pretty likely to just use
the built-in JavaScript sort.

19
00:01:07.620 --> 00:01:11.164
And if that's somehow for
some reason not gonna work,

20
00:01:11.164 --> 00:01:15.568
then I'm just gonna grab a QuickSort
implementation most likely.

21
00:01:15.568 --> 00:01:19.220
I'm not gonna get much deeper than that,
in my own coding.

22
00:01:19.220 --> 00:01:24.853
But your mileage may vary,
your mileage will vary on that.

23
00:01:24.853 --> 00:01:26.917
All right so setting aside sorts,

24
00:01:26.917 --> 00:01:30.263
there are things that I find
much more interesting, and

25
00:01:30.263 --> 00:01:34.480
the kinds of work that I tackle,
I seem to come across a lot more often.

26
00:01:34.480 --> 00:01:39.960
Traversing things like trees and graphs,
and when we get into graphs, we have this

27
00:01:39.960 --> 00:01:45.523
interesting concept called path finding,
which is, I have this multi-connected

28
00:01:45.523 --> 00:01:50.100
set of these different elements that
will be called nodes in a graph.

29
00:01:50.100 --> 00:01:53.110
And they may not actually have
direct connections to each other.

30
00:01:53.110 --> 00:01:57.166
So there's various different circuitous
routes that I could take to go from one

31
00:01:57.166 --> 00:01:59.539
node in the graph to
another node in the graph.

32
00:01:59.539 --> 00:02:02.940
And there are different reasons why I
might wanna take one route versus another.

33
00:02:02.940 --> 00:02:06.862
For example, you might have
what's called a weighted graph,

34
00:02:06.862 --> 00:02:09.748
meaning that the connection
between node A and

35
00:02:09.748 --> 00:02:15.120
B costs more for some definition of
cost than the connection from A to C.

36
00:02:15.120 --> 00:02:19.374
And it might be that a longer route,
meaning more hops,

37
00:02:19.374 --> 00:02:22.371
costs less than the more direct route.

38
00:02:22.371 --> 00:02:27.240
So we can think about variety of
different real world metaphors for this,

39
00:02:27.240 --> 00:02:29.918
like toll bridges and things like that.

40
00:02:29.918 --> 00:02:34.000
There's a more circuitous route
that costs less, takes you longer.

41
00:02:34.000 --> 00:02:36.614
There's the more direct route,
you get there quicker, but

42
00:02:36.614 --> 00:02:38.550
now you've gotta pay a higher toll.

43
00:02:38.550 --> 00:02:42.816
By the way, I talk about toll roads and
toll bridges.

44
00:02:42.816 --> 00:02:47.022
I used to think that must be one of
those universal cultural references, and

45
00:02:47.022 --> 00:02:48.070
it's not.

46
00:02:48.070 --> 00:02:52.400
I've gone places in the world that people
are like, you have to pay to use a road?

47
00:02:52.400 --> 00:02:54.578
It's completely foreign to them.

48
00:02:54.578 --> 00:02:57.908
I grew up in the land of toll roads.

49
00:02:57.908 --> 00:03:01.702
I happily pay that as one of
my little life's luxury taxes.

50
00:03:01.702 --> 00:03:06.654
I don't worry about toll roads, but for
some people, that's a foreign concept, so.

51
00:03:06.654 --> 00:03:08.392
Maybe you need a different metaphor there.

52
00:03:08.392 --> 00:03:10.007
Insert your own metaphor for

53
00:03:10.007 --> 00:03:14.859
having something that you have to pay more
to get across or something, I don't know.

54
00:03:14.859 --> 00:03:18.800
Anyway, so we have algorithms that
allow us to go through a tree.

55
00:03:18.800 --> 00:03:21.513
And again, remember trees are ordered, so

56
00:03:21.513 --> 00:03:25.264
the order that we go through
the tree is a big deal sometimes.

57
00:03:25.264 --> 00:03:28.682
Sometimes we wanna just visit
every node in the tree and

58
00:03:28.682 --> 00:03:32.315
it doesn't matter what
order we visit them in.

59
00:03:32.315 --> 00:03:34.686
But many, and
I would say maybe even most of the time,

60
00:03:34.686 --> 00:03:37.125
if we're gonna go through a tree,
the order matters.

61
00:03:38.725 --> 00:03:40.354
The order matters, perhaps,

62
00:03:40.354 --> 00:03:43.492
because we're processing all
of the elements in the tree.

63
00:03:43.492 --> 00:03:46.835
And the operations that we're
processing with are not commutative, so

64
00:03:46.835 --> 00:03:49.150
the order of the operations matters.

65
00:03:49.150 --> 00:03:54.080
Sometimes it's that we're traversing
through a tree and we need to stop early.

66
00:03:54.080 --> 00:03:57.237
So obviously, the order matters,
because if we go in the wrong order,

67
00:03:57.237 --> 00:04:00.270
we're not gonna stop as
early as we wanted to.

68
00:04:00.270 --> 00:04:02.090
So those are things to think about.

69
00:04:02.090 --> 00:04:04.115
And then I threw up here binary search.

70
00:04:04.115 --> 00:04:07.287
Similar to sorting,
search algorithms, to me,

71
00:04:07.287 --> 00:04:12.043
seem pretty straightforward, and I,
again, don't end up implementing

72
00:04:12.043 --> 00:04:16.080
search algorithms in
the classical sense very often.

73
00:04:16.080 --> 00:04:20.870
But the concept of binary search,
we'll get into this in the next slide.

74
00:04:20.870 --> 00:04:25.040
The concept of binary search being
recursive, that's pretty important.

75
00:04:25.040 --> 00:04:30.909
So being able to understand the underlying
principle of something like binary search,

76
00:04:30.909 --> 00:04:34.175
meaning divide a problem
into multiple pieces.

77
00:04:34.175 --> 00:04:37.864
In the binary world, that's divided
into two pieces, so divided in half and

78
00:04:37.864 --> 00:04:40.999
only work with half the problem,
and then divided again in half.

79
00:04:40.999 --> 00:04:45.448
But there's higher order divisions where
you divide it into three or four or

80
00:04:45.448 --> 00:04:46.150
whatever.

81
00:04:46.150 --> 00:04:49.900
So binary is just the most common,
and that means divided in half.

82
00:04:49.900 --> 00:04:54.034
Divide and conquer would be sort
of a general term for that.

83
00:04:54.034 --> 00:04:55.819
And finally, a few of the techniques,

84
00:04:55.819 --> 00:04:59.720
these are things that we'll go
through throughout the workshop.

85
00:04:59.720 --> 00:05:04.120
These are common techniques that you
end up using to create your algorithms.

86
00:05:04.120 --> 00:05:08.980
You implement algorithms, these are some
of those LEGO pieces that we put together.

87
00:05:08.980 --> 00:05:12.318
Iteration, pretty straightforward things,
like for loops, while loops.

88
00:05:12.318 --> 00:05:15.617
We're going to iterate in order for
a certain amount of time or

89
00:05:15.617 --> 00:05:17.912
perhaps just an arbitrary amount of time.

90
00:05:17.912 --> 00:05:20.536
When you have a while true loop,

91
00:05:20.536 --> 00:05:26.570
that's just gonna keep going until
you've exhausted the entire space.

92
00:05:26.570 --> 00:05:28.754
So that's iteration.

93
00:05:28.754 --> 00:05:33.629
Recursion, one of those terms, I heard
somebody earlier talk about regular

94
00:05:33.629 --> 00:05:38.970
expressions, those are things that we
can kinda be sometimes frightened about.

95
00:05:38.970 --> 00:05:41.753
With recursion, it's similar.

96
00:05:41.753 --> 00:05:45.790
There's a few other topics in computer
science that people kind of feel like

97
00:05:45.790 --> 00:05:47.870
those are taboo or difficult.

98
00:05:47.870 --> 00:05:52.761
That closure is one of those that
many people feel like you have to

99
00:05:52.761 --> 00:05:58.360
have this special magical enlightenment
to be able to understand.

100
00:05:58.360 --> 00:06:00.517
We are gonna use recursion a lot.

101
00:06:00.517 --> 00:06:03.841
Depending on your level of
comfort with recursion,

102
00:06:03.841 --> 00:06:06.566
some of that may feel a bit uncomfortable.

103
00:06:06.566 --> 00:06:10.856
But you absolutely cannot make any
progress in this DSA topic and

104
00:06:10.856 --> 00:06:15.631
data structures and algorithms
without getting comfortable with it.

105
00:06:15.631 --> 00:06:19.149
So whereas somebody might be able to tell
you, you can get away with never using

106
00:06:19.149 --> 00:06:21.992
a regular expression,
I don't necessarily agree with that.

107
00:06:21.992 --> 00:06:26.408
But you might have somebody say, you don't
need a regular expression, there's other

108
00:06:26.408 --> 00:06:30.001
ways to solve those problems, or
at least, abstract those problems.

109
00:06:30.001 --> 00:06:35.292
While that might be true of regular
expressions, you can't get anywhere

110
00:06:35.292 --> 00:06:40.427
[LAUGH] in this topic without getting
real comfortable with recursion.

111
00:06:40.427 --> 00:06:42.930
So I think it's just one of those
bullets that we need to bite.

112
00:06:43.960 --> 00:06:46.209
We need to tackle it, so
we're gonna use it quite a bit.

113
00:06:47.260 --> 00:06:51.858
Put very simply, recursion means
that a function ends up calling

114
00:06:51.858 --> 00:06:54.711
itself either directly or indirectly.

115
00:06:54.711 --> 00:06:57.345
So a cycle of function calls where, well,

116
00:06:57.345 --> 00:07:02.100
while the function is still running,
another indication on the call stack,

117
00:07:02.100 --> 00:07:04.968
there's that word stack
that we referred to.

118
00:07:04.968 --> 00:07:08.715
So you think about a function call, and
then when it calls another function while

119
00:07:08.715 --> 00:07:12.146
it's still running, then it adds to
what we call the call stack, right?

120
00:07:12.146 --> 00:07:14.160
So you have these functions
that call each other.

121
00:07:14.160 --> 00:07:17.152
That's not the same thing as calling
a function, it finishing, and

122
00:07:17.152 --> 00:07:19.790
then calling another function and
it finishing.

123
00:07:19.790 --> 00:07:24.489
It's call function a, who then,
while it's running, invokes function b,

124
00:07:24.489 --> 00:07:27.351
and while it's running,
invokes function c.

125
00:07:27.351 --> 00:07:30.350
That's what we mean by the call stack,
that's stacking up.

126
00:07:30.350 --> 00:07:33.724
And if a calls b and b calls a,

127
00:07:33.724 --> 00:07:39.796
then you would have a stack,
you'd have a, b, a.

128
00:07:39.796 --> 00:07:45.555
Or if a calls b, b call c, and
c calls a, now you have a cycle.

129
00:07:45.555 --> 00:07:47.781
Either one of those is
what we call recursion.

130
00:07:47.781 --> 00:07:52.412
And you might be thinking, why on earth
would you have a calling a or a calling b,

131
00:07:52.412 --> 00:07:55.470
or why would you have direct or
indirect recursion?

132
00:07:55.470 --> 00:07:57.690
Why would you allow that
sort of thing to happen?

133
00:07:58.690 --> 00:08:04.506
The answer is that the call
stack is an abstraction

134
00:08:04.506 --> 00:08:10.020
over iteration with state management.

135
00:08:10.020 --> 00:08:12.483
Every time we invoke a new function,

136
00:08:12.483 --> 00:08:17.900
we get a new set of state that is
encapsulated in that function call.

137
00:08:17.900 --> 00:08:21.906
And therefore, we have this stack where
we're doing things, we're stacking

138
00:08:21.906 --> 00:08:26.510
up work, a, b, c, d, however deep it goes,
could go 10,000 deep, right?

139
00:08:26.510 --> 00:08:30.160
Each one of those individual
function calls has its own state.

140
00:08:30.160 --> 00:08:34.815
And what we're asking the engine to do,
the JavaScript engine, in this case,

141
00:08:34.815 --> 00:08:39.530
is manage that state for us in a stack,
where it pushes it on and pops it off.

142
00:08:39.530 --> 00:08:42.592
The entire world of that function
gets pushed onto the stack, and

143
00:08:42.592 --> 00:08:44.272
then when it's done, it pops off.

144
00:08:44.272 --> 00:08:46.780
I don't wanna worry
about all of that stuff.

145
00:08:46.780 --> 00:08:48.970
Well, what if I did worry about it?

146
00:08:48.970 --> 00:08:52.115
I absolutely could take any
recursive algorithm and

147
00:08:52.115 --> 00:08:55.860
implement it with an iteration,
with a while loop.

148
00:08:55.860 --> 00:08:59.083
And what I would have to do is take
every bit of state that I need for

149
00:08:59.083 --> 00:09:01.790
each one of those levels and
manage it in my own stack.

150
00:09:02.880 --> 00:09:03.690
Can I do that?

151
00:09:03.690 --> 00:09:08.321
Yes, sometimes I do do that, sometimes
I do implement something that we would

152
00:09:08.321 --> 00:09:12.688
think of recursively, you can implement
it iteratively and vice versa.

153
00:09:12.688 --> 00:09:15.701
They are what we call in
a formal mathematical term,

154
00:09:15.701 --> 00:09:18.290
they're isomorphic of each other.

155
00:09:18.290 --> 00:09:21.978
Any recursion can be expressed
iteratively, and in fact,

156
00:09:21.978 --> 00:09:26.050
most compilers for most languages
unroll these sorts of things.

157
00:09:26.050 --> 00:09:30.623
They will unroll recursion and
make it a loop or sometimes vice versa,

158
00:09:30.623 --> 00:09:34.425
because the same is also true or
generally true.

159
00:09:34.425 --> 00:09:37.253
So you have to be real
familiar with iteration.

160
00:09:37.253 --> 00:09:39.986
And most of us that have done any
sort of imperative programming,

161
00:09:39.986 --> 00:09:42.530
we're familiar with for loops and
while loops and so forth.

162
00:09:42.530 --> 00:09:46.968
Maybe less familiar with recursion, but
you just need to understand that that's

163
00:09:46.968 --> 00:09:49.424
asking the engine to do our iteration for
us.

164
00:09:49.424 --> 00:09:53.704
It's an abstraction over iteration,
that says, I'm gonna need this state to be

165
00:09:53.704 --> 00:09:57.380
separate for each of these iterations,
and I want you to manage that.

166
00:09:57.380 --> 00:10:00.910
I'm gonna do that through the call stack
rather than through my own data structure.

167
00:10:02.820 --> 00:10:08.371
In fact, we have examples of places in
modern framework world where the call

168
00:10:08.371 --> 00:10:13.834
stack that, as we think of as being
a built-in thing that we just invoke and

169
00:10:13.834 --> 00:10:19.931
use, turns out the call stack isn't quite
malleable enough for some frameworks.

170
00:10:19.931 --> 00:10:25.328
And they have re-implemented the entire
concept of a call stack in the framework,

171
00:10:25.328 --> 00:10:27.112
so that they have control.

172
00:10:27.112 --> 00:10:31.751
I'm referring to react and fibers and
non-local continuations and stuff,

173
00:10:31.751 --> 00:10:33.901
but you can get real deep with this.

174
00:10:33.901 --> 00:10:38.150
So anything that you think of that's built
in, you could re-implement yourself.

175
00:10:38.150 --> 00:10:41.846
You could implement memory
management yourself if you wanted.

176
00:10:41.846 --> 00:10:42.745
You shouldn't, but you could, all right?

177
00:10:42.745 --> 00:10:47.870
So recursion is just a dual of or
isomorphic of the iteration.

178
00:10:47.870 --> 00:10:51.100
And sometimes it's more convenient to
invoke recursion than it is to do it

179
00:10:51.100 --> 00:10:52.200
with iteration.

180
00:10:52.200 --> 00:10:57.175
In fact, we'll see an example
where we're going through a tree.

181
00:10:57.175 --> 00:11:01.278
And you typically think,
because trees are typically referred to as

182
00:11:01.278 --> 00:11:06.645
a recursively-defined data structure,
because every node is its own sub tree.

183
00:11:06.645 --> 00:11:09.805
That's what we mean by
recursively-defined data structure.

184
00:11:09.805 --> 00:11:12.696
Because trees are recursive
data structures,

185
00:11:12.696 --> 00:11:17.235
recursion is a very natural
operation to use to traverse them.

186
00:11:17.235 --> 00:11:18.444
But that's not always true.

187
00:11:18.444 --> 00:11:22.820
The breadth-first traversal, again,
we'll come back to this later, but

188
00:11:22.820 --> 00:11:27.198
the breadth-first traversal of a tree
is much more complicated to represent

189
00:11:27.198 --> 00:11:28.740
recursively.

190
00:11:28.740 --> 00:11:31.199
In fact, most people don't
implement it recursively.

191
00:11:31.199 --> 00:11:33.170
You could, but most people don't.

192
00:11:33.170 --> 00:11:38.161
They implement it with
an iterative loop and a queue.

193
00:11:38.161 --> 00:11:42.305
An interval loop plus a queue is
a breadth-first traversal of a tree.

194
00:11:42.305 --> 00:11:45.438
We'll come back to some of that stuff,
so if that feels unfamiliar to you,

195
00:11:45.438 --> 00:11:47.200
don't worry, we're coming back to it.

196
00:11:47.200 --> 00:11:51.432
I'm just giving you a sense of how
these pieces can fit together.

197
00:11:51.432 --> 00:11:56.270
Okay, the last thing I have listed
up here is indexes and references.

198
00:11:56.270 --> 00:12:00.893
I've really come up with a good term
to describe what I mean here, but

199
00:12:00.893 --> 00:12:03.778
I'm using indexes in the database sense.

200
00:12:03.778 --> 00:12:06.046
When I talk about this
concept of an index,

201
00:12:06.046 --> 00:12:10.086
you might have a name data structure,
let's say, for example, an array.

202
00:12:10.086 --> 00:12:13.423
It's a sequentially-ordered
collection of values,

203
00:12:13.423 --> 00:12:18.670
doesn't matter what the value types are,
just sequentially-ordered.

204
00:12:18.670 --> 00:12:23.785
But you might want a separate data
structure alongside that array

205
00:12:23.785 --> 00:12:29.990
that tells you something about some or
all of the elements in there.

206
00:12:29.990 --> 00:12:30.910
Here's an example.

207
00:12:30.910 --> 00:12:33.310
I wanna know all of
the elements in the array.

208
00:12:33.310 --> 00:12:38.138
I need quick access to all of the elements
in the array whose value starts with

209
00:12:38.138 --> 00:12:39.049
the letter a.

210
00:12:39.049 --> 00:12:41.240
And that's literally
like what databases do.

211
00:12:41.240 --> 00:12:44.620
When they create indexes, they have
this big old table of all your data.

212
00:12:44.620 --> 00:12:47.413
And then they create these
separate data structures that say,

213
00:12:47.413 --> 00:12:49.993
if you're gonna look for
all the things that start with a,

214
00:12:49.993 --> 00:12:52.785
if you're gonna look for
all the IDs that are even or whatever,

215
00:12:52.785 --> 00:12:55.990
we maintain this separate list,
this separate data structure.

216
00:12:55.990 --> 00:13:00.839
That's not a copy of the data, its
pointers, its references into the data.

217
00:13:00.839 --> 00:13:05.946
This is extremely common in data
structure algorithm creation, is that we

218
00:13:05.946 --> 00:13:11.133
construct the main data structure,
and rather than duplicating the data,

219
00:13:11.133 --> 00:13:16.826
we construct these other data structures
that have references into these places.

220
00:13:16.826 --> 00:13:23.297
And you can access it sequentially and
by name, if you will.

221
00:13:23.297 --> 00:13:26.705
Those are generally at odds, but
we can have two parallel data structures,

222
00:13:26.705 --> 00:13:28.600
one with references to the other.

223
00:13:28.600 --> 00:13:31.640
That's what I mean here,
that's another extremely common technique.

224
00:13:31.640 --> 00:13:35.690
So these are things that we
will see play out for us.

