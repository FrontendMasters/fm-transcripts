WEBVTT

1
00:00:00.000 --> 00:00:04.063
The first thing that we need to
do is load up this libraries.

2
00:00:04.063 --> 00:00:06.584
You can copy this from the option for

3
00:00:06.584 --> 00:00:11.728
branch in the same way that we copied
in the object pool if you'd like to.

4
00:00:11.728 --> 00:00:16.341
This one unfortunately doesn't get
distributed as an ES module, so

5
00:00:16.341 --> 00:00:22.883
we're just gonna load it in
the old school, Script tag way.

6
00:00:26.769 --> 00:00:30.374
But you do need to make
sure that it is loaded.

7
00:00:35.837 --> 00:00:44.162
In our wordie,
we're going to take out the isWord and

8
00:00:44.162 --> 00:00:49.167
we're gonna take out the pool.

9
00:00:49.167 --> 00:00:52.802
And our dictionary is going to
be a minimal word graph, so

10
00:00:52.802 --> 00:00:56.613
we're gonna construct
an instance of MinimalWordGraph.

11
00:01:06.697 --> 00:01:08.087
Our loadWords function,

12
00:01:08.087 --> 00:01:12.003
we're gonna be able to take out all of
this code because the data structure

13
00:01:12.003 --> 00:01:15.742
is gonna take care of all the work
of constructing our data structure.

14
00:01:15.742 --> 00:01:16.308
So really,

15
00:01:16.308 --> 00:01:19.822
all we need to do is double check to
make sure we don't need to empty it out.

16
00:01:19.822 --> 00:01:23.900
So we're gonna say if the dict.size() &gt; 0,

17
00:01:23.900 --> 00:01:28.379
then recreate an instance
of our MinimalWordGraph.

18
00:01:33.122 --> 00:01:37.645
And we're going to call for
let word of wordList.

19
00:01:37.645 --> 00:01:39.438
We're gonna go through each of the words.

20
00:01:39.438 --> 00:01:44.791
And this is the nice part is that we're
just gonna call their method dict.add and

21
00:01:44.791 --> 00:01:45.979
pass in the word.

22
00:01:48.013 --> 00:01:49.570
So that data structure and

23
00:01:49.570 --> 00:01:53.831
their API is taking care of all that
complexity under the covers for us.

24
00:01:56.990 --> 00:02:01.280
There's one final step that we wanna do,
and this is basically the step where it

25
00:02:01.280 --> 00:02:04.305
does all that compaction and
memory efficiency stuff.

26
00:02:04.305 --> 00:02:08.886
Gets rid of all the intermediary steps,
they called it makeImmutable,

27
00:02:08.886 --> 00:02:13.932
which is actually one of my favorite API
nicknames I've ever seen in a library.

28
00:02:13.932 --> 00:02:16.813
It's just very straightforward
what it's doing,

29
00:02:16.813 --> 00:02:19.047
it's making the word graph immutable.

30
00:02:19.047 --> 00:02:22.196
What we should take from that,
importantly,

31
00:02:22.196 --> 00:02:25.668
is that once we have
constructed this word graph and

32
00:02:25.668 --> 00:02:30.513
done all of the compression to get
the nice, efficient memory usage and

33
00:02:30.513 --> 00:02:34.499
all that, we cannot add any
more words to the dictionary.

34
00:02:34.499 --> 00:02:38.424
In our case, that's no big deal because
we pre-loaded the whole dictionary.

35
00:02:38.424 --> 00:02:43.430
There might be cases where you need to
be able to add words to the dictionary

36
00:02:43.430 --> 00:02:48.701
as things go along, in which case this
is not the right data structure to use.

37
00:02:48.701 --> 00:02:53.393
Cuz this data structure really gets its
benefit from the ability to have compacted

38
00:02:53.393 --> 00:02:55.716
it, but that's a one-way operation.

39
00:02:55.716 --> 00:02:58.793
Once you've done all that compaction,

40
00:02:58.793 --> 00:03:03.778
you can no longer add stuff to
the graph cuz it just breaks things.

41
00:03:03.778 --> 00:03:08.532
So for us, that's good,

42
00:03:08.532 --> 00:03:15.016
we go ahead and make it immutable and

43
00:03:15.016 --> 00:03:19.998
we return the dict.size.

44
00:03:24.617 --> 00:03:30.826
We are going to, Yes, question.

45
00:03:30.826 --> 00:03:35.933
&gt;&gt; How does it reduce the memory usage
while the same nodes still exist?

46
00:03:35.933 --> 00:03:41.419
&gt;&gt; It does not create all of the same
nodes, it removes any unnecessary nodes.

47
00:03:41.419 --> 00:03:46.131
And the way that it removes the nodes
is not to actually combine nodes, but

48
00:03:46.131 --> 00:03:48.032
to create additional links.

49
00:03:48.032 --> 00:03:51.816
That's what makes it
a graph versus a trie tree.

50
00:03:51.816 --> 00:03:57.294
But it does actually literally
have far fewer nodes in memory.

51
00:03:57.294 --> 00:04:00.529
We will actually see that once
we run our working solution,

52
00:04:00.529 --> 00:04:04.603
we'll actually be able to see the number
of entries in the data structure is

53
00:04:04.603 --> 00:04:07.201
drastically lower than in the trie tree,
yes?

54
00:04:07.201 --> 00:04:10.127
&gt;&gt; As an aside, this is where I work, and

55
00:04:10.127 --> 00:04:16.079
the typical graph in production has
millions of nodes and billions of edges.

56
00:04:16.079 --> 00:04:20.149
So you are going to have a lot
more links than entities in

57
00:04:20.149 --> 00:04:24.321
general because it's a much
more efficient strategy.

58
00:04:24.321 --> 00:04:29.911
&gt;&gt; And references are a pretty small value
for JavaScript engines to keep track of.

59
00:04:29.911 --> 00:04:32.923
Is it creating more memory to
keep all those references?

60
00:04:32.923 --> 00:04:38.263
Yes, but it's way less than creating
a big old node of string value in it or

61
00:04:38.263 --> 00:04:39.912
something like that.

62
00:04:39.912 --> 00:04:41.610
References are pretty small.

63
00:04:41.610 --> 00:04:44.140
We're gonna call the API
that they provide, and

64
00:04:44.140 --> 00:04:47.174
they actually have several of
these you can read through.

65
00:04:47.174 --> 00:04:51.987
Fair warning, the documentation for this
particular library is not as great as I

66
00:04:51.987 --> 00:04:56.726
wish it would be, and it doesn't seem to
be a super well maintained library, so

67
00:04:56.726 --> 00:04:58.953
your mileage may vary with using it.

68
00:04:58.953 --> 00:05:01.740
But I was able to get this to work.

69
00:05:01.740 --> 00:05:05.201
If they don't say this
in the documentation,

70
00:05:05.201 --> 00:05:08.049
you absolutely have to pass an array.

71
00:05:08.049 --> 00:05:11.714
Implies that if you pass a string,
everything's fine, and I can tell you,

72
00:05:11.714 --> 00:05:12.987
you have to pass an array.

73
00:05:12.987 --> 00:05:16.349
[LAUGH] That I know for sure.

74
00:05:16.349 --> 00:05:21.476
So here I'm just making sure that
we are in fact passing in an array

75
00:05:21.476 --> 00:05:26.977
if we've received a string,
then I'm just turning it into a string,

76
00:05:26.977 --> 00:05:30.540
I mean,
turning it into an array with split.

77
00:05:33.440 --> 00:05:36.310
So containsOnly is
the method name on that data

78
00:05:36.310 --> 00:05:40.663
structure that basically does
the search that we're talking about.

79
00:05:40.663 --> 00:05:43.972
That's our findAllWords functionality,
basically.

80
00:05:43.972 --> 00:05:46.107
So we're gonna be able to
take away findAllWords.

81
00:05:54.298 --> 00:05:59.352
We still have the same problem, their data
structure still has the same problem,

82
00:05:59.352 --> 00:06:03.759
which is that they might end up
returning as duplicates if we passed in,

83
00:06:08.337 --> 00:06:12.513
Duplicate input letters, they might end
up returning as duplicate output letters.

84
00:06:12.513 --> 00:06:18.423
So in the code repo,
you'll see this line of code.

85
00:06:18.423 --> 00:06:24.101
Somebody asked earlier,
could we just deduplicate

86
00:06:24.101 --> 00:06:28.754
the input letters before passing them in?

87
00:06:28.754 --> 00:06:32.742
In our previous algorithm,
that would not work.

88
00:06:32.742 --> 00:06:37.706
But something that's interesting about
the way that containsOnly works is that

89
00:06:37.706 --> 00:06:41.283
it does not care, it will find
all of the matches even if you

90
00:06:41.283 --> 00:06:43.920
didn't provide enough of those letters.

91
00:06:43.920 --> 00:06:46.759
So technically,
we don't actually need line 35,

92
00:06:46.759 --> 00:06:48.472
this is the way it is in the repo.

93
00:06:48.472 --> 00:06:55.911
Technically, we actually could have solved
line 35 by deduplicating what's in input.

94
00:06:55.911 --> 00:07:00.783
So you could do it that way,
pass in a deduplicated set of input

95
00:07:00.783 --> 00:07:05.846
into containsOnly, and
then you won't get those repeats out,

96
00:07:05.846 --> 00:07:09.058
and then line 36 wouldn't be needed.

97
00:07:09.058 --> 00:07:15.061
There is, however, whether you deduplicate
the input or you deduplicate the output,

98
00:07:15.061 --> 00:07:20.012
there is, however, another annoyance
that we have, which is that they

99
00:07:20.012 --> 00:07:25.065
were matching words that we said at
the beginning we don't want to allow.

100
00:07:25.065 --> 00:07:29.328
In other words, they would have matched
appear because they allow that duplication

101
00:07:29.328 --> 00:07:30.299
of those letters.

102
00:07:30.299 --> 00:07:34.706
So it's both a good and a bad thing that
their search algorithm works that way.

103
00:07:34.706 --> 00:07:39.676
So now we are going to be returning
words that our previous algorithm would

104
00:07:39.676 --> 00:07:43.364
not have returned,
because we would allow a single p as

105
00:07:43.364 --> 00:07:47.635
an input to match the word appear
even though it needs two p's.

106
00:07:47.635 --> 00:07:52.254
So to fix that up so that this code
works the same as the code before,

107
00:07:52.254 --> 00:07:57.122
you could make a decision at this
point that I'm using a data structure

108
00:07:57.122 --> 00:08:01.677
that allows the repeats,
maybe that's how I want my app to work.

109
00:08:01.677 --> 00:08:05.853
But just so that we are comparing
Apples to Apples, we're gonna show,

110
00:08:05.853 --> 00:08:08.914
we kind of hinted at this,
but we're gonna show a way

111
00:08:08.914 --> 00:08:12.896
of ensuring that we don't end up
with any of those false positives.

112
00:08:12.896 --> 00:08:18.498
And that is that we're gonna implement
a utility called countLetters, okay?

113
00:08:18.498 --> 00:08:22.086
And this is obviously a very down and
dirty way of doing it,

114
00:08:22.086 --> 00:08:24.357
but we wanna go through any string and

115
00:08:24.357 --> 00:08:28.478
count how many occurrences of each
letter there are in the string.

116
00:08:28.478 --> 00:08:32.899
In fact,
this is a classic interview question.

117
00:08:32.899 --> 00:08:38.747
And I'm only showing one very down and
dirty implementation that you could do for

118
00:08:38.747 --> 00:08:41.931
counting the number of
letters in a string.

119
00:08:41.931 --> 00:08:46.330
So we're gonna go through each
character in the string, and

120
00:08:46.330 --> 00:08:51.951
we're going to create a reference in
this counts object for that character.

121
00:08:51.951 --> 00:08:56.944
So we'll say str[i], And

122
00:08:56.944 --> 00:08:59.587
then we're gonna say,

123
00:09:03.024 --> 00:09:09.927
We're gonna set it equal to either,
I'm just

124
00:09:09.927 --> 00:09:13.899
trying to write it the way I have it in
the repo so that the code compares here.

125
00:09:13.899 --> 00:09:19.035
This is just a way of code golfing,
writing fewer lines of code.

126
00:09:19.035 --> 00:09:22.148
What I'm basically saying is,
if it's there, use it.

127
00:09:22.148 --> 00:09:25.765
Otherwise, default to 0, and
then add 1 to the count.

128
00:09:25.765 --> 00:09:28.104
So I'm incrementing the count, but

129
00:09:28.104 --> 00:09:32.265
I'm defaulting it to 0 if it
hasn't already been said in there.

130
00:09:33.585 --> 00:09:35.132
And then return counts.

131
00:09:35.132 --> 00:09:41.738
So for the word appear,
it would return as an object that had a1,

132
00:09:41.738 --> 00:09:46.080
p2, I'm sorry, a2, p2, e1, r1.

133
00:09:46.080 --> 00:09:46.773
Everybody follow that?

134
00:09:46.773 --> 00:09:48.277
That's what this function does.

135
00:09:53.778 --> 00:09:57.934
We're going to need those counts for
our input.

136
00:09:57.934 --> 00:10:02.168
So I'm gonna come back here and
get those at the beginning.

137
00:10:02.168 --> 00:10:08.944
I'm gonna say inputCounts
= countLetters(input).

138
00:10:11.701 --> 00:10:15.700
If you did the deduplicating of
input rather than the set approach,

139
00:10:15.700 --> 00:10:19.207
whichever way,
if you did the deduplicating of the input,

140
00:10:19.207 --> 00:10:22.953
you would wanna do that after
you counted the letters, right?

141
00:10:22.953 --> 00:10:26.641
So do that deduplication here on line 38.

142
00:10:26.641 --> 00:10:31.971
Okay, so to fix up our result set so that
it filters out the words that we should

143
00:10:31.971 --> 00:10:36.891
not have allowed in there,
that their search should not have allowed,

144
00:10:36.891 --> 00:10:40.349
we're just gonna do a plain
old filter function.

145
00:10:40.349 --> 00:10:45.740
Again, just trying to kind of quickly

146
00:10:45.740 --> 00:10:50.122
sketch this kind of logic out.

147
00:10:53.077 --> 00:10:57.305
For each word in the return set,

148
00:10:57.305 --> 00:11:01.226
we're gonna get its counts.

149
00:11:03.416 --> 00:11:06.659
And then we're gonna go through
all of those letters and

150
00:11:06.659 --> 00:11:10.661
compare the count to what's returned
to the counts of our input, and

151
00:11:10.661 --> 00:11:13.702
if they don't match,
then we're gonna reject it.

152
00:11:13.702 --> 00:11:19.893
So for, Take

153
00:11:19.893 --> 00:11:25.143
the letter and
the count of Object.entries.

154
00:11:28.189 --> 00:11:29.262
WordLetterCounts.

155
00:11:33.659 --> 00:11:38.739
So we're saying, oops, I misspelled that,
we're saying this object

156
00:11:38.739 --> 00:11:44.238
that got the counts for us from this
particular word in our candidate returns,

157
00:11:44.238 --> 00:11:47.847
we're gonna go through
each one of those entries.

158
00:11:47.847 --> 00:11:52.303
So we're gonna go through the a, the p,
the e, and the r entry in that object, for

159
00:11:52.303 --> 00:11:52.897
example.

160
00:11:52.897 --> 00:11:58.842
And we're going to compare the count
that was returned from this word,

161
00:11:58.842 --> 00:12:02.917
so p2,
to the count on our inputCounts object.

162
00:12:02.917 --> 00:12:06.377
So we're gonna say,
if there wasn't that letter,

163
00:12:06.377 --> 00:12:10.309
which this should never happen,
but this is just kind of a,

164
00:12:10.309 --> 00:12:15.202
if there wasn't that letter at all,
that's just sort of a sanity check.

165
00:12:15.202 --> 00:12:21.292
But it shouldn't ever happen that it gives
us back word with a letter that wasn't

166
00:12:21.292 --> 00:12:27.131
in the input set, or the count is greater
than what it was in our inputCounts.

167
00:12:30.145 --> 00:12:35.124
If either of those things is true,
I got one too many parentheses there.

168
00:12:35.124 --> 00:12:38.479
If either of those things is true,
we can reject this

169
00:12:38.479 --> 00:12:42.849
word because it should not be in our list,
so that's return false.

170
00:12:42.849 --> 00:12:47.949
And if we make it all the way
through all the letter counts and

171
00:12:47.949 --> 00:12:52.136
everything was fine,
we return true to keep it.

172
00:12:55.444 --> 00:12:56.915
Thumbs up, thumbs down, how do we feel?

173
00:12:56.915 --> 00:12:58.123
Does that sound all right?

174
00:13:00.759 --> 00:13:05.796
It's a little bit regrettable that their
API doesn't allow a flag that says,

175
00:13:05.796 --> 00:13:09.474
don't allow duplicate letters or
something like that.

176
00:13:09.474 --> 00:13:12.077
It'd be nice if their API did that.

177
00:13:12.077 --> 00:13:16.083
This particular case we just have to
clean up the results a little bit.

178
00:13:16.083 --> 00:13:22.680
So it's kind of a paper cut, but it's on
a much smaller set than the input set.

179
00:13:22.680 --> 00:13:30.703
So as algorithms, we're not too worried
about the performance hit there.

180
00:13:30.703 --> 00:13:36.763
If I did not make mistakes, if I was
able to successfully type that in,

181
00:13:36.763 --> 00:13:40.174
we should be able to go back to our page.

182
00:13:40.174 --> 00:13:47.432
Oop, I did make a mistake on line 30.

183
00:13:47.432 --> 00:13:52.548
Aha, this colon should
have been a semicolon.

184
00:13:55.223 --> 00:13:57.056
Another mistake, line 40.

185
00:14:00.960 --> 00:14:06.079
That's a question mark, sorry.

186
00:14:06.079 --> 00:14:08.374
I didn't even finish that line,
I got ahead of myself.

187
00:14:08.374 --> 00:14:12.184
Array.isArray we just return input and
then colon.

188
00:14:18.094 --> 00:14:24.066
Okay, right off the bat,
notice that our 2,400-word dictionary is

189
00:14:24.066 --> 00:14:31.390
now being represented by directed acyclic
word graph that only needs 1,562 entries.

190
00:14:31.390 --> 00:14:35.092
So we see it having fewer entries
than there are letters in there,

191
00:14:35.092 --> 00:14:38.740
and that's because it's able to
do all of that sharing, okay?

192
00:14:40.820 --> 00:14:44.334
If we go up to
the 370,000-word dictionary,

193
00:14:44.334 --> 00:14:49.567
you'll notice that it did take 810
milliseconds to create that instance and

194
00:14:49.567 --> 00:14:53.350
make it immutable,
do all the compaction on it.

195
00:14:53.350 --> 00:14:58.300
This is a pretty large dictionary as
dictionaries go, but that is a noticeable

196
00:14:58.300 --> 00:15:03.560
impact on startup performance, something
that you would have to think about.

197
00:15:03.560 --> 00:15:07.844
Can I do this in the background thread or
hide it somewhere or whatever, okay?

198
00:15:07.844 --> 00:15:11.517
So just be aware of it,
it's not 20 seconds or something crazy,

199
00:15:11.517 --> 00:15:15.404
that's completely impractical, but
it is something to be aware of.

200
00:15:15.404 --> 00:15:20.372
Notice here, though, instead of having the
1 million entries that we did in our trie

201
00:15:20.372 --> 00:15:22.522
tree, we now have 160,000.

202
00:15:22.522 --> 00:15:27.175
We have less than half the number
of words in our dictionary.

203
00:15:27.175 --> 00:15:32.473
So we went from being three or
four times as many to being

204
00:15:32.473 --> 00:15:37.787
less than half,
pretty significant memory savings.

205
00:15:37.787 --> 00:15:39.137
So again, trade-offs.

206
00:15:39.137 --> 00:15:45.775
Better memory, a little bit harsher
penalty on the startup time.

207
00:15:45.775 --> 00:15:49.422
Let's go back to
the 2,400-word dictionary.

208
00:15:49.422 --> 00:15:55.185
Let's try our examples again,
like the word America, six milliseconds.

209
00:15:55.185 --> 00:15:59.476
We were seeing around one millisecond
before, so slightly slower.

210
00:15:59.476 --> 00:16:05.044
We wanna make sure that we don't see
growth though, so let's try it with nine?

211
00:16:05.044 --> 00:16:05.963
Still pretty quick.

212
00:16:09.970 --> 00:16:11.002
Still very quick.

213
00:16:15.306 --> 00:16:25.278
Even something much longer, Right?

214
00:16:25.278 --> 00:16:29.211
So we see that it's still not
going in a runaway growth,

215
00:16:29.211 --> 00:16:32.150
still performing pretty much linearly.

216
00:16:32.150 --> 00:16:36.444
And that's what we know that we get
from those algorithms like the traversal

217
00:16:36.444 --> 00:16:40.556
of a trie tree, or in this case,
slightly more complex data structure.

218
00:16:40.556 --> 00:16:41.690
But under the covers,

219
00:16:41.690 --> 00:16:45.340
that containsOnly is fundamentally
doing a similar algorithm to ours.

