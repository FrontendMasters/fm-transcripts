WEBVTT

1
00:00:00.000 --> 00:00:04.403
The next set of optimization that we wanna
look at is we wanna look at this tree.

2
00:00:04.403 --> 00:00:08.684
Here's our trie tree implementation
that we've been working with.

3
00:00:08.684 --> 00:00:12.030
We wanna think about our
trie tree implementation and

4
00:00:12.030 --> 00:00:17.097
realize it does seem like there's a fair
bit of kind of wasted space in this tree.

5
00:00:17.097 --> 00:00:21.672
Look at all these extra nodes that are
being created here that aren't there for

6
00:00:21.672 --> 00:00:26.244
any other purpose other than that's the
structure of a trie tree is that it goes

7
00:00:26.244 --> 00:00:27.507
one letter at a time.

8
00:00:27.507 --> 00:00:31.813
In our particular dictionary case,
we're not a child on top of those, so

9
00:00:31.813 --> 00:00:33.693
those are kind of wasted nodes.

10
00:00:33.693 --> 00:00:38.632
You might think about it as, well,
I could construct the trie tree but

11
00:00:38.632 --> 00:00:43.670
then go back and compact that trie
tree and save a whole bunch of memory.

12
00:00:43.670 --> 00:00:45.669
Because I'm likely to have, on average,

13
00:00:45.669 --> 00:00:49.292
quite a few of these nodes that are just
hanging out with no reason to be there.

14
00:00:49.292 --> 00:00:52.042
They're intermediary nodes
with no reason to be there.

15
00:00:52.042 --> 00:00:54.785
It turns out there is an actual term for

16
00:00:54.785 --> 00:00:58.222
these kinds of trees
that have been compacted.

17
00:00:58.222 --> 00:01:02.416
A trie tree that has been compacted
is referred to as a radix tree.

18
00:01:02.416 --> 00:01:06.814
So the way the compaction worked as
you can see is that we just took

19
00:01:06.814 --> 00:01:10.805
the internal nodes,
combine them into one connection and

20
00:01:10.805 --> 00:01:16.274
then made that connection the augmentation
of all of the intermediate states.

21
00:01:16.274 --> 00:01:21.090
So that LAP, for example there,
LAP took the place of

22
00:01:21.090 --> 00:01:25.703
the nodes that were L,
A and P separately, right?

23
00:01:25.703 --> 00:01:29.893
So at each one of these levels now,
instead of only having single letters,

24
00:01:29.893 --> 00:01:34.433
we can also have multi-letter combinations
as the children, as the end point.

25
00:01:34.433 --> 00:01:38.966
You wouldn't construct your tree this
way cuz you couldn't possibly know in

26
00:01:38.966 --> 00:01:41.209
advance where those would need to be.

27
00:01:41.209 --> 00:01:46.434
So you would construct a trie tree
with single letters and then run

28
00:01:46.434 --> 00:01:52.717
an optimization step to effectively
compress the tree down into this format.

29
00:01:52.717 --> 00:01:57.116
So that would end up saving
potentially a significant

30
00:01:57.116 --> 00:02:00.546
amount of memory usage of your trie tree.

31
00:02:00.546 --> 00:02:04.419
In the case of doing
something like autocompletes,

32
00:02:04.419 --> 00:02:10.589
that ends up being a pretty effective
technique because you don't lose anything.

33
00:02:10.589 --> 00:02:12.460
You haven't lost any information.

34
00:02:12.460 --> 00:02:15.940
You still know the words,
you just do a lot less traversing.

35
00:02:15.940 --> 00:02:19.910
It's faster and uses a lot less memory.

36
00:02:19.910 --> 00:02:24.698
So most of the time, a production
implementation would construct a trie

37
00:02:24.698 --> 00:02:28.869
tree and then compress it into
something like this radix tree.

38
00:02:28.869 --> 00:02:32.289
Unfortunately for us,
the way our algorithm works,

39
00:02:32.289 --> 00:02:35.560
a radix tree would not be
the right data structure.

40
00:02:35.560 --> 00:02:38.488
It would significantly
complicate our algorithm and

41
00:02:38.488 --> 00:02:41.879
we would lose most of the performance
benefit that we wanted.

42
00:02:41.879 --> 00:02:44.622
So I only wanted to show you that
something like this exists and

43
00:02:44.622 --> 00:02:46.648
when you Google around, you'll find them.

44
00:02:46.648 --> 00:02:50.677
But in our particular exercise,
this is not the right data structure.

45
00:02:50.677 --> 00:02:55.413
And had we chosen this, we would have
ended up beating our head against why is

46
00:02:55.413 --> 00:02:59.930
this so hard, and why doesn't it work and
why is it not very performing?

47
00:02:59.930 --> 00:03:03.250
The reason why it would be difficult
is because at every level,

48
00:03:03.250 --> 00:03:06.949
we currently take advantage of the fact
that we can take a single letter

49
00:03:06.949 --> 00:03:10.857
out of our remaining letters and find
a child node or not find a child node.

50
00:03:10.857 --> 00:03:15.118
Here, we would actually need to enumerate
all of the children of every node to see

51
00:03:15.118 --> 00:03:16.569
if there were any matches.

52
00:03:16.569 --> 00:03:20.022
Cuz that would end up creating a whole
lot extra performance overhead and

53
00:03:20.022 --> 00:03:20.820
complication.

54
00:03:24.729 --> 00:03:28.891
However, there are other data
structures and we have now,

55
00:03:28.891 --> 00:03:34.146
without possibly even some of you
realizing it, we just made a giant leap.

56
00:03:34.146 --> 00:03:39.081
Actually, kind of a tiny leap
from tree data structures

57
00:03:39.081 --> 00:03:41.706
into graph data structures.

58
00:03:41.706 --> 00:03:44.635
Why is this a graph data structure?

59
00:03:44.635 --> 00:03:49.327
Well, you can easily see that
it's a graph data structure

60
00:03:49.327 --> 00:03:54.613
because you have one or
more nodes that have more than one parent.

61
00:03:54.613 --> 00:03:57.402
The H node has two parents.

62
00:03:57.402 --> 00:04:00.642
It has the C parent and
it has the A parent.

63
00:04:00.642 --> 00:04:05.352
That's what lets you know this is
a graph instead of a tree, okay?

64
00:04:05.352 --> 00:04:11.042
This is referred to as, actually,
it goes by a couple of different names.

65
00:04:11.042 --> 00:04:15.823
The first place that I found
it was under the name DAFSA,

66
00:04:15.823 --> 00:04:22.350
D-A-F-S-A which stands for
Directed Acyclic Finite State Automaton.

67
00:04:22.350 --> 00:04:27.490
So that's a whole bunch of really
fancy sounding computer science terms.

68
00:04:27.490 --> 00:04:30.923
Go back to your co-workers the next
time you're at your work and say,

69
00:04:30.923 --> 00:04:33.776
I learned about directed
acyclic finite state automata,

70
00:04:33.776 --> 00:04:36.351
and they'll probably buy you a free lunch,
right?

71
00:04:36.351 --> 00:04:38.880
Because that's gonna
sound really impressive.

72
00:04:38.880 --> 00:04:43.609
It's not quite as impressive as it sounds,
but it sounds good.

73
00:04:43.609 --> 00:04:47.696
The other way of referring
to these is DAWG,

74
00:04:47.696 --> 00:04:51.901
D-A-W-G, Directed Acyclic Word Graph.

75
00:04:51.901 --> 00:04:55.331
And I like that name a lot better cuz
that speaks to what we're creating here.

76
00:04:55.331 --> 00:04:58.321
We're creating a word graph.

77
00:04:58.321 --> 00:05:05.191
This Directed Acyclic Word Graph is,
you see how it looks a lot smaller?

78
00:05:05.191 --> 00:05:09.363
It's doing the kind of compaction
that a radix tree does, but

79
00:05:09.363 --> 00:05:11.381
it does it in a different way.

80
00:05:11.381 --> 00:05:17.331
Instead of removing nodes, it creates
these extra graph links within the tree.

81
00:05:17.331 --> 00:05:20.206
So we get the compaction of memory usage,

82
00:05:20.206 --> 00:05:25.054
but we still get the properties that
we're going to be able to traverse

83
00:05:25.054 --> 00:05:29.679
this in a similar way to the way
that we were traversing a trie tree.

84
00:05:29.679 --> 00:05:33.543
It's not exactly the same cuz there are
complexities in graphs, you have to worry

85
00:05:33.543 --> 00:05:37.017
about getting into loops and, I mean,
into cycles and things like that.

86
00:05:37.017 --> 00:05:42.317
But it ends up significantly reducing the
memory footprint of the data structure,

87
00:05:42.317 --> 00:05:44.371
and we'll see that in a moment.

88
00:05:44.371 --> 00:05:51.411
And it ends up creating a pretty
significantly performance implementation.

89
00:05:51.411 --> 00:05:54.334
Maybe not quite as nice as the trie tree,
but

90
00:05:54.334 --> 00:05:58.380
a very performance of limitation and
much better in memory.

91
00:05:58.380 --> 00:06:02.570
So it ends up creating for us
a potentially a better balance than what

92
00:06:02.570 --> 00:06:07.215
we saw a few iterations back with
the trie tree implementation that we did.

93
00:06:07.215 --> 00:06:11.947
This one shifts the balance a little
bit and makes better usage of memory.

94
00:06:11.947 --> 00:06:13.939
It pays a little bit more up front.

95
00:06:13.939 --> 00:06:19.338
A directed acyclic word graph is a much
more complex data structure to create.

96
00:06:19.338 --> 00:06:23.243
And we are not going to learn
the algorithms of compaction that end up

97
00:06:23.243 --> 00:06:24.826
creating all these links.

98
00:06:24.826 --> 00:06:28.114
Those algorithms are significantly
out of scope for our discussion.

99
00:06:28.114 --> 00:06:30.932
And also,
even if I taught them to you, it's so

100
00:06:30.932 --> 00:06:34.885
complex that I would never recommend
writing this code yourself.

101
00:06:34.885 --> 00:06:39.908
We have firmly crossed into the area
where you wanna use a known and

102
00:06:39.908 --> 00:06:42.750
battle-tested library to do this.

103
00:06:42.750 --> 00:06:44.208
And I have included for

104
00:06:44.208 --> 00:06:48.980
you an implementation of this from
a library called Minimal Word Graphs.

105
00:06:48.980 --> 00:06:51.032
That's a library that I found out there,
and

106
00:06:51.032 --> 00:06:54.655
I've included an implementation of that,
and that's what we're gonna use.

