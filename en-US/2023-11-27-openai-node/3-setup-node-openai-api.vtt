WEBVTT

1
00:00:00.047 --> 00:00:03.294
Okay, enough with the talking,
let's write some code, right?

2
00:00:03.294 --> 00:00:07.396
Okay, so like I said, we need node,
you need your API account.

3
00:00:07.396 --> 00:00:09.301
Let's do some Hello world stuff.

4
00:00:09.301 --> 00:00:11.074
So let's just hop right in.

5
00:00:11.074 --> 00:00:14.892
So like I said, I already have the repo
here, I'm just gonna make another one.

6
00:00:18.809 --> 00:00:27.568
Mkdir ai = course, And
I'll make this bigger.

7
00:00:27.568 --> 00:00:29.654
So I got enough.

8
00:00:29.654 --> 00:00:34.137
Cool, okay, start project from scratch.

9
00:00:34.137 --> 00:00:36.200
So I'm gonna use my npm here.

10
00:00:36.200 --> 00:00:38.632
And that that, there we go.

11
00:00:38.632 --> 00:00:44.107
And then the one thing that you wanna
install is gonna be openai, like that.

12
00:00:44.107 --> 00:00:45.481
So make sure you install that.

13
00:00:47.522 --> 00:00:53.399
And for my editor, I'm using Cursor.

14
00:00:53.399 --> 00:00:55.571
Anyone ever use Cursor before?

15
00:00:55.571 --> 00:00:58.396
No, okay, so if you use VS Code,
it's the same thing,

16
00:00:58.396 --> 00:01:00.332
it's actually just a fork of VS Code.

17
00:01:00.332 --> 00:01:04.235
It's the exact same thing, all your
plugins work, your keyboard shortcuts,

18
00:01:04.235 --> 00:01:05.236
everything works.

19
00:01:05.236 --> 00:01:11.014
The only difference about Cursor is that
it has AI features built into it that are,

20
00:01:11.014 --> 00:01:16.140
in my opinion, way better than Copilot,
it's a million times better.

21
00:01:16.140 --> 00:01:19.426
And I think they chose a fork cuz it
just makes them feel more official, but

22
00:01:19.426 --> 00:01:22.456
they could have made a plugin too,
and it would have been just fine.

23
00:01:22.456 --> 00:01:24.584
I might use some of those today,
maybe not.

24
00:01:24.584 --> 00:01:27.201
Just an example, but yeah, I use Cursor.

25
00:01:27.201 --> 00:01:29.206
If you're wondering like,
what the hell is this?

26
00:01:29.206 --> 00:01:31.380
Yeah, it's just an editor, it's free.

27
00:01:31.380 --> 00:01:36.049
You can add your own GPT key to it and not
pay them anything, or you can pay them and

28
00:01:36.049 --> 00:01:40.195
they'll use their GPT version for
you, but that's what I'm using.

29
00:01:40.195 --> 00:01:44.165
If you're wondering why I have this thing,
that's what this is.

30
00:01:44.165 --> 00:01:48.961
Let's make a new file here,
I'll just call it index.js.

31
00:01:48.961 --> 00:01:51.116
Also wanna make sure I'm using modules,

32
00:01:51.116 --> 00:01:55.505
so I'm gonna go to my package.json and add
a field here called type and put modules.

33
00:01:55.505 --> 00:01:59.249
That way, I can use ES6 imports and
exports, and

34
00:01:59.249 --> 00:02:02.816
not have to use require and
module.exports and

35
00:02:02.816 --> 00:02:08.452
stuff like that cuz I'm not going back
to that, [LAUGH] I'm done with that.

36
00:02:08.452 --> 00:02:11.939
No more require, I'm over it.

37
00:02:11.939 --> 00:02:14.071
If I was using Bun,
I would just use TypeScript.

38
00:02:14.071 --> 00:02:18.497
So, cool, now let's make something.

39
00:02:18.497 --> 00:02:22.992
Okay, so the first thing we wanna do,
we installed openai, so let's import that.

40
00:02:22.992 --> 00:02:30.115
So we can say, we were to say
OpenAI from openai, like this.

41
00:02:30.115 --> 00:02:32.756
Actually, I'm gonna
make a prettier file so

42
00:02:32.756 --> 00:02:38.157
I can get my, Get my linting and my fix.

43
00:02:38.157 --> 00:02:42.597
I don't like semicolons, so
I always put that on false, and

44
00:02:42.597 --> 00:02:46.352
I only like single quotes,
I'll put that on true.

45
00:02:46.352 --> 00:02:50.716
It really bothers me, so there we go.

46
00:02:50.716 --> 00:02:54.876
So we got OpenAI, and then what we wanna
do is we just wanna make a new instance.

47
00:02:54.876 --> 00:03:01.530
So I'll say openai = new OpenAI,
like this.

48
00:03:01.530 --> 00:03:04.351
And right here,
this is ready to parse in your API key.

49
00:03:04.351 --> 00:03:07.499
So you can pass in your API key like this.

50
00:03:07.499 --> 00:03:11.626
You can also just expose it as an
environment variable with a specific name,

51
00:03:11.626 --> 00:03:13.249
it will pick it up from there.

52
00:03:13.249 --> 00:03:15.869
&gt;&gt; That's actually the way that
I'm gonna do it cuz we're gonna be

53
00:03:15.869 --> 00:03:18.698
using this a lot and I just don't
wanna be writing it a million times.

54
00:03:18.698 --> 00:03:23.297
So I'm just gonna make a .env file,
like this.

55
00:03:23.297 --> 00:03:29.609
And I'm gonna say OPENAI_API_KEY=,

56
00:03:29.609 --> 00:03:35.747
and then I'm gonna go get my OpenAI key.

57
00:03:35.747 --> 00:03:41.152
So if you're following along,
you might feel the urge to copy my

58
00:03:41.152 --> 00:03:47.173
API key that I'm about to get, but
I'm gonna remove it, eventually.

59
00:03:47.173 --> 00:03:49.156
So, good luck.

60
00:03:49.156 --> 00:03:50.163
So here we go.

61
00:03:50.163 --> 00:03:54.940
I'm just gonna call this demo-key,
create myself a key.

62
00:03:54.940 --> 00:03:58.007
I'm gonna copy it, and
I'm just gonna paste it in here.

63
00:04:03.713 --> 00:04:06.356
Okay, got my open API key, and

64
00:04:06.356 --> 00:04:12.362
I just need to load up the environment
variables into my environment.

65
00:04:12.362 --> 00:04:15.949
And the best way to do that is just
to install something called .env.

66
00:04:15.949 --> 00:04:23.123
So npm i dotenv, one word,
all lowercase, dotenv, like that.

67
00:04:23.123 --> 00:04:27.939
I'll install that, and
at the top of my file here,

68
00:04:27.939 --> 00:04:31.414
I'll just import dotenv/config.

69
00:04:31.414 --> 00:04:35.270
And what that's gonna do is it's gonna
find a dotenv file in this repo,

70
00:04:35.270 --> 00:04:39.769
take all the environment variables out of
it, load them up in the environment, and

71
00:04:39.769 --> 00:04:41.397
then move on to the next line.

72
00:04:41.397 --> 00:04:42.720
So by the time my code runs,

73
00:04:42.720 --> 00:04:46.245
the environment variables have
already been added to the environment.

74
00:04:46.245 --> 00:04:47.444
So I don't have to do that manually.

75
00:04:49.721 --> 00:04:51.207
Okay, there we go.

76
00:04:51.207 --> 00:04:53.427
And we can just not even pass
an object there because, again,

77
00:04:53.427 --> 00:04:55.274
this is gonna read from
the environment variable.

78
00:04:57.537 --> 00:05:01.889
All right, so let's actually just try to,
I don't know, talk to the AI and

79
00:05:01.889 --> 00:05:02.978
see what happens.

80
00:05:02.978 --> 00:05:07.230
So the way we can do that is we can say

81
00:05:07.230 --> 00:05:12.808
openai.chat.completions.create, and

82
00:05:12.808 --> 00:05:15.605
this takes an object.

83
00:05:18.164 --> 00:05:21.288
And this object takes a few things.

84
00:05:21.288 --> 00:05:23.477
So the first thing is model.

85
00:05:23.477 --> 00:05:24.688
So what model do we want?

86
00:05:24.688 --> 00:05:29.284
And if you see, here are all
the models that this SDK supports,

87
00:05:29.284 --> 00:05:32.343
there are tons of different models here.

88
00:05:32.343 --> 00:05:36.655
For this example, it doesn't matter which
one you pick, they all have different

89
00:05:36.655 --> 00:05:41.422
costs associated with them, or 3.5 and 4
have different costs associated with them.

90
00:05:41.422 --> 00:05:45.518
And then between those two, like I said,
the higher the number, usually,

91
00:05:45.518 --> 00:05:46.807
the later the release.

92
00:05:46.807 --> 00:05:52.228
And then the number with the k on it just
describes something called context limit,

93
00:05:52.228 --> 00:05:57.119
as in you can send it more and it can send
you back more before it hits a limit.

94
00:05:57.119 --> 00:05:58.332
That's just what that number means.

95
00:05:58.332 --> 00:06:01.925
So you can see they're up to
32k on one of these models.

96
00:06:01.925 --> 00:06:05.293
Claude has one that's 100k, but
there's problems with that,

97
00:06:05.293 --> 00:06:07.313
it's not always the more, the better.

98
00:06:07.313 --> 00:06:10.886
And we'll talk about that when
we get into the chat example or

99
00:06:10.886 --> 00:06:14.614
the semantic search, one of those,
I'll talk more about it.

100
00:06:14.614 --> 00:06:16.752
But for now,
I'm just gonna pick 3.5-turbo,

101
00:06:16.752 --> 00:06:18.701
it doesn't really matter
which one you pick.

102
00:06:18.701 --> 00:06:23.000
So I'm just gonna pick that one,
and then we're gonna say messages.

103
00:06:23.000 --> 00:06:25.825
messages is gonna be an array.

104
00:06:25.825 --> 00:06:29.736
And the way messages work,
it's an array, so

105
00:06:29.736 --> 00:06:34.864
they're gonna be in the order
in which they are in the array.

106
00:06:34.864 --> 00:06:37.210
The format is very specific.

107
00:06:37.210 --> 00:06:41.476
So a message is an object and
there's always a role, right?

108
00:06:41.476 --> 00:06:45.774
And then for the role,
it's one of these four things,

109
00:06:45.774 --> 00:06:50.831
it's either an assistant,
a function, the system, or user.

110
00:06:50.831 --> 00:06:52.763
The two main ones are user and assistant.

111
00:06:52.763 --> 00:06:57.449
Assistant is like the AI wrote this,
this is a response from the AI.

112
00:06:57.449 --> 00:06:59.573
A user is this is you talking to the AI.

113
00:06:59.573 --> 00:07:03.661
So if you make a new message,
you have to make sure the role says user.

114
00:07:03.661 --> 00:07:06.697
If the role says assistant,
that's a message you got back.

115
00:07:06.697 --> 00:07:10.894
System is, if you've ever used ChatGPT,
there's a way for

116
00:07:10.894 --> 00:07:16.100
you to kind of, what's called,
steer the system and kinda give it a role.

117
00:07:16.100 --> 00:07:20.768
Like, hey, you are a pirate that
only talks in pirate slang and

118
00:07:20.768 --> 00:07:23.071
you're educated on physics.

119
00:07:23.071 --> 00:07:27.990
So you're talking to the system as
a whole and not so much the assistant.

120
00:07:27.990 --> 00:07:29.764
It's like instructions for
the entire system.

121
00:07:29.764 --> 00:07:33.584
So you typically only do that at
the beginning of your chat history, and

122
00:07:33.584 --> 00:07:35.119
first, we'll do that now.

123
00:07:35.119 --> 00:07:38.161
So I might say role system,
and then I'll say content,

124
00:07:38.161 --> 00:07:40.053
which is the message I wanna send.

125
00:07:40.053 --> 00:07:42.873
And I'll be like,

126
00:07:42.873 --> 00:07:47.991
You are an AI assistant, answer,

127
00:07:51.541 --> 00:07:58.140
Any questions to the best of your ability,
right?

128
00:07:58.140 --> 00:07:59.985
So that's a system message.

129
00:07:59.985 --> 00:08:02.231
This is not me talking to the AI,

130
00:08:02.231 --> 00:08:06.741
this is me kind of priming the system
in which the AI operates in.

131
00:08:06.741 --> 00:08:09.940
I like to think of it as
the background story for the AI.

132
00:08:09.940 --> 00:08:13.108
This is its background,
this is its origin story.

133
00:08:13.108 --> 00:08:14.963
That's the way I like to think of it.

134
00:08:14.963 --> 00:08:18.882
Like I said, you typically only do
it at the beginning of the message.

135
00:08:18.882 --> 00:08:23.582
And then from there, for
instance, let's send a message.

136
00:08:23.582 --> 00:08:29.334
We're gonna say, role: user, right?

137
00:08:29.334 --> 00:08:34.838
So user, and
then we'll send a message that says,

138
00:08:34.838 --> 00:08:38.938
or I'm sorry, content that says Hi.

139
00:08:38.938 --> 00:08:40.461
We'll do that, we'll say Hi.

140
00:08:43.408 --> 00:08:47.174
Okay, and then what we wanna do is just
wanna get the response back from this.

141
00:08:47.174 --> 00:08:53.467
So we save this in a variable,
I'll just say results = await this.

142
00:08:53.467 --> 00:08:57.177
So if you're on version 18 of node,
which you should be,

143
00:08:57.177 --> 00:08:58.969
you can do top level awaits.

144
00:08:58.969 --> 00:09:01.612
If you're wondering how am I doing await,
not inside of an async?

145
00:09:01.612 --> 00:09:05.060
You could do this at the root of a file
now without having to be an async

146
00:09:05.060 --> 00:09:05.676
function.

147
00:09:05.676 --> 00:09:09.235
If you're on the latest version of node,
some's gonna await that.

148
00:09:09.235 --> 00:09:11.342
And then I think we'll just log this for
now so

149
00:09:11.342 --> 00:09:14.519
you can kinda see what's coming back and
then we'll talk about it.

150
00:09:14.519 --> 00:09:20.646
So let's do that, and then yeah,
let's see if we broke something or not.

151
00:09:20.646 --> 00:09:25.293
So now we'll say node index,
we'll run that.

152
00:09:25.293 --> 00:09:28.343
And you can see, this is what I get back.

153
00:09:28.343 --> 00:09:31.998
I get back an id, some pretty cool stuff.

154
00:09:31.998 --> 00:09:36.257
I think the two things that you really
wanna look at are, one, the usage.

155
00:09:36.257 --> 00:09:39.085
So this is how many tokens my prompt has.

156
00:09:39.085 --> 00:09:43.923
You can think of a token, basically,
I think it averages out to every three or

157
00:09:43.923 --> 00:09:45.126
four characters.

158
00:09:45.126 --> 00:09:49.393
You can kinda think of it as
softly is a token as a word.

159
00:09:49.393 --> 00:09:51.966
It's not a character,
it's not like every character is a token.

160
00:09:51.966 --> 00:09:55.301
It's on average,
three to four characters, maybe a word.

161
00:09:55.301 --> 00:10:00.218
So you see I had 29 tokens, and
then the completion tokens are the tokens

162
00:10:00.218 --> 00:10:04.433
in which the AI responded back with,
so how many tokens it had.

163
00:10:04.433 --> 00:10:07.155
And then you combine them all,
these are the total tokens.

164
00:10:07.155 --> 00:10:11.860
And at least for OpenAI, ChatGPT,
you get charged on how many

165
00:10:11.860 --> 00:10:16.852
tokens you are generating because
that is a linear cost for them.

166
00:10:16.852 --> 00:10:18.938
However many tokens their
system has to generate,

167
00:10:18.938 --> 00:10:20.984
that's more compute that
they're charged for.

168
00:10:20.984 --> 00:10:23.206
That's more electricity, that's more time.

169
00:10:23.206 --> 00:10:26.066
So they charge you for that, and
they basically pass that cost on.

170
00:10:26.066 --> 00:10:30.523
So the more tokens you have as a prompt,
and the more completion tokens that

171
00:10:30.523 --> 00:10:34.156
are sent back, that's how you
are charged on a token basis.

172
00:10:34.156 --> 00:10:38.838
So there are products out there that help
you figure out how to maintain this cost.

173
00:10:38.838 --> 00:10:43.737
How to make a router that sends specific
prompts to specific AIs that are optimized

174
00:10:43.737 --> 00:10:45.520
for a specific token causes.

175
00:10:45.520 --> 00:10:48.312
It's crazy, it gets really nuts.

176
00:10:48.312 --> 00:10:51.588
So you got that, but
the other thing is the choices, and

177
00:10:51.588 --> 00:10:53.419
this is where the AI responds.

178
00:10:53.419 --> 00:10:55.556
So let's actually take a look at that.

179
00:10:55.556 --> 00:11:00.208
So I'm gonna log results.choices(0).

180
00:11:00.208 --> 00:11:03.032
Let's run that again.

181
00:11:03.032 --> 00:11:06.328
And you can see I get back an object,
it has a message, and

182
00:11:06.328 --> 00:11:08.134
you can see role is assistant.

183
00:11:08.134 --> 00:11:10.307
I mean, this is the AI talking now.

184
00:11:10.307 --> 00:11:13.909
role: assistant, and then content,
Hello, How can I assist you today?

185
00:11:13.909 --> 00:11:16.089
And then finish_reason is stop.

186
00:11:16.089 --> 00:11:19.194
That just means it had nothing else
to say, like I'm done talking to you.

187
00:11:19.194 --> 00:11:22.979
This is important later when we talk about
functions cuz sometimes it's not finished.

188
00:11:22.979 --> 00:11:26.506
It might respond and it might not be done.

189
00:11:26.506 --> 00:11:28.521
Because either a, it hit a context,

190
00:11:28.521 --> 00:11:32.811
it hit the limit of which it can respond
as far as a token limit, and/or it needs

191
00:11:32.811 --> 00:11:37.249
you to do something in your code before
it can continue and provide more context.

192
00:11:37.249 --> 00:11:40.779
So it might not always be done
even though it responded, and

193
00:11:40.779 --> 00:11:43.186
that's where it can get pretty tricky.

194
00:11:43.186 --> 00:11:46.098
If you ever use ChatGPT,
there's a button that says, or

195
00:11:46.098 --> 00:11:50.540
sometimes it will stop halfway through the
generation, you have to say, keep going.

196
00:11:50.540 --> 00:11:54.840
This is it, it hit a limit on how
much it can respond with, but yet

197
00:11:54.840 --> 00:11:56.205
it's not done yet.

198
00:11:56.205 --> 00:11:58.814
So pretty cool stuff.

199
00:11:58.814 --> 00:12:01.115
But anyway, yeah, so we get that back.

200
00:12:01.115 --> 00:12:05.430
So if we just log that, .message.content.

201
00:12:05.430 --> 00:12:09.760
And then we can just change this,
we can be like, Hi!

202
00:12:09.760 --> 00:12:14.844
Can you tell me what is

203
00:12:14.844 --> 00:12:19.645
the best way to learn

204
00:12:19.645 --> 00:12:23.605
how to do maths?

205
00:12:23.605 --> 00:12:26.895
I don't know, let's see what it says.

206
00:12:26.895 --> 00:12:27.555
There it is.

207
00:12:27.555 --> 00:12:31.411
So you can see, the more tokens it needed
to respond with, the longer it took.

208
00:12:31.411 --> 00:12:35.348
This is why the cost goes up,
cuz that is a cost for them, right?

209
00:12:35.348 --> 00:12:40.653
And yeah, it kinda went crazy here,
it really thought about this.

210
00:12:40.653 --> 00:12:45.631
I'm not gonna read this, but yeah,
the thing to notice here is that if I run

211
00:12:45.631 --> 00:12:50.631
this again, what do you think would
happen if I run the exact prompt again?

212
00:12:50.631 --> 00:12:52.130
You think I'll get back the same answer?

213
00:12:52.130 --> 00:12:54.617
&gt;&gt; No, it's similar.

214
00:12:54.617 --> 00:12:57.732
&gt;&gt; Hopefully it's similar, but it's
definitely not gonna be exactly the same.

215
00:12:57.732 --> 00:13:02.202
So the important thing to remember
is that LLMs are non-deterministic,

216
00:13:02.202 --> 00:13:05.893
you're never gonna get back
the same thing from them twice.

217
00:13:05.893 --> 00:13:09.831
And that is probably the hardest part
about building apps with these things,

218
00:13:09.831 --> 00:13:12.508
is that you're never gonna
get back the same thing.

219
00:13:12.508 --> 00:13:16.511
So if you need some structured data or
some consistency around what you're

220
00:13:16.511 --> 00:13:19.763
getting back, you're never
gonna get back the same thing.

221
00:13:19.763 --> 00:13:21.153
So how do you build around?

222
00:13:21.153 --> 00:13:25.350
Imagine your database didn't have
a schema, and every time you created,

223
00:13:25.350 --> 00:13:28.014
it was a different shape
every single time, and

224
00:13:28.014 --> 00:13:30.562
you didn't know what
the shape was gonna be.

225
00:13:30.562 --> 00:13:31.735
How do you write code against that?

226
00:13:31.735 --> 00:13:33.177
There's no contract there.

227
00:13:33.177 --> 00:13:36.687
So a lot of the tools and
things are trying to solve this problem,

228
00:13:36.687 --> 00:13:39.884
and we'll talk about the different
solutions around it.

229
00:13:39.884 --> 00:13:44.696
But that's usually the most difficult
part, is how do you get back consistent,

230
00:13:44.696 --> 00:13:45.912
constructed data.

231
00:13:45.912 --> 00:13:46.940
How do you test this?

232
00:13:46.940 --> 00:13:48.026
It's non-deterministic.

233
00:13:48.026 --> 00:13:50.215
How do you evaluate this?

234
00:13:50.215 --> 00:13:55.059
[LAUGH] It's kinda hard,
but pretty exciting.

235
00:13:55.059 --> 00:13:56.725
We've all talked to it before.

236
00:13:56.725 --> 00:13:58.400
If you haven't, what is a chatbot?

237
00:13:58.400 --> 00:14:02.185
I mean, basically, it's an assistant
in which you can talk to.

238
00:14:02.185 --> 00:14:04.973
I think the main difference
in what we just did and

239
00:14:04.973 --> 00:14:08.104
what a chatbot actually is
that it has memory, right?

240
00:14:08.104 --> 00:14:12.238
The example that we just wrote,
right now I can't say hey,

241
00:14:12.238 --> 00:14:17.380
my name is Scott and then ask a follow-up
question like, what is my name?

242
00:14:17.380 --> 00:14:22.370
It won't know because,
let me describe it in a better way, right?

243
00:14:22.370 --> 00:14:29.450
So if I say, Hi, my name is scott,

244
00:14:29.450 --> 00:14:33.614
right, like this.

245
00:14:33.614 --> 00:14:37.880
And I run this, this is cool,
how can I assist you today, right?

246
00:14:37.880 --> 00:14:40.557
I can't run this again and
be like, hi, what is my name?

247
00:14:49.497 --> 00:14:50.461
It's not gonna know.

248
00:14:50.461 --> 00:14:51.205
It's like, what?

249
00:14:51.205 --> 00:14:52.168
What are you talking about, right?

250
00:14:52.168 --> 00:14:57.184
So the only way to guarantee that
it knows what you're talking about

251
00:14:57.184 --> 00:15:02.464
is you have to feed it the entire
history of your conversation forever,

252
00:15:02.464 --> 00:15:05.460
[LAUGH] up until it hits a context window.

253
00:15:05.460 --> 00:15:08.898
So really,
ChatGPT is just what we just built.

254
00:15:08.898 --> 00:15:13.587
Plus, it's being smart about how it
sends the history, because by default,

255
00:15:13.587 --> 00:15:18.292
what it'll do is it'll just keep sending
the history the whole time, right?

256
00:15:18.292 --> 00:15:24.669
So for instance, if I say,
Hi, my name is scott?

257
00:15:24.669 --> 00:15:28.926
And then if I got back the response
from the bot and added it here, and

258
00:15:28.926 --> 00:15:31.402
then I did a follow up of what is my name?

259
00:15:31.402 --> 00:15:35.317
Then it would know, but that's because
I included the entire history, right,

260
00:15:35.317 --> 00:15:36.306
every single time.

261
00:15:36.306 --> 00:15:39.648
So eventually you do hit a limit of
how much tokens you can send, and

262
00:15:39.648 --> 00:15:42.466
then it can't remember anymore,
it ran out of memory.

263
00:15:42.466 --> 00:15:46.851
So then you might have hit that in
ChatGPT, where it's like, I can't talk

264
00:15:46.851 --> 00:15:51.317
anymore, I forgot, I don't know what
you're talking about and hit its limit.

265
00:15:51.317 --> 00:15:55.746
So at that point what it tries to do,
there's different techniques, it'll try to

266
00:15:55.746 --> 00:15:59.814
summarize the entire conversation up
until that point to create a summary.

267
00:15:59.814 --> 00:16:03.995
So it might miss some details
specifically about the conversation, but

268
00:16:03.995 --> 00:16:07.899
it will just make, basically let's
say you have 50 entries, and

269
00:16:07.899 --> 00:16:11.624
then on the 51st one,
that's when it hit its limit of memory.

270
00:16:11.624 --> 00:16:14.932
It'll take all the 50 before that and
create a summary and

271
00:16:14.932 --> 00:16:17.060
make that one message in its history.

272
00:16:17.060 --> 00:16:20.410
So therefore,
it just summarize the history before that.

273
00:16:20.410 --> 00:16:22.477
So that's kinda one technique.

274
00:16:22.477 --> 00:16:25.953
There's different techniques in
which it tries to remember, but

275
00:16:25.953 --> 00:16:28.199
that's basically ChatGPT in a nutshell.

276
00:16:28.199 --> 00:16:29.694
So we're gonna create something similar.

277
00:16:29.694 --> 00:16:34.287
We're not gonna handle the edge
case of running out of memory.

278
00:16:34.287 --> 00:16:40.506
[LAUGH] Yeah, we're not doing that, but
we're gonna make it so you can chat to it.

279
00:16:40.506 --> 00:16:43.939
Cool, so that's basically
the limits of it, is the memory,

280
00:16:43.939 --> 00:16:45.862
so you provide the whole history.

281
00:16:45.862 --> 00:16:50.093
And then yeah, I think the other part
of it is just having it stream back

282
00:16:50.093 --> 00:16:53.754
the responses versus waiting for
the response to come back,

283
00:16:53.754 --> 00:16:55.284
you can stream the back.

284
00:16:55.284 --> 00:17:00.739
We're not gonna do that because it's
nothing too crazy about doing that,

285
00:17:00.739 --> 00:17:03.779
it's really a UI that makes that better.

286
00:17:03.779 --> 00:17:07.051
But that's really the only interesting
thing we just built in a chatbot,

287
00:17:07.051 --> 00:17:08.545
it's mostly the memory aspect.

