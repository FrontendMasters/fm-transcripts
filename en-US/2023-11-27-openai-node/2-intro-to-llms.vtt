WEBVTT

1
00:00:00.200 --> 00:00:02.784
Anyone know an LLMs, wanna give it a shot?

2
00:00:02.784 --> 00:00:04.204
&gt;&gt; Large language model?

3
00:00:04.204 --> 00:00:06.201
&gt;&gt; Large language model exactly.

4
00:00:06.201 --> 00:00:08.620
It's exactly what it sounds like.

5
00:00:08.620 --> 00:00:11.544
So let me kinda break it down.

6
00:00:11.544 --> 00:00:13.590
Large language model.

7
00:00:13.590 --> 00:00:16.400
So in AI world there are these
things called models.

8
00:00:16.400 --> 00:00:20.935
You can think of models as it's really
just like this gigantic file that

9
00:00:20.935 --> 00:00:22.730
represents, weights.

10
00:00:22.730 --> 00:00:27.442
And basically, it represents statistical
choices that have to be made by

11
00:00:27.442 --> 00:00:31.550
some system, and it kinda guides
it throughout those choices.

12
00:00:31.550 --> 00:00:33.710
It's just a bunch of numbers.

13
00:00:33.710 --> 00:00:39.960
And that's using output of something that
has been trained on some sort of data.

14
00:00:39.960 --> 00:00:43.987
And those files, depending on how much
data went into them, can be large files,

15
00:00:43.987 --> 00:00:47.860
they could be smaller files, things
like that, but that's what a model is.

16
00:00:47.860 --> 00:00:52.932
A large language model is
something that is going to be

17
00:00:52.932 --> 00:00:58.620
useful at understanding and
creating human language.

18
00:00:58.620 --> 00:01:00.490
And that's why it's called
a large language model.

19
00:01:00.490 --> 00:01:04.726
Large being that it was trained on,
and in this case,

20
00:01:04.726 --> 00:01:10.550
billions of tokens of training data,
so billions of parameters.

21
00:01:10.550 --> 00:01:12.800
Yeah, that's basically what that means.

22
00:01:12.800 --> 00:01:15.664
So you can think of them as, I mean,

23
00:01:15.664 --> 00:01:20.164
if you've ever used ChatGPT,
that's an LLM, right?

24
00:01:20.164 --> 00:01:25.130
Using GPT 3.5, or 4 if you switch
that over, they feel very smart.

25
00:01:25.130 --> 00:01:28.510
They almost feel conscious, right?

26
00:01:28.510 --> 00:01:30.140
How do they feel that way?

27
00:01:30.140 --> 00:01:36.412
That basically comes down to
one big discovery not too long

28
00:01:36.412 --> 00:01:42.430
ago actually,
something called transformers.

29
00:01:42.430 --> 00:01:45.929
Up until transformers was created
the way that people have built LLMs,

30
00:01:45.929 --> 00:01:49.499
like some of the early versions of GPT and
this other thing called Burke.

31
00:01:50.910 --> 00:01:54.655
When those LLMs tried to
make predictions on text,

32
00:01:54.655 --> 00:01:59.445
they wouldn't consider the characters
that came before it, and

33
00:01:59.445 --> 00:02:04.750
the text and the words and
the semantics behind those characters.

34
00:02:04.750 --> 00:02:10.130
They would only try to predict what would
come next based off of where you are now.

35
00:02:10.130 --> 00:02:14.270
They didn't have this recursive
graph understanding of

36
00:02:14.270 --> 00:02:16.930
everything up until that point.

37
00:02:16.930 --> 00:02:22.060
So the context was very limited on what it
needed to be able to predict something.

38
00:02:22.060 --> 00:02:28.258
So something called transformers with
attention basically changed that, so this

39
00:02:28.258 --> 00:02:34.293
kind of upgraded the LLMs to be a little
more aware of the intent, or the meaning.

40
00:02:34.293 --> 00:02:39.885
Or the semantics behind whatever
language as the input was coming in,

41
00:02:39.885 --> 00:02:42.636
which therefore exponentially,

42
00:02:42.636 --> 00:02:47.210
upgraded the quality of
the output of those LLM systems.

43
00:02:47.210 --> 00:02:50.313
So now because they know so
much more about what is being said,

44
00:02:50.313 --> 00:02:52.920
they have more context on
what they can predict.

45
00:02:52.920 --> 00:02:56.126
It's basically like, if you were
listening to me talk right now and

46
00:02:56.126 --> 00:02:58.939
you didn't hear anything I said
until just this last word,

47
00:02:58.939 --> 00:03:01.830
how would you predict what I
was gonna say next effectively?

48
00:03:01.830 --> 00:03:05.233
It'd be really tough, but if you do
everything I said up until this point,

49
00:03:05.233 --> 00:03:08.490
you might have a better chance of
predicting what I might say next.

50
00:03:08.490 --> 00:03:11.848
So that's the difference
on a very small scale.

51
00:03:11.848 --> 00:03:16.320
So yeah, pretty effective technology.

52
00:03:16.320 --> 00:03:19.420
So all today's LLMs are mostly
based off of transformers.

53
00:03:19.420 --> 00:03:22.590
That's the T in GPT is transformers.

54
00:03:22.590 --> 00:03:26.480
And attention is the technique
in which it uses.

55
00:03:26.480 --> 00:03:30.130
So some of the ways that you
might see LLMs crushing it today,

56
00:03:30.130 --> 00:03:34.220
you have things that you probably know,
like writing and content.

57
00:03:34.220 --> 00:03:36.380
So if you need a blog post written or
something like that,

58
00:03:36.380 --> 00:03:37.930
it's very helpful for that.

59
00:03:37.930 --> 00:03:43.890
For me, I use it to help me make sense
of some of the code that I was writing.

60
00:03:43.890 --> 00:03:46.951
And in a way that I wanted to
present to you also was actually

61
00:03:46.951 --> 00:03:50.386
one of the first times I used an LLM
to help me figure out the best way

62
00:03:50.386 --> 00:03:53.350
to deliver a curriculum versus
me just doing it myself.

63
00:03:53.350 --> 00:03:55.200
So that was a new experience for me.

64
00:03:55.200 --> 00:03:56.830
Customer support is a big one.

65
00:03:58.720 --> 00:04:02.920
Research is very reliable up until
a point and we'll talk about that.

66
00:04:02.920 --> 00:04:08.906
Education is huge, using LLMs to
learn things a lot more effectively.

67
00:04:08.906 --> 00:04:12.232
When I was actually going back and
learning a lot of those math concepts,

68
00:04:12.232 --> 00:04:15.120
I used GPT, to help me make sense
of the things that were reading

69
00:04:15.120 --> 00:04:17.870
in a textbook because it
didn't make sense to me.

70
00:04:17.870 --> 00:04:21.331
So I'd always ask you, give me a real
world analogy of what the hell this means,

71
00:04:21.331 --> 00:04:23.410
because this explanation
means nothing to me.

72
00:04:23.410 --> 00:04:28.107
And those analogies actually helped me see
it a lot better cuz I need to visually see

73
00:04:28.107 --> 00:04:29.380
things.

74
00:04:29.380 --> 00:04:34.137
And then, obviously, automation,
because you now have this system

75
00:04:34.137 --> 00:04:38.750
that can understand unstructured
content like human text.

76
00:04:38.750 --> 00:04:42.850
Well, now you can automate things that
you couldn't previously have done before.

77
00:04:42.850 --> 00:04:47.774
So there's a lot of human only tasks that
you couldn't really automate efficiently

78
00:04:47.774 --> 00:04:50.267
enough to make it worth
it that you can now,

79
00:04:50.267 --> 00:04:53.200
so we'll talk about some
of those things as well.

80
00:04:53.200 --> 00:04:56.371
And if you've ever used Copilot or
anything like that,

81
00:04:56.371 --> 00:04:58.335
that's what I'm talking about.

82
00:05:00.948 --> 00:05:06.356
Yeah, so here's this, some other
stuff about why they are super cool,

83
00:05:06.356 --> 00:05:08.670
if you wanna read a little bit.

84
00:05:08.670 --> 00:05:12.575
But basically it just talks about
how the predictions are made,

85
00:05:12.575 --> 00:05:15.880
how it's trained on tons of data,
things like that.

86
00:05:15.880 --> 00:05:17.520
I don't think you really
need to know any of this.

87
00:05:17.520 --> 00:05:19.910
I'll just put this in here as context.

88
00:05:19.910 --> 00:05:23.150
If you really wanna get into
some of the more details about

89
00:05:23.150 --> 00:05:25.174
how the LLMs are created and stuff,

90
00:05:25.174 --> 00:05:29.065
I think the most important thing to
know is that it is not conscious.

91
00:05:29.065 --> 00:05:32.557
[LAUGH] It is not or
as far as we know, I mean, do we know?

92
00:05:32.557 --> 00:05:36.202
I don't know, as far as we know, it's not.

93
00:05:36.202 --> 00:05:39.458
But then again we don't really know
how the inner layers of a neural

94
00:05:39.458 --> 00:05:40.284
network works.

95
00:05:40.284 --> 00:05:43.972
As far as we know, we could also
have a neural network who knows?

96
00:05:43.972 --> 00:05:47.773
I didn't say that, but
yeah, it's not conscious,

97
00:05:47.773 --> 00:05:51.585
it's just predicting things
based off the context.

98
00:05:51.585 --> 00:05:55.915
So it feels very powerful, and that's
because it's been trained on a bunch of

99
00:05:55.915 --> 00:06:00.395
data, which requires tons of compute
that you just don't have sitting around.

100
00:06:00.395 --> 00:06:04.371
You need millions of dollars worth of
compute and tons of time to be able to

101
00:06:04.371 --> 00:06:08.035
train one of these things, and
even then, it's not good enough.

102
00:06:08.035 --> 00:06:10.525
There's still more training after that,
that needs to happen.

103
00:06:10.525 --> 00:06:14.894
And then, as far as the LLMs that
we're gonna use today, like I said,

104
00:06:14.894 --> 00:06:16.687
we're gonna use an OpenAI.

105
00:06:16.687 --> 00:06:21.816
OpenAI, see if they have a list
of all there's somewhere,

106
00:06:21.816 --> 00:06:28.905
they have tons of LLMs actually should
be one of their playground, there we go.

107
00:06:28.905 --> 00:06:30.960
They have tons of them.

108
00:06:30.960 --> 00:06:33.220
Here, if I click on Show More Models,

109
00:06:33.220 --> 00:06:36.903
these are basically all
the different models that they have.

110
00:06:36.903 --> 00:06:41.434
They start off at like GPT 3.5 Turbo,
which is the default one that if you use

111
00:06:41.434 --> 00:06:44.660
ChatGPT, that's the one
that you're talking to.

112
00:06:44.660 --> 00:06:49.091
It's their fastest one, it's pretty good,
it sets the standard of what things are.

113
00:06:49.091 --> 00:06:52.210
Since then, they've made so
many more releases.

114
00:06:52.210 --> 00:06:56.117
The way that this works is that
each number after it is basically,

115
00:06:56.117 --> 00:07:01.310
you can think of it as like the higher
number here just means a newer version.

116
00:07:01.310 --> 00:07:03.723
There's more that goes into it,
but that's basically it.

117
00:07:03.723 --> 00:07:07.930
And then sometimes you might see something
like 16K or something like that.

118
00:07:07.930 --> 00:07:10.680
That describes something
called the token limit.

119
00:07:10.680 --> 00:07:14.037
And we'll talk more about
the token limit and

120
00:07:14.037 --> 00:07:18.020
context window when we get
into some of the examples.

121
00:07:18.020 --> 00:07:22.395
I don't wanna get into it too far, but
this just means that this thing can handle

122
00:07:22.395 --> 00:07:25.724
more text than the thing that
doesn't have this by default,

123
00:07:25.724 --> 00:07:28.684
it doesn't handle 60,000
tokens of context.

124
00:07:28.684 --> 00:07:33.238
And then you have the next version
of that which is GPT-4, so better.

125
00:07:33.238 --> 00:07:36.387
Right now,
GPT-4 is probably the best LLM out today,

126
00:07:36.387 --> 00:07:40.206
there really isn't anything that
comes close to how good it is, but

127
00:07:40.206 --> 00:07:44.107
there are things that are more
than good enough, including 3.5.

128
00:07:44.107 --> 00:07:48.282
So most of today I'll be using 3.5,
you can switch over to 4 if you wanna

129
00:07:48.282 --> 00:07:51.066
spend some money on,
when I say spend some money,

130
00:07:51.066 --> 00:07:54.402
I'm talking like a penny or
less, it's still very cheap.

131
00:07:54.402 --> 00:07:58.170
But I'll mostly be using 3.5 because it's
not gonna be that big of a difference per

132
00:07:58.170 --> 00:08:01.380
se, but you can switch out to wherever
you want when me run the examples.

133
00:08:01.380 --> 00:08:07.892
And then, of course, there's other ones
you have, like Claude from Anthropic,

134
00:08:07.892 --> 00:08:13.840
which is a very good alternative,
it's basically the same thing.

135
00:08:13.840 --> 00:08:17.040
They have an API,
they have a really cool stuff.

136
00:08:17.040 --> 00:08:21.928
I think the models that they currently
have right now are mostly all better than

137
00:08:21.928 --> 00:08:25.720
3.5, but not as good as GPT 4,
so somewhere in between.

138
00:08:25.720 --> 00:08:30.736
And then there's Cohear,
there's also a huge list

139
00:08:30.736 --> 00:08:37.160
of just open source ones that
you can go to like Hugging Face.

140
00:08:37.160 --> 00:08:38.600
I know it's such a weird name right?

141
00:08:38.600 --> 00:08:39.300
I love it though.

142
00:08:39.300 --> 00:08:42.780
Hugging Face is like the GitHub for
AI, basically.

143
00:08:42.780 --> 00:08:45.444
And there's just tons
of models on here and

144
00:08:45.444 --> 00:08:48.034
the biggest one might be called Llama 2.

145
00:08:48.034 --> 00:08:52.366
This is probably one of the biggest
open-source ones made by Facebook,

146
00:08:52.366 --> 00:08:57.720
that if you train it a certain way, it can
be just as good as 3.5, if not better.

147
00:08:57.720 --> 00:09:00.950
But probably not as good as 4,
depending on how many parameters you want.

148
00:09:00.950 --> 00:09:03.770
So this version is Llama 2,
7 billion parameters.

149
00:09:03.770 --> 00:09:07.172
And they have another one that might
have like 20 billion parameters or

150
00:09:07.172 --> 00:09:09.600
something like that, that makes it better.

151
00:09:09.600 --> 00:09:12.631
And that's another difference, Claude,
Open AI, those things that are hosted,

152
00:09:12.631 --> 00:09:14.010
you don't need any infrastructure.

153
00:09:14.010 --> 00:09:16.650
You just hit an API,
you get back a response.

154
00:09:16.650 --> 00:09:21.149
The open-source ones, you have to deploy
those somewhere or run them locally,

155
00:09:21.149 --> 00:09:23.150
which means you need a GPU.

156
00:09:23.150 --> 00:09:26.980
So it's open-source, but you still
need the infrastructure to run it.

157
00:09:26.980 --> 00:09:31.230
You can't really run these things on a CPU
yet efficiently, they'll be really slow.

158
00:09:31.230 --> 00:09:35.424
And that's just because,
like how a GPU runs compared to a CPU,

159
00:09:35.424 --> 00:09:39.638
the CPU doesn't run as many
processes in parallel as a GPU does.

160
00:09:39.638 --> 00:09:44.208
The GPU was built to show frames
on the screen very fast, so

161
00:09:44.208 --> 00:09:49.899
it's built in such a way that it can
do a bunch of smaller tasks in parallel

162
00:09:49.899 --> 00:09:55.428
way better than a CPU can, where CPU
might do a few things in parallel.

163
00:09:55.428 --> 00:10:00.250
But those few things are very high
compute, like some algorithm or something

164
00:10:00.250 --> 00:10:05.610
like doing Fibonacci in one process and
crunching numbers in another process.

165
00:10:05.610 --> 00:10:08.590
Whereas a GPU is like,
I'm throwing frames on the screen.

166
00:10:08.590 --> 00:10:13.696
So it's great for something that
needs a lot of work done quickly,

167
00:10:13.696 --> 00:10:16.170
in the case of generating text.

168
00:10:16.170 --> 00:10:19.854
So that's why you need a GPU to
run a lot of these things, and

169
00:10:19.854 --> 00:10:22.087
that's why there's a GPU crunch.

170
00:10:22.087 --> 00:10:24.880
Right now, that's why NVIDIA is one of the
biggest companies in the world right now.

171
00:10:24.880 --> 00:10:26.770
Because everyone needs a GPU to run AI.

172
00:10:27.970 --> 00:10:29.470
That's kind of where we are right now.

173
00:10:29.470 --> 00:10:34.261
&gt;&gt; Any thoughts on the AI for personal
usage, like diet plan calendar set up?

174
00:10:34.261 --> 00:10:36.440
&gt;&gt; I use it all the time for that.

175
00:10:36.440 --> 00:10:41.402
I mean, there are people who are creating
apps around that to help you but yeah,

176
00:10:41.402 --> 00:10:42.970
you can go into ChatGPT.

177
00:10:42.970 --> 00:10:47.213
I've literally went into ChatGPT and
I was like, here are my goals,

178
00:10:47.213 --> 00:10:49.570
create me a fitness plan.

179
00:10:49.570 --> 00:10:51.715
And here's what I'm good at,
here's what I'm bad at,

180
00:10:51.715 --> 00:10:53.990
here are previous injuries that I've had.

181
00:10:53.990 --> 00:10:58.791
And it'll create me something, I don't
take its word as fact because, again,

182
00:10:58.791 --> 00:11:01.870
it's just predicting
what it should say next.

183
00:11:01.870 --> 00:11:03.807
But for me, it's a great place to start.

184
00:11:03.807 --> 00:11:06.766
For me, like, yeah, that's okay,
I can see that, and wait,

185
00:11:06.766 --> 00:11:10.150
that actually doesn't make sense,
I'm not gonna jump rope for three hours.

186
00:11:10.150 --> 00:11:10.870
Why did you say that?

187
00:11:10.870 --> 00:11:12.900
That sounds stupid, but I get it.

188
00:11:12.900 --> 00:11:14.980
Maybe I should jump rope for
20 minutes, though.

189
00:11:14.980 --> 00:11:17.280
The benefits of why you
said I should make sense.

190
00:11:17.280 --> 00:11:22.360
So it's just like a great place to start,
so I highly would recommend.

191
00:11:23.890 --> 00:11:28.800
Basically, as far as personal usage,
bringing in something like GPT where

192
00:11:28.800 --> 00:11:33.440
you would typically go Google something,
I'm gonna go look this up.

193
00:11:33.440 --> 00:11:36.940
Cuz if you think about how that works,
you're looking something up,

194
00:11:36.940 --> 00:11:39.906
you're hoping Google responds
back with some article, or

195
00:11:39.906 --> 00:11:43.959
something that you can read, that you can
then like try to make a decision off of.

196
00:11:43.959 --> 00:11:48.503
You can skip a lot of that by just going
straight to GPT because it was probably

197
00:11:48.503 --> 00:11:53.402
trained on those same articles that you're
eventually gonna click on anyway, so

198
00:11:53.402 --> 00:11:56.900
it's just another
additional research tool.

199
00:11:56.900 --> 00:12:00.860
And then even Google and
Bing have AI results built into the search

200
00:12:00.860 --> 00:12:04.473
engine now that, I don't know,
can be useful sometimes.

201
00:12:04.473 --> 00:12:09.477
I would say one out of ten,
the Google stuff

202
00:12:09.477 --> 00:12:13.930
that I get, let me see, what is a car?

203
00:12:15.570 --> 00:12:16.560
Right, I can click Generate.

204
00:12:19.020 --> 00:12:22.500
And yeah, I'll get some AI
stuff here from Google, right?

205
00:12:22.500 --> 00:12:25.698
So it can be useful, but, yeah,
for me, I think of it as more of,

206
00:12:25.698 --> 00:12:27.820
it's just another tool to research.

207
00:12:27.820 --> 00:12:29.730
So take it with a grain of salt.

208
00:12:29.730 --> 00:12:32.320
All right, so
everybody knows what an LLM is.

209
00:12:32.320 --> 00:12:33.720
Everybody's educated on that?

210
00:12:33.720 --> 00:12:36.588
And just to clear,
LLMs are not way different, but

211
00:12:36.588 --> 00:12:41.458
they're mostly different from things like
if you ever use mid-journey and discord,

212
00:12:41.458 --> 00:12:44.700
that's not an LLM,
that's a diffusion model.

213
00:12:44.700 --> 00:12:50.540
Very similar to LLM is just
that the output is an image.

214
00:12:50.540 --> 00:12:53.960
And that's through some diffusion process
that we're not gonna cover today.

215
00:12:53.960 --> 00:12:57.855
But I actually do have a blog post that
goes into that very scientifically that I

216
00:12:57.855 --> 00:13:01.230
can link to later if you wantna
check out how diffusion models work.

217
00:13:01.230 --> 00:13:05.163
I actually broke down a few of
the scientific white papers and

218
00:13:05.163 --> 00:13:09.340
tried to understand it from that level and
explained it in a way.

219
00:13:09.340 --> 00:13:12.923
So that one's way different as far as
the output, but very similar as far as it

220
00:13:12.923 --> 00:13:16.364
being able to understand human language
and then outputting something.

