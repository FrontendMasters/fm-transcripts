WEBVTT

1
00:00:00.520 --> 00:00:03.276
So before we can get into building
the search, there's one or

2
00:00:03.276 --> 00:00:07.300
a few other things we need to understand,
and that's embeddings and vectors.

3
00:00:07.300 --> 00:00:08.180
So let's talk about that.

4
00:00:09.410 --> 00:00:12.490
So what is a Word Embedding?

5
00:00:12.490 --> 00:00:14.043
You can read this description, but

6
00:00:14.043 --> 00:00:16.959
I'm gonna try to explain it to you
in a way that I'm feeling it now.

7
00:00:16.959 --> 00:00:22.800
I feel different when I'm thinking about
things, depending on the time of the day.

8
00:00:22.800 --> 00:00:27.903
But basically, an embedding is,
let's think about it this way,

9
00:00:27.903 --> 00:00:34.112
computers are better at understanding
numbers than they are things like words.

10
00:00:34.112 --> 00:00:37.250
In fact,
they understand numbers very well.

11
00:00:37.250 --> 00:00:42.210
They can actually understand everything as
a number, if you really think about it.

12
00:00:42.210 --> 00:00:47.067
So an embedding is basically
a collection of those numbers.

13
00:00:47.067 --> 00:00:48.791
And in this case,
the numbers are called vectors.

14
00:00:48.791 --> 00:00:56.370
So it's a collection of those vectors
that represent the meaning behind a word.

15
00:00:56.370 --> 00:00:59.810
So we're basically
translating words to numbers.

16
00:00:59.810 --> 00:01:03.310
And those numbers represent
the meaning of those words.

17
00:01:04.530 --> 00:01:07.720
I know, it sounds crazy, but
that's basically what it is.

18
00:01:07.720 --> 00:01:09.220
So why do we do that?

19
00:01:09.220 --> 00:01:13.704
Well, because if we can convert
the words and the meaning behind those

20
00:01:13.704 --> 00:01:17.965
words into numbers, we can better
use that on a computer, right?

21
00:01:17.965 --> 00:01:20.089
Because computers understands
numbers a lot better.

22
00:01:20.089 --> 00:01:23.258
And if it's numbers,
then now we can do math,

23
00:01:23.258 --> 00:01:28.640
do all different types of comparisons
around, right now we understand.

24
00:01:28.640 --> 00:01:33.014
If I say computer, or if I say Apple,
you might think computer,

25
00:01:33.014 --> 00:01:34.723
you might think iPhone.

26
00:01:34.723 --> 00:01:38.180
We understand that,

27
00:01:38.180 --> 00:01:42.795
cuz we have so much context.

28
00:01:42.795 --> 00:01:44.164
But how does a computer understand that?

29
00:01:44.164 --> 00:01:45.948
Well, if we turn everything into numbers,

30
00:01:45.948 --> 00:01:47.886
it can actually do math
to understand that now.

31
00:01:47.886 --> 00:01:51.570
So that's what an embedding is.

32
00:01:51.570 --> 00:01:55.580
And the way that that looks is
basically something like this.

33
00:01:55.580 --> 00:01:57.627
So, for instance, if I had this text,

34
00:01:57.627 --> 00:02:00.047
I can convert that text
to an embedding model,

35
00:02:00.047 --> 00:02:04.161
which comes out as a multi-dimensional
array of numbers or vectors, right?

36
00:02:04.161 --> 00:02:10.850
And a vector is a number between 0 and 1,
basically, and it's just some decimal.

37
00:02:10.850 --> 00:02:16.610
And these vectors are what
they call high-dimensional.

38
00:02:16.610 --> 00:02:20.180
As in, depending on the embedding model,
you might have thousands of dimensions.

39
00:02:20.180 --> 00:02:25.520
So I don't know how to visualize that for
you, but there's more than two dimensions.

40
00:02:25.520 --> 00:02:30.590
It's, think of a dimension is like,
I don't know, an array within an array.

41
00:02:30.590 --> 00:02:33.840
There's like 1,500 of those, basically,
depending on the embedding model.

42
00:02:33.840 --> 00:02:39.144
So it's like a multi-dimensional
array of numbers that represent

43
00:02:39.144 --> 00:02:43.711
the meaning behind the text
in which you're converting.

44
00:02:43.711 --> 00:02:46.776
And there are AI models
that convert that for you.

45
00:02:46.776 --> 00:02:49.073
So we're using AI to convert
the text to numbers,

46
00:02:49.073 --> 00:02:51.780
in which we can then talk to the AI with.

47
00:02:51.780 --> 00:02:54.673
This is very important,
because without this,

48
00:02:54.673 --> 00:02:58.423
the AI would not understand
the semantics behind the meaning.

49
00:02:58.423 --> 00:03:01.058
And we would not be able to compare them,

50
00:03:01.058 --> 00:03:04.244
which is the whole point of doing search,
okay?

51
00:03:04.244 --> 00:03:08.730
And it's deterministic, so if I embed
the same thing, I'll pretty much get back

52
00:03:08.730 --> 00:03:12.972
the same vectors every single time,
which is great, because you need that.

53
00:03:15.552 --> 00:03:18.464
Cool, any questions on that?

54
00:03:18.464 --> 00:03:20.493
You don't really need to
know how all this works.

55
00:03:20.493 --> 00:03:22.511
This is,
remember I said the part that I went on,

56
00:03:22.511 --> 00:03:25.580
I just went like this is the part,
I just went crazy, okay?

57
00:03:25.580 --> 00:03:28.941
You just need to know that I can hit this
embedding thing and get back some numbers,

58
00:03:28.941 --> 00:03:29.487
that's it.

59
00:03:29.487 --> 00:03:32.205
How it works,
you can read about that for a month.

60
00:03:35.135 --> 00:03:38.222
But it is very interesting.

61
00:03:38.222 --> 00:03:41.566
Okay, how that relates to semantic search?

62
00:03:41.566 --> 00:03:45.810
Well, like I said,
if everything is a number now,

63
00:03:45.810 --> 00:03:49.877
and it's dimensional,
we can put it on a graph.

64
00:03:49.877 --> 00:03:51.882
Actually, there we go,
I have a visualization here.

65
00:03:51.882 --> 00:03:54.809
So okay, think about it like this.

66
00:03:54.809 --> 00:03:56.321
We have these different colors, right?

67
00:03:56.321 --> 00:03:59.355
Orange is animal, green is athlete,
this blue is film,

68
00:03:59.355 --> 00:04:02.404
this purple's transportation,
this pink is village.

69
00:04:02.404 --> 00:04:05.974
And when we convert those
words into vectors, and

70
00:04:05.974 --> 00:04:09.036
we have embeddings, and we plotted them.

71
00:04:09.036 --> 00:04:14.680
This is just a three-dimensional graph,
but remember it's over 1,000 dimensions.

72
00:04:14.680 --> 00:04:17.224
So, [LAUGH] I don't know how to
visually represent that to you.

73
00:04:17.224 --> 00:04:20.836
So here's a three-dimensional
representation of it to keep it simple.

74
00:04:20.836 --> 00:04:25.504
You can see how they plot within
that three-dimensional space.

75
00:04:25.504 --> 00:04:29.198
And from there, because we have an x,
y, and z in this case,

76
00:04:29.198 --> 00:04:32.253
we can do math to see how
close things are, right?

77
00:04:32.253 --> 00:04:35.062
So how close is this to this?

78
00:04:35.062 --> 00:04:37.545
How close is this one to this one, right?

79
00:04:37.545 --> 00:04:40.810
And the closer they are in that graph,

80
00:04:40.810 --> 00:04:45.515
the closer they are in semantics and
meaning, right?

81
00:04:45.515 --> 00:04:50.503
So that's how we're able to determine
whether or not something sounds the same

82
00:04:50.503 --> 00:04:55.748
or is in relation, so, That's why
it's important for semantics search.

83
00:04:55.748 --> 00:04:58.720
Because without that, we would not
know the thing that you asked and

84
00:04:58.720 --> 00:05:01.133
how it relates to the thing
that you're searching on.

85
00:05:01.133 --> 00:05:02.440
So that's how this works.

86
00:05:02.440 --> 00:05:07.597
So the flow is, basically,
take the things that you wanna index,

87
00:05:07.597 --> 00:05:11.480
convert them to embeddings and
vectors, right?

88
00:05:12.620 --> 00:05:15.760
Put them in something
called a vector database.

89
00:05:15.760 --> 00:05:18.040
So there are specific databases, or

90
00:05:18.040 --> 00:05:23.820
a lot of databases have plugins that you
can then enhance to make vector databases.

91
00:05:23.820 --> 00:05:26.270
That way we can store these
arrays of numbers, right?

92
00:05:26.270 --> 00:05:29.106
These vectors, these embeddings,
we can store them.

93
00:05:29.106 --> 00:05:32.066
And then from there,
when a query comes in,

94
00:05:32.066 --> 00:05:35.276
I want to convert that
query also into a vector.

95
00:05:35.276 --> 00:05:40.285
And I'm gonna put it in the same space
as the things that I want a search on.

96
00:05:40.285 --> 00:05:43.267
So in here, all the things I wanna
search on, I'll take my query,

97
00:05:43.267 --> 00:05:44.562
I'll also throw it in here.

98
00:05:44.562 --> 00:05:45.525
Why would I do that?

99
00:05:45.525 --> 00:05:48.910
Because I need to, because I need to
see what is close to this query, right?

100
00:05:48.910 --> 00:05:52.277
So if I take my query, and
it lands over here, okay,

101
00:05:52.277 --> 00:05:56.270
I need to return the things that
are the closest to it, right?

102
00:05:56.270 --> 00:06:00.161
Because those are the results that
someone's asking for, right, and meaning.

103
00:06:00.161 --> 00:06:04.701
So I convert my query into a vector and
an embedding.

104
00:06:04.701 --> 00:06:08.026
And then I should get back all
of the things closest to it,

105
00:06:08.026 --> 00:06:10.469
and then I can just show those as results.

106
00:06:10.469 --> 00:06:14.365
Like, is this what you meant?

107
00:06:14.365 --> 00:06:16.521
I can show the closest things in meaning.

108
00:06:16.521 --> 00:06:19.279
And that's basically how it works.

109
00:06:19.279 --> 00:06:23.558
So it sounds really complicated,
[LAUGH] but luckily,

110
00:06:23.558 --> 00:06:29.431
we'll be using some SDKs and tools to
help us make that not as complicated.

111
00:06:29.431 --> 00:06:32.137
That was a lot, I promise this is
the hardest part of today, and

112
00:06:32.137 --> 00:06:34.550
you don't even need to understand this.

113
00:06:34.550 --> 00:06:38.762
This is just something I feel like it's
impressive to know, if you're not an AI,

114
00:06:38.762 --> 00:06:39.992
to know how this works.

115
00:06:39.992 --> 00:06:45.694
Because this is actually the root,
this is what's happening right

116
00:06:45.694 --> 00:06:50.706
now with generative AI and LLMs,
it all comes down to this.

117
00:06:50.706 --> 00:06:55.169
I mean there are like, I don't know, $100
million companies now whose main purpose

118
00:06:55.169 --> 00:06:57.800
is to build a database to save this now,
that's it.

119
00:06:57.800 --> 00:07:00.620
That's all they do, it's just like,
we're gonna save vectors.

120
00:07:00.620 --> 00:07:01.566
And it's a huge market.

121
00:07:01.566 --> 00:07:05.244
So I think this is gonna be so
much common in the future.

122
00:07:05.244 --> 00:07:08.075
When you go pick your database for
your next app, I'm using Postgres,

123
00:07:08.075 --> 00:07:11.430
you'll be considering,
does this thing support vectors, or not?

124
00:07:11.430 --> 00:07:15.966
Does it support co-similarity search,
which we'll talk about soon, or

125
00:07:15.966 --> 00:07:17.696
cosine similarity search.

126
00:07:17.696 --> 00:07:20.974
You're gonna have to think about that
because you know that you're gonna be

127
00:07:20.974 --> 00:07:22.517
using the AI somewhere in your app.

128
00:07:22.517 --> 00:07:26.752
And you need the ability to save these
things, so I want you to be aware of them.

129
00:07:26.752 --> 00:07:29.913
But you don't really need
to know how all this works.

130
00:07:32.810 --> 00:07:37.945
&gt;&gt; They didn't, PlanetScale,
just fork MySQL or something?

131
00:07:37.945 --> 00:07:39.756
&gt;&gt; [CROSSTALK]
&gt;&gt; PlanetScale did, yeah.

132
00:07:39.756 --> 00:07:42.750
So the story about that is, PlanetScale,

133
00:07:42.750 --> 00:07:47.088
they already had internal fork
that they use of MySQL anyway.

134
00:07:47.088 --> 00:07:51.396
Cuz MySQL was just slow to update,
so I already had one internally.

135
00:07:51.396 --> 00:07:55.826
So it wasn't hard for them to fork it
to add support for vectors, but, yes,

136
00:07:55.826 --> 00:07:56.462
they did.

137
00:07:56.462 --> 00:08:01.112
Because if you go look at Postgres, which
is a better alternative, in my opinion.

138
00:08:01.112 --> 00:08:07.016
So MySQL, they have,
their competitor, NeonDB,

139
00:08:07.016 --> 00:08:13.065
which is basically plan at scale,
but for Postgres.

140
00:08:13.065 --> 00:08:21.050
Postgres already has a vector plug-in, so
therefore NeonDB had a vector plug-in.

141
00:08:21.050 --> 00:08:22.581
PlanetScale is like,

142
00:08:22.581 --> 00:08:26.918
our competitor is getting a lot of
customers that have needs for vector data.

143
00:08:26.918 --> 00:08:28.753
And then MySQL wasn't being
updated anytime soon.

144
00:08:28.753 --> 00:08:31.960
So yeah, they just forked it again,
added a vector plug into it.

145
00:08:31.960 --> 00:08:36.902
So now you can support these vector
embeddings inside of a MySQL database on

146
00:08:36.902 --> 00:08:39.580
PlanetScale, which is great for them.

147
00:08:39.580 --> 00:08:47.022
So yeah, it is a serious thing, and
there is a huge list of, let's see,

148
00:08:47.022 --> 00:08:52.670
let me find it right quick,
there's a huge list.

149
00:08:52.670 --> 00:08:55.541
Here's a list of all
the different vector stores.

150
00:08:55.541 --> 00:08:58.147
This is way bigger than
the last time I used this.

151
00:08:58.147 --> 00:09:03.187
These are just databases or
stores that support vector embeddings.

152
00:09:03.187 --> 00:09:06.447
Last time when I used this
there was like five here.

153
00:09:06.447 --> 00:09:10.246
And I'm sure this is even a huge list,
I'm sure it's bigger than this.

154
00:09:10.246 --> 00:09:12.937
But some of the big ones
are like Chroma is open source.

155
00:09:15.963 --> 00:09:18.895
The biggest one is,
I'll know it when I see it.

156
00:09:18.895 --> 00:09:21.323
I don't know what it's called, but, yeah,

157
00:09:21.323 --> 00:09:23.980
Superbase supports it now
because it's got Postgres.

158
00:09:23.980 --> 00:09:28.780
Vectara is good, yeah, Weaviate is
a big one, but there's the biggest one,

159
00:09:28.780 --> 00:09:32.510
I can't think of it right now,
but it's on here somewhere.

160
00:09:32.510 --> 00:09:34.730
They're raising a bunch of money or
whatever.

161
00:09:34.730 --> 00:09:36.100
But yeah, you have tons of options.

162
00:09:36.100 --> 00:09:38.008
So vector stores, any questions?

163
00:09:40.830 --> 00:09:48.600
&gt;&gt; Just resources on going further
with high-dimensional vectors.

164
00:09:48.600 --> 00:09:52.600
&gt;&gt; Honestly, the best way that I
learned it, there were two resources.

165
00:09:52.600 --> 00:09:58.225
ChatGPT, [LAUGH] I just asked
the ChatGPT about, teach me this stuff.

166
00:09:58.225 --> 00:10:03.522
It actually created a lesson plan
on vector data and embeddings,

167
00:10:03.522 --> 00:10:08.646
and how it works on a math level,
and then on a practical level.

168
00:10:08.646 --> 00:10:13.090
I had it do that, and then I also had it,
I was like, write me a quiz for it.

169
00:10:13.090 --> 00:10:17.459
And it wrote me a quiz on it, and
it taught me a lot, so that was one way.

170
00:10:17.459 --> 00:10:24.563
And then too, Open AI's blog
actually talks about it a lot.

171
00:10:24.563 --> 00:10:30.845
So if you go to their blog, and
I'll just say blog, and then embedding.

172
00:10:32.863 --> 00:10:34.833
They have two or three blog posts in here.

173
00:10:34.833 --> 00:10:36.815
And you'll see I got some
of the imagery from here.

174
00:10:36.815 --> 00:10:38.384
They have some really good imagery here.

175
00:10:38.384 --> 00:10:43.830
That talks about that,
how it works on that level.

176
00:10:43.830 --> 00:10:48.391
It's very good, it's very comprehensive,
it doesn't get too deep in the science,

177
00:10:48.391 --> 00:10:50.805
but it makes sense on
the practical levels.

178
00:10:50.805 --> 00:10:53.636
So you can see this is where I got that.

179
00:10:53.636 --> 00:10:55.710
It's interactive on here,
that's really cool.

180
00:10:55.710 --> 00:10:58.924
I didn't even know that,
this is nuts, who made this?

181
00:10:58.924 --> 00:11:04.722
[LAUGH] What psychopath sat down,
was like, I'm gonna make this, and did it?

182
00:11:04.722 --> 00:11:07.121
Good for that person.

183
00:11:07.121 --> 00:11:08.591
Yeah, so this is a good resource.

184
00:11:08.591 --> 00:11:11.362
They have other blog posts on here.

185
00:11:11.362 --> 00:11:13.525
YouTube videos are really good.

186
00:11:13.525 --> 00:11:14.744
I mean, honestly,

187
00:11:14.744 --> 00:11:19.492
there's no limit of resources on this
topic cuz of how massive of a topic it is.

188
00:11:19.492 --> 00:11:23.894
So I don't think you would have trouble
finding anything if you just Google, like,

189
00:11:23.894 --> 00:11:25.134
what is a vector store?

190
00:11:25.134 --> 00:11:26.417
What is embeddings?

191
00:11:26.417 --> 00:11:30.989
You'll find tons of resources, but
that's mostly what I did, ChatGPT and

192
00:11:30.989 --> 00:11:32.140
then open AI blog.

193
00:11:32.140 --> 00:11:34.932
And then just playing
around with it over and

194
00:11:34.932 --> 00:11:38.334
over and over again to see
what's actually going on.

195
00:11:38.334 --> 00:11:42.831
A lot of the vector store providers also
have really good content out there, since

196
00:11:42.831 --> 00:11:47.346
they have to, to promote and educate what
vector stores are and things like that.

197
00:11:47.346 --> 00:11:49.779
So they're also very good resources.

