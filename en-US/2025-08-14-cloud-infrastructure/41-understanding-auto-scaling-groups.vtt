WEBVTT

1
00:00:00.039 --> 00:00:03.200
&gt;&gt; Erik Reinert: Okay, so
then we go into the auto scaling group.

2
00:00:03.200 --> 00:00:06.045
And the auto scaling group is
pretty straightforward as well.

3
00:00:06.045 --> 00:00:10.953
It's solving the problems of desired
capacity, meaning how many instances do

4
00:00:10.953 --> 00:00:14.409
we want right now, our min size,
our max size, right?

5
00:00:14.409 --> 00:00:17.950
How many instances do we
wanna scale to and stop at?

6
00:00:17.950 --> 00:00:22.312
In this case, we want to stop at 5,
which means that we have auto scaling and

7
00:00:22.312 --> 00:00:26.060
it'll work, which is fantastic,
but we won't actually go to,

8
00:00:26.060 --> 00:00:28.122
you know, beyond five instances.

9
00:00:28.122 --> 00:00:30.830
So this is kind of helpful when
you want to, like, save money.

10
00:00:30.830 --> 00:00:33.800
You want to make sure that if you
scale all the way to the maximum,

11
00:00:33.800 --> 00:00:36.830
you're not going to 1000 instances or
something like that.

12
00:00:36.830 --> 00:00:39.449
You can at least limit it.

13
00:00:39.449 --> 00:00:41.333
Min size is nice.

14
00:00:41.333 --> 00:00:46.283
If, say, for example, you want more than
one instance, you say, okay, I want 3 min,

15
00:00:46.283 --> 00:00:50.182
5 max, make sure you have high
availability, all that kinda stuff.

16
00:00:50.182 --> 00:00:54.990
Name_prefix, you'll notice
that I have var.name each.key.

17
00:00:54.990 --> 00:01:00.103
So in this case, going back to the whole
passing the name parameter thing,

18
00:01:00.103 --> 00:01:05.719
this would be staging, spot staging,
on-demand staging, whatever, right?

19
00:01:05.719 --> 00:01:09.254
But we're combining these two
together to make the actual name of

20
00:01:09.254 --> 00:01:11.570
the autoscaling group.

21
00:01:11.570 --> 00:01:12.962
We add our launch_template,

22
00:01:12.962 --> 00:01:17.250
we tell it that we always wanna use
the latest version of the launch template.

23
00:01:17.250 --> 00:01:19.090
Instance_refresh is kinda nice.

24
00:01:19.090 --> 00:01:22.930
This will just make sure that your
instances properly get rolled.

25
00:01:22.930 --> 00:01:27.864
So for example, if you have 100 instances
and you don't wanna just take them all

26
00:01:27.864 --> 00:01:32.798
down at once, you can tell it, like, hey,
I wanna make sure that at least half of my

27
00:01:32.798 --> 00:01:36.740
instances are available so
it'll roll like the first 50.

28
00:01:36.740 --> 00:01:41.844
Make sure that all those roll successfully
before moving to the next 50, whatever.

29
00:01:41.844 --> 00:01:45.470
So that percentage is important
to make sure that you don't

30
00:01:45.470 --> 00:01:47.580
ever completely go down, right?

31
00:01:47.580 --> 00:01:49.780
Lose all of your instances or
anything like that.

32
00:01:51.300 --> 00:01:53.299
We then add a couple of tags.

33
00:01:53.299 --> 00:01:57.300
So we just tell it that we want to
add the Amazon ECS managed tag.

34
00:01:57.300 --> 00:01:59.738
This is something specific to Amazon E.

35
00:01:59.738 --> 00:02:04.160
This is something that they recommend
you do for their clustering.

36
00:02:04.160 --> 00:02:05.984
And then we just give it a name so

37
00:02:05.984 --> 00:02:09.360
that we know that the instance
is the name of the cluster.

38
00:02:10.720 --> 00:02:13.200
And then we have an autoscaling_policy.

39
00:02:13.200 --> 00:02:18.023
Now, I thought about this a little
bit because we are making one

40
00:02:18.023 --> 00:02:19.760
significant change.

41
00:02:21.040 --> 00:02:25.380
If we move from App Runner to ecs,

42
00:02:25.380 --> 00:02:29.565
how we scale is going to change,

43
00:02:29.565 --> 00:02:34.991
what that means is at
least out of the box.

44
00:02:34.991 --> 00:02:41.927
The easy solution, which is what we're
going for right now, the easy solution,

45
00:02:41.927 --> 00:02:46.954
what that means is that it will
only track CPU utilization,

46
00:02:46.954 --> 00:02:53.110
whereas in App Runner we got request
concurrency scaling in App Runner.

47
00:02:53.110 --> 00:02:57.485
We got a little bit more
of a tunable scalable

48
00:02:57.485 --> 00:03:01.630
metric if we're serving HTTP requests.

49
00:03:01.630 --> 00:03:03.806
Because then we could say, well,

50
00:03:03.806 --> 00:03:08.430
it's not necessarily that I want
the CPU to go to 75 or 80%.

51
00:03:08.430 --> 00:03:13.827
It's more that I wanna scale off of 1,000
requests a second or 10,000 requests

52
00:03:13.827 --> 00:03:18.810
a second, because that's what I know this
service can process reliably, right?

53
00:03:19.930 --> 00:03:25.032
Because we're doing a CPU tracking target,
it means that we are now

54
00:03:25.032 --> 00:03:30.134
moving to a model of just hammer this
thing as hard as you possibly can

55
00:03:30.134 --> 00:03:35.530
until I need another thing for
you to hammer as hard as you possibly can.

56
00:03:36.730 --> 00:03:39.931
I think this is better.

57
00:03:39.931 --> 00:03:45.790
I know that request count is important,
but I think in this case

58
00:03:45.790 --> 00:03:51.873
you want to worry more about just
the resources you're paying for

59
00:03:51.873 --> 00:03:56.620
versus the idle processes
that are on them.

60
00:03:56.620 --> 00:04:00.065
And what I mean by that
is if you're paying for

61
00:04:00.065 --> 00:04:04.964
an EC2 instance that's like four cores and
eight gigs of RAM and

62
00:04:04.964 --> 00:04:09.953
you're scaling out only when you
use two out of those four cores,

63
00:04:09.953 --> 00:04:14.783
then you're paying for
two cores that you're not using ever.

64
00:04:14.783 --> 00:04:17.452
But when you're managing
infrastructure like this,

65
00:04:17.452 --> 00:04:21.338
you kind of want to look at these as like
resource pools that you're utilizing to

66
00:04:21.338 --> 00:04:23.030
your maximum as much as possible.

67
00:04:24.390 --> 00:04:28.906
So I would make the slight argument
that you should really be developing

68
00:04:28.906 --> 00:04:34.230
applications that scale their
requests as performantly as possible.

69
00:04:34.230 --> 00:04:40.630
So your real issue is CPU utilization and
not saying, well, it doesn't.

70
00:04:40.630 --> 00:04:42.682
You know, I know it was written in Go, but

71
00:04:42.682 --> 00:04:46.390
it can only take 100 requests per
second because of how we've coded it.

72
00:04:46.390 --> 00:04:47.632
It's like, well,

73
00:04:47.632 --> 00:04:52.790
then we're spending a ton of money on
resources that we are never going to use.

74
00:04:52.790 --> 00:04:55.278
Because now I have to
create a new instance for

75
00:04:55.278 --> 00:05:01.030
every 100 requests versus being able to
take 10,000 requests on a single instance.

76
00:05:01.030 --> 00:05:04.240
So, yeah, and I'll be honest with you,
I deal with this.

77
00:05:04.240 --> 00:05:07.007
This is something I deal with quite often.

78
00:05:07.007 --> 00:05:11.204
There was a scenario not too,
well, I would say a while ago,

79
00:05:11.204 --> 00:05:14.168
where we discovered, [LAUGH] we discovered

80
00:05:14.168 --> 00:05:19.440
that we were provisioning 2 core
4 gigabyte instances, right?

81
00:05:19.440 --> 00:05:24.720
And we were provisioning six,
six DaemonSets on them.

82
00:05:24.720 --> 00:05:27.810
Now normally you'd be like, well,
you know, that's not that big a deal.

83
00:05:27.810 --> 00:05:33.340
A DaemonSets probably 0.25 of a core,
right?

84
00:05:33.340 --> 00:05:35.850
But when you provision six of them,

85
00:05:35.850 --> 00:05:40.700
[LAUGH] that's the equivalent of
almost a core and a half, right?

86
00:05:40.700 --> 00:05:45.526
And what we didn't realize was,
as we were Provisioning so

87
00:05:45.526 --> 00:05:51.318
small of instances that we were
provisioning like 75% Daemonset and

88
00:05:51.318 --> 00:05:57.993
then only like 25% of space was available
for our service that we wanted to run.

89
00:05:57.993 --> 00:06:02.152
So we were basically paying for
daemon sets more than we were paying for

90
00:06:02.152 --> 00:06:03.504
running our service.

91
00:06:03.504 --> 00:06:07.133
What that meant was we had to
provision bigger instances or

92
00:06:07.133 --> 00:06:10.922
we had to get rid of daemon sets
that were on those instances.

93
00:06:10.922 --> 00:06:15.757
That's what I mean by having the ability
to really optimize off of what you're

94
00:06:15.757 --> 00:06:16.890
paying for.

95
00:06:16.890 --> 00:06:19.574
In this case, when you're at this level,

96
00:06:19.574 --> 00:06:22.570
you're really just paying for
CPU and memory.

97
00:06:23.710 --> 00:06:27.786
And so if you can get it to a point to
where you don't care about how many

98
00:06:27.786 --> 00:06:30.811
requests per second are going
to the application,

99
00:06:30.811 --> 00:06:34.350
you can just simply say
the application needs to be scaled.

100
00:06:34.350 --> 00:06:36.591
When the compute gets to this level,

101
00:06:36.591 --> 00:06:39.710
then that resource management
becomes very easy.

102
00:06:39.710 --> 00:06:42.550
Then you just go, okay,
75% of cores are being used.

103
00:06:42.550 --> 00:06:45.390
Great, give me a new
instance.75 on that one too.

104
00:06:45.390 --> 00:06:46.670
Great, give me a new instance.

105
00:06:46.670 --> 00:06:49.460
And you just keep doing that, just keep
doing that until it gets bigger and

106
00:06:49.460 --> 00:06:50.230
bigger and bigger.

107
00:06:50.230 --> 00:06:55.890
And then eventually it'll like accordion,
and then it'll go back down, right?

108
00:06:55.890 --> 00:06:58.530
And that's the real approach
I think you should take.

109
00:06:58.530 --> 00:07:01.610
So I did keep with the CPU utilization.

110
00:07:01.610 --> 00:07:06.456
I do think that the request count going
back to the App Runner comparison

111
00:07:06.456 --> 00:07:09.370
can also be a little scheme Y.

112
00:07:09.370 --> 00:07:14.927
The reason why I say that is because if
you're scaling at a value that's far,

113
00:07:14.927 --> 00:07:19.202
far too low for
what your application can actually handle,

114
00:07:19.202 --> 00:07:22.391
then you are over
provisioning quite a bit.

115
00:07:22.391 --> 00:07:25.732
Say, for example,
your application can actually take

116
00:07:25.732 --> 00:07:31.440
10,000 requests on app Runner, but
then you tell it to scale every hundred.

117
00:07:31.440 --> 00:07:36.164
Now you're paying for tons of instances,
but you have more potential there.

118
00:07:36.164 --> 00:07:38.697
That request per second metric,

119
00:07:38.697 --> 00:07:43.678
it doesn't always mean it's the best
metric to use in this case.

120
00:07:43.678 --> 00:07:48.270
Then the last couple things we have
here are the capacity provider and

121
00:07:48.270 --> 00:07:50.024
the capacity providers.

122
00:07:50.024 --> 00:07:55.225
The easiest way to describe these are
these are just attaching the clusters to,

123
00:07:55.225 --> 00:07:58.660
or I'm sorry,
the instances to the clusters.

124
00:07:58.660 --> 00:08:02.252
So the auto scaling groups
are basically the providers and

125
00:08:02.252 --> 00:08:06.020
then the ECS instance or
the ECS cluster is our cluster.

126
00:08:06.020 --> 00:08:10.012
And so we use these two resources
to bind the auto scaling groups and

127
00:08:10.012 --> 00:08:13.577
capacity providers to the specific
clusters that we want,

128
00:08:13.577 --> 00:08:18.660
because you can create capacity providers
and then you can assign them to clusters.

129
00:08:19.750 --> 00:08:26.860
And so that's really all we're
doing here at the bottom.

