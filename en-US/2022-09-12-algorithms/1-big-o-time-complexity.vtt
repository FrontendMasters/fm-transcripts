WEBVTT

1
00:00:00.000 --> 00:00:04.158
We're gonna first tackle Big O cuz I do
feel like Big O is kinda that one thing

2
00:00:04.158 --> 00:00:08.316
that if you don't know it just means
I'm gonna be saying a bunch of nonsense

3
00:00:08.316 --> 00:00:08.858
up here.

4
00:00:08.858 --> 00:00:12.245
And it just kinda sucks because then
we keep talking about it you don't know

5
00:00:12.245 --> 00:00:12.791
what it is.

6
00:00:12.791 --> 00:00:17.547
So Big O is the easiest way to put it
is that it categorizes your algorithm

7
00:00:17.547 --> 00:00:20.020
on time or memory based on the input.

8
00:00:20.020 --> 00:00:21.419
It's not meant to be an exact measurement.

9
00:00:21.419 --> 00:00:26.462
So if someone someone's not gonna say your
algorithm is gonna take 450 CPU units.

10
00:00:26.462 --> 00:00:28.725
You're gonna first say,
well, what's the CPU unit?

11
00:00:28.725 --> 00:00:31.114
And second, how did you come
up with that exact number?

12
00:00:31.114 --> 00:00:36.309
Instead it's a generalized way for
you to be able to understand how your

13
00:00:36.309 --> 00:00:41.257
algorithm will react as your input or
the things you are using grow.

14
00:00:41.257 --> 00:00:44.667
And so if someone says, this is Of N,

15
00:00:44.667 --> 00:00:49.951
what they mean is your algorithm is
a grows linearly based on input.

16
00:00:49.951 --> 00:00:51.549
We'll go over some more.

17
00:00:51.549 --> 00:00:55.740
And so why do we use it well it often
helps us make decisions on why you should

18
00:00:55.740 --> 00:00:58.318
or should not use
a specific data structure.

19
00:00:58.318 --> 00:01:02.156
You will see as we do this data
structures progressively make

20
00:01:02.156 --> 00:01:05.852
these constraints to make them more and
more performant.

21
00:01:05.852 --> 00:01:10.850
But if you use them incorrectly they
become massively in performance,

22
00:01:10.850 --> 00:01:16.097
I don't I don't know what the opposite
of performance is in performance,

23
00:01:16.097 --> 00:01:17.028
there we go.

24
00:01:17.028 --> 00:01:20.131
So here, we're gonna take
a quick code example right here.

25
00:01:20.131 --> 00:01:22.998
For those who know Big O,
this is a breeze you know it immediately.

26
00:01:22.998 --> 00:01:25.466
So if you don't know Big O,
this would actually be really hard.

27
00:01:25.466 --> 00:01:29.272
You wouldn't have a way to be able to kind
of describe what is the running time of

28
00:01:29.272 --> 00:01:29.857
this code.

29
00:01:29.857 --> 00:01:31.609
You wouldn't actually have the lingo.

30
00:01:31.609 --> 00:01:32.915
So hopefully we can do that.

31
00:01:32.915 --> 00:01:34.422
We can get this lingo going.

32
00:01:34.422 --> 00:01:38.431
To say it differently Big O as your input
grows, how fast is your computation or

33
00:01:38.431 --> 00:01:39.300
memory grille?

34
00:01:39.300 --> 00:01:41.598
So that's the most important concept.

35
00:01:41.598 --> 00:01:44.876
Number one,
growth is with respect to input.

36
00:01:44.876 --> 00:01:46.904
So just remember that,
put that in your head.

37
00:01:46.904 --> 00:01:51.596
Obviously in the real world, this is not a
real trade off in the sense that depending

38
00:01:51.596 --> 00:01:55.886
on the size of data you use, depending
on how much memory you have to allocate

39
00:01:55.886 --> 00:02:00.533
depending on the GC pressure, these things
are not necessarily free trade offs.

40
00:02:00.533 --> 00:02:05.041
You can't really trade time for memory,
because it takes time to create memory.

41
00:02:05.041 --> 00:02:08.616
So if you create a linear amount of memory
in some sense your algorithm is bound

42
00:02:08.616 --> 00:02:10.053
by how much memory you create.

43
00:02:10.053 --> 00:02:13.527
So it's not necessarily for free.

44
00:02:13.527 --> 00:02:14.933
So let's go back to our example.

45
00:02:14.933 --> 00:02:18.895
So how does our program
grow with respect to input?

46
00:02:18.895 --> 00:02:21.329
Well, I conveniently named
it n intentionally, but

47
00:02:21.329 --> 00:02:22.579
notice that it is a string.

48
00:02:22.579 --> 00:02:26.254
So it has a length and has a series
of characters associated with it.

49
00:02:26.254 --> 00:02:29.367
Strings are effectively arrays
really when you think about it.

50
00:02:29.367 --> 00:02:30.887
But anyways, so here we go.

51
00:02:30.887 --> 00:02:33.868
So if you look at the for loop,
what you'll see is that the for

52
00:02:33.868 --> 00:02:36.102
loop has to execute
the length of the string.

53
00:02:36.102 --> 00:02:42.737
That means if our string grows by 50%,
how much slower is our function, 50%.

54
00:02:42.737 --> 00:02:44.121
It grows linearly.

55
00:02:44.121 --> 00:02:49.542
For every one more unit of string,
there is one more loop that it has to do.

56
00:02:49.542 --> 00:02:52.355
So there you go,
that is the simplest way to put it.

57
00:02:52.355 --> 00:02:57.045
And for those who did not see that, it's
not very obvious, but for me, that's over,

58
00:02:57.045 --> 00:02:58.665
and you can see it right away.

59
00:02:58.665 --> 00:03:02.936
So how can you tell, simplest trick and
all of it is just simply look for loops.

60
00:03:02.936 --> 00:03:04.951
Where do you loop over the input?

61
00:03:04.951 --> 00:03:07.818
If you see that,
easiest way to kind of tell,

62
00:03:07.818 --> 00:03:10.540
the Big O complexity of anything, right?

63
00:03:10.540 --> 00:03:14.461
So when we look at this, you'll notice
that I now put a new function and

64
00:03:14.461 --> 00:03:18.981
we have a function that goes over and sums
all the things that are happening here in

65
00:03:18.981 --> 00:03:21.129
this string, and then we do it again.

66
00:03:21.129 --> 00:03:25.472
What's the running time of this function,
anyone?

67
00:03:25.472 --> 00:03:26.365
&gt;&gt; Of N.

68
00:03:26.365 --> 00:03:30.602
&gt;&gt; Okay, we got one Of N, anything else
cuz we do a sum and then we do some again.

69
00:03:30.602 --> 00:03:33.907
I was really hoping can
someone just say 2 of N,

70
00:03:33.907 --> 00:03:37.641
2 of N hanging on Of 2 of N,
okay, you're right.

71
00:03:37.641 --> 00:03:38.902
What's the running time?

72
00:03:38.902 --> 00:03:40.202
Right, no it's not that.

73
00:03:40.202 --> 00:03:41.928
It's actually not that at all.

74
00:03:41.928 --> 00:03:45.883
Second most important concept is you
always drop constants, the reason why

75
00:03:45.883 --> 00:03:49.553
is here's a very good example of
why you don't care about constants.

76
00:03:49.553 --> 00:03:52.234
Though practically speaking
constants are extremely important.

77
00:03:52.234 --> 00:03:54.628
Theoretically they're not important,

78
00:03:54.628 --> 00:03:57.907
if you had something that
grew at 10N versus N squared,

79
00:03:57.907 --> 00:04:02.301
you'd clearly see as we get down here
N squared just gets massively larger.

80
00:04:02.301 --> 00:04:07.199
It grows disproportionately fast
in comparison to no matter what

81
00:04:07.199 --> 00:04:10.323
constant is in front of the linear Of N.

82
00:04:10.323 --> 00:04:14.103
So yeah, that is why constants aren't
necessarily important cuz again,

83
00:04:14.103 --> 00:04:15.965
we're not trying to get exact time.

84
00:04:15.965 --> 00:04:19.525
We're not trying to get this is how
many units of CPU you need to use.

85
00:04:19.525 --> 00:04:21.244
It's how does it grow?

86
00:04:21.244 --> 00:04:24.137
If I'm gonna give this thing 10,000 as N,

87
00:04:24.137 --> 00:04:27.748
is it gonna hold my computer or
am I gonna be able to do it fast?

88
00:04:27.748 --> 00:04:28.739
Is it going to be instant?

89
00:04:28.739 --> 00:04:31.786
I need to kinda just generally
know where it's gonna be.

90
00:04:31.786 --> 00:04:36.329
And so that's kinda the heart
of Big O if you will, all right?

91
00:04:36.329 --> 00:04:39.962
And I've talked about this,
right practical versus theoretical.

92
00:04:39.962 --> 00:04:42.847
Yes, Of N is faster than Of N squared.

93
00:04:42.847 --> 00:04:47.073
But just like in sorting algorithms,
often it will use insertion sort for

94
00:04:47.073 --> 00:04:48.582
smaller subsets of data.

95
00:04:48.582 --> 00:04:53.236
Because insertion sort, though slower in
theoretical terms, cuz it's N squared,

96
00:04:53.236 --> 00:04:57.563
is actually faster than say quicksort,
which is log of N when it comes to small,

97
00:04:57.563 --> 00:04:58.229
datasets.

98
00:04:58.229 --> 00:05:01.776
Because practically speaking,
sometimes things that are N squared or

99
00:05:01.776 --> 00:05:02.965
faster than N, right?

100
00:05:02.965 --> 00:05:04.464
Because your N is sufficiently small.

101
00:05:04.464 --> 00:05:09.259
And the constant that is dropped is larger
enough that it actually makes real impact.

102
00:05:09.259 --> 00:05:11.861
All right, so
let's do another example right here.

103
00:05:11.861 --> 00:05:14.299
We're gonna go over the entire
length of the string.

104
00:05:14.299 --> 00:05:17.060
And if we happen to run into a capital E,

105
00:05:17.060 --> 00:05:20.760
we are gonna return whatever
we have at that moment.

106
00:05:20.760 --> 00:05:23.486
And so what is the running
time of this algorithm?

107
00:05:27.028 --> 00:05:28.925
&gt;&gt; Of N.

108
00:05:28.925 --> 00:05:32.647
&gt;&gt; Of N would be correct, good job.

109
00:05:32.647 --> 00:05:36.358
The reason why we do that is that
often we consider worst case.

110
00:05:36.358 --> 00:05:39.633
And so when you're in interviews you're
gonna pretty much exclusively describe

111
00:05:39.633 --> 00:05:40.161
worst case.

112
00:05:40.161 --> 00:05:42.321
And if you think about it
a string without a Capital E,

113
00:05:42.321 --> 00:05:44.253
how much of the string
are you gonna go through?

114
00:05:44.253 --> 00:05:45.493
Well, you're gonna go through all of it.

115
00:05:45.493 --> 00:05:49.466
A string with a capital E at the end or
maybe one unit from the end or

116
00:05:49.466 --> 00:05:53.235
two units from the end,
you're gonna get big O of N minus 2.

117
00:05:53.235 --> 00:05:55.955
Well, that's just simply N,
we drop all constants there you go.

118
00:05:55.955 --> 00:05:59.309
It's always N because it
doesn't matter where that E is.

119
00:05:59.309 --> 00:06:02.201
Practically speaking it could be in
the middle it could be at the end.

120
00:06:02.201 --> 00:06:05.839
And if it's one half N
still again it's just N.

121
00:06:05.839 --> 00:06:11.329
So the three important concepts I want to
drive home for time and space complexity

122
00:06:11.329 --> 00:06:16.433
are these, growth with respect to input,
constants are always dropped.

123
00:06:16.433 --> 00:06:21.097
Whenever you're pretty much doing in
an interview, it's worst case scenario,

124
00:06:21.097 --> 00:06:24.761
rarely are they gonna ask you for
best case or even average case.

125
00:06:24.761 --> 00:06:28.255
Now there are some algorithms that it
makes more sense to reason about the best

126
00:06:28.255 --> 00:06:31.692
case and the average, case and we will
go through some of those today because

127
00:06:31.692 --> 00:06:35.044
they actually change and there actually
is a variety of runtimes for him.

128
00:06:35.044 --> 00:06:38.593
But, at least whenever
I've done interviewing,

129
00:06:38.593 --> 00:06:41.983
I have never once been asked for
best or average.

130
00:06:41.983 --> 00:06:44.286
And I've conducted about
200 interviews and

131
00:06:44.286 --> 00:06:46.936
I've been in about 50
interviews I've never had it.

132
00:06:46.936 --> 00:06:50.803
So I just assume most people
here aren't ever gonna happen.

133
00:06:50.803 --> 00:06:53.064
So here's some common
complexities you will see.

134
00:06:53.064 --> 00:06:56.529
You got right down here in the ones
that you can't really see there's

135
00:06:56.529 --> 00:07:00.487
actually two lines right here,
which is Of 1, which means constant time.

136
00:07:00.487 --> 00:07:02.763
Doesn't matter how big the input is,

137
00:07:02.763 --> 00:07:07.760
it does the same set of operations every
single time it's instant, effectively.

138
00:07:07.760 --> 00:07:11.511
Now that constant, that's in front of
the one, can be a very large number but

139
00:07:11.511 --> 00:07:15.283
it doesn't matter cuz it's the same
constant no matter how big the input is.

140
00:07:15.283 --> 00:07:18.289
Then there's log of N,
that's base two log for

141
00:07:18.289 --> 00:07:22.664
those that forgot what log means,
don't worry I failed it on my ACT.

142
00:07:22.664 --> 00:07:25.117
I'm sure we all forget
it at some points but

143
00:07:25.117 --> 00:07:28.472
we'll kinda go over it more log
of N being the screen line.

144
00:07:28.472 --> 00:07:32.328
You can see it grows linearly even
though this graph is rectangular.

145
00:07:32.328 --> 00:07:38.606
So it doesn't look very linear, log of
N very common running time, you'll see.

146
00:07:38.606 --> 00:07:41.335
N squared, you can see that it
starts becoming kind of ridiculous.

147
00:07:41.335 --> 00:07:42.351
It grows pretty fast.

148
00:07:42.351 --> 00:07:46.530
And the last two are effectively
algorithms that can't run on traditional

149
00:07:46.530 --> 00:07:47.268
computers.

150
00:07:47.268 --> 00:07:50.217
I don't know if it can run in the quantum
realm cuz I'm in the quantum realm is

151
00:07:50.217 --> 00:07:51.430
practically magical as it is.

152
00:07:51.430 --> 00:07:56.235
So whatever realm that is I don't know
about that but traditionally speaking,

153
00:07:56.235 --> 00:07:59.760
you can't solve the traveling salesman for
12 cities.

154
00:07:59.760 --> 00:08:03.596
That is a algorithm that runs in
the last two time range, right?

155
00:08:03.596 --> 00:08:07.496
It's just effectively impractical on
a computer, will take thousands of years.

156
00:08:07.496 --> 00:08:09.584
All right, so,
let's go over some more examples.

157
00:08:09.584 --> 00:08:12.070
Again, I said count the loops.

158
00:08:12.070 --> 00:08:16.146
So N squared, for
every single character in the string.

159
00:08:16.146 --> 00:08:19.330
We're gonna go over every
single character in the string.

160
00:08:19.330 --> 00:08:22.414
That is like computing
the area of a square, right?

161
00:08:22.414 --> 00:08:24.971
N by N,
it's just a square we're going over.

162
00:08:24.971 --> 00:08:26.507
We're just going down each single line.

163
00:08:26.507 --> 00:08:28.958
You can think of it kinda like
graph that you're going over.

164
00:08:28.958 --> 00:08:32.262
So for every single character we
go over every single character,

165
00:08:32.262 --> 00:08:33.412
same thing for cubed.

166
00:08:33.412 --> 00:08:37.284
For every single character we go over
every single character every single

167
00:08:37.284 --> 00:08:41.281
character we won't pretty much see any
of these type of algorithms today but

168
00:08:41.281 --> 00:08:43.787
that's like multiplying a matrices, right?

169
00:08:43.787 --> 00:08:48.663
If you just simply multiply matrices
together two of them you're gonna get N

170
00:08:48.663 --> 00:08:50.120
cube like algorithm.

171
00:08:50.120 --> 00:08:54.887
Because you have to do
this every single time.

172
00:08:54.887 --> 00:08:57.111
And then you'd have to do
that a bunch more times.

173
00:08:57.111 --> 00:08:58.467
It's very painful.

174
00:08:58.467 --> 00:08:59.135
O(n log n),

175
00:08:59.135 --> 00:09:02.776
we will see some O(n log n) and some
O(log n) algorithms quick sort is one.

176
00:09:02.776 --> 00:09:05.182
It'll make more sense
once you look at it but

177
00:09:05.182 --> 00:09:09.098
effectively how you think about it
is that for every time, you go over.

178
00:09:09.098 --> 00:09:11.602
Half the amount of space
you need to search, but

179
00:09:11.602 --> 00:09:14.488
you need to search the whole
space once for every time.

180
00:09:14.488 --> 00:09:17.661
So, you go over n characters,
then you have how much you need to do,

181
00:09:17.661 --> 00:09:20.740
then you go over n characters you
have how much you need to look at.

182
00:09:20.740 --> 00:09:22.877
And you just do that cuz that's
kind of how I think about.

183
00:09:22.877 --> 00:09:26.364
And then of course log n, you just have
the amount of input you have to search,

184
00:09:26.364 --> 00:09:28.468
but you only need to look
at one point at a time.

185
00:09:28.468 --> 00:09:30.559
And so it eventually goes down to zero.

186
00:09:30.559 --> 00:09:34.832
All right, and then we'll also see
the craziest of all runtimes today once.

187
00:09:34.832 --> 00:09:38.587
I've only seen this in one problem
once very excited about it.

188
00:09:38.587 --> 00:09:41.525
Have scored of N, yes,
I do math.square root as squirt.

189
00:09:41.525 --> 00:09:42.580
I think that's fantastic.

190
00:09:42.580 --> 00:09:47.381
But a score of n it's a very unique one.

191
00:09:47.381 --> 00:09:51.783
And I think actually it does show
a mind blowing trick that you can do.

192
00:09:51.783 --> 00:09:54.916
All right, so why so obviated,
of course, we don't need yet

193
00:09:54.916 --> 00:09:56.380
another Big O explanation.

194
00:09:56.380 --> 00:09:58.482
I just wanted to make sure
we're all on the same page.

195
00:09:58.482 --> 00:10:01.835
Cuz again, the algorithms in this
course are the important part.

196
00:10:01.835 --> 00:10:04.592
It's not necessarily the Big O.

197
00:10:04.592 --> 00:10:08.399
I'd rather have you have the technical
understanding of algorithms than be

198
00:10:08.399 --> 00:10:10.070
able to tell me the running time.

199
00:10:10.070 --> 00:10:13.169
The run time once you kinda get
the trick look for loops, okay,

200
00:10:13.169 --> 00:10:16.227
I can see how we're going over input,
it's easy after that.

201
00:10:16.227 --> 00:10:19.591
Long as you keep these three concepts in
your head, growth with respect to input,

202
00:10:19.591 --> 00:10:22.928
constants are dropped, consider worst
case, you'll do well in an interview.

203
00:10:22.928 --> 00:10:26.613
Long as you can kinda qualify okay,
here's where my loops theoretically are,

204
00:10:26.613 --> 00:10:27.499
we're good to go.

205
00:10:27.499 --> 00:10:32.125
One more note, there are other things that
measure time complexity other than Big O,

206
00:10:32.125 --> 00:10:33.938
just people don't use it right?

207
00:10:33.938 --> 00:10:35.710
They're Stata, there's little omega.

208
00:10:35.710 --> 00:10:38.783
There's all these other ways,
but Big O is the upper bound.

209
00:10:38.783 --> 00:10:42.421
The effectively the least
upper bound you can do.

210
00:10:42.421 --> 00:10:44.561
All right, space, the final frontier,

211
00:10:44.561 --> 00:10:47.632
we are not gonna really be
talking much about space growth.

212
00:10:47.632 --> 00:10:49.423
I will mention it from time to time.

213
00:10:49.423 --> 00:10:50.902
It's just less interesting.

214
00:10:50.902 --> 00:10:52.757
I've gotten it once or
twice in an interview.

215
00:10:52.757 --> 00:10:57.262
But the reality is that people do this
in react, which emotionally bruises me.

216
00:10:57.262 --> 00:11:01.927
And so when I see this I just assume
people don't care necessarily about space

217
00:11:01.927 --> 00:11:03.163
or time sometimes.

218
00:11:03.163 --> 00:11:09.367
And I care about it but
I can't do anything about it.

219
00:11:09.367 --> 00:11:12.470
This may not be the popular way to do it
anymore, react to changes too fast for

220
00:11:12.470 --> 00:11:13.355
me to keep up with it.

221
00:11:13.355 --> 00:11:18.338
I don't really do a lot of front end
stuff, so anyways before we go, questions.

222
00:11:18.338 --> 00:11:20.041
&gt;&gt; What is your favorite algorithm.

223
00:11:20.041 --> 00:11:23.972
&gt;&gt; My favorite algorithm to
implement is probably quicksort.

224
00:11:23.972 --> 00:11:25.025
And I'll explain why.

225
00:11:25.025 --> 00:11:28.425
Cuz I think it's just super duper neat,

226
00:11:28.425 --> 00:11:32.832
practically speaking I think
a ring buffer is just.

227
00:11:35.360 --> 00:11:37.164
It just always is awesome.

228
00:11:37.164 --> 00:11:42.097
And then you can replace a ring buffer
with an array list if you don't need first

229
00:11:42.097 --> 00:11:45.000
in first out behavior, right?

230
00:11:45.000 --> 00:11:46.016
Everyone agree?

231
00:11:46.016 --> 00:11:47.058
By the end of the course,

232
00:11:47.058 --> 00:11:50.346
you will agree with me because then
you'll understand what I'm saying here.

233
00:11:50.346 --> 00:11:54.150
In fact, by the end of today, you should
be able to understand what we're talking

234
00:11:54.150 --> 00:11:57.690
about, yap
&gt;&gt; If I don't know, TypeScript,

235
00:11:57.690 --> 00:12:00.013
can I follow the course?

236
00:12:00.013 --> 00:12:03.445
&gt;&gt; if you don't know TypeScript,
can you follow the course?

237
00:12:03.445 --> 00:12:05.469
Well, I chose TypeScript because the, a,

238
00:12:05.469 --> 00:12:07.671
the typing language is
very straightforward.

239
00:12:07.671 --> 00:12:09.276
We're not gonna do a ton of type script.

240
00:12:09.276 --> 00:12:13.015
So if you know JavaScript, you know
TypeScript, except for, you'll see a,

241
00:12:13.015 --> 00:12:15.861
kind of some foreign characters
in some unique positions.

242
00:12:15.861 --> 00:12:18.059
But it's really the types
that are important.

243
00:12:18.059 --> 00:12:22.117
And so I really wanna make sure I can
type out the types so you can see it.

244
00:12:22.117 --> 00:12:27.175
And I kinda created a nice little project
in which we will all practice typing out

245
00:12:27.175 --> 00:12:32.310
algorithms together and that one is going
to be in TypeScript as well and there's

246
00:12:32.310 --> 00:12:37.317
a reason why I chose that for just the
technical reasons of TypeScript as well.

247
00:12:37.317 --> 00:12:42.187
So you should be able to,
if you're familiar with C or Go or Rust,

248
00:12:42.187 --> 00:12:46.010
TypeScript is fairly easy,
just think C, but easy

