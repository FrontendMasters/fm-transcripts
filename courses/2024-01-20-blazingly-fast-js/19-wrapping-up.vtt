WEBVTT

1
00:00:00.057 --> 00:00:01.473
Hopefully, you guys really did enjoy that.

2
00:00:01.473 --> 00:00:06.185
Because at the end of the day,
no one's gonna care a lot if you make big

3
00:00:06.185 --> 00:00:11.156
performance wins other than people
in a really bad situations, right?

4
00:00:11.156 --> 00:00:14.283
I have someone right now that's just like,
they didn't have a problem,

5
00:00:14.283 --> 00:00:17.361
they didn't have a problem,
they gained 10,000 customers, and

6
00:00:17.361 --> 00:00:19.945
now they can't keep their node
instances up in production.

7
00:00:19.945 --> 00:00:23.554
So at some point it usually goes,
it doesn't matter, it doesn't matter.

8
00:00:23.554 --> 00:00:26.366
Shit, it's the worst thing
we've had in a long time.

9
00:00:26.366 --> 00:00:32.685
And so, if you care early,
it will often lead somewhere good.

10
00:00:32.685 --> 00:00:38.811
&gt;&gt; You didn't necessarily mention this but
I think I heard it somewhere else that had

11
00:00:38.811 --> 00:00:45.047
Bun done anything with async to make that
stuff from an asynchronous fashion bomb?

12
00:00:45.047 --> 00:00:50.048
&gt;&gt; They do have a different event loop,
so it's libuv versus whatever is in SIG.

13
00:00:50.048 --> 00:00:50.954
So is that faster?

14
00:00:50.954 --> 00:00:51.637
Is that slower?

15
00:00:51.637 --> 00:00:53.249
Probably, not a lot difference.

16
00:00:53.249 --> 00:00:55.670
And if it is, for whatever reason,
one is fast than the other,

17
00:00:55.670 --> 00:00:56.826
they'll be able to catch up.

18
00:00:56.826 --> 00:01:00.481
A lot of it just comes down to
Node async versus Bun async.

19
00:01:00.481 --> 00:01:03.496
Node just has a lot more hooks and
everything built in.

20
00:01:03.496 --> 00:01:07.737
As far as I can tell, that's what someone
from Datadog effectively said, and

21
00:01:07.737 --> 00:01:11.741
so they're working a lot on trying to
make that thing much, much faster.

22
00:01:11.741 --> 00:01:15.742
And so that's a big bottleneck and
a problem in mode right now,

23
00:01:15.742 --> 00:01:17.639
is just how slow that part is.

24
00:01:17.639 --> 00:01:23.065
It's probably not libuv, though maybe
there is something to it, which if you

25
00:01:23.065 --> 00:01:28.509
have a small additional cost per every
single promise that is introspection.

26
00:01:28.509 --> 00:01:30.637
And then on top of it,
you're making a ton of promises.

27
00:01:30.637 --> 00:01:32.674
And then on top of that,
you're trying to process requests.

28
00:01:32.674 --> 00:01:35.153
Are you gonna create
some general problems?

29
00:01:35.153 --> 00:01:39.938
Potentially, you may, instead of having
one request that takes 500 milliseconds,

30
00:01:39.938 --> 00:01:44.752
you have all your requests taking 250
milliseconds, 400 milliseconds instead.

31
00:01:44.752 --> 00:01:46.235
Which one's worse?

32
00:01:46.235 --> 00:01:50.736
I don't know which one's worse,
is rising your 50 or your 90 worse?

33
00:01:50.736 --> 00:01:55.340
So that's more of a you problem
[LAUGH] which one you want.

34
00:01:55.340 --> 00:01:57.355
But Bun, in general,
seems to be really fast.

35
00:01:57.355 --> 00:02:01.031
But Bun, in general,
isn't nearly as feature complete or

36
00:02:01.031 --> 00:02:03.319
as much legacy support as Node does.

37
00:02:03.319 --> 00:02:05.624
Is legacy support important to you?

38
00:02:05.624 --> 00:02:09.116
If it's not, maybe you can try Bun out.

39
00:02:09.116 --> 00:02:12.429
If Bun also segfaults in production,
is that a problem?

40
00:02:12.429 --> 00:02:14.253
Well, it's segfaulted on me a few times.

41
00:02:14.253 --> 00:02:16.972
It hasn't been doing that lately so
maybe it's a good thing now.

42
00:02:16.972 --> 00:02:19.927
But still, nonetheless, it's like
there's risks, there's straight offs.

43
00:02:21.746 --> 00:02:23.271
JSC, is JSC faster?

44
00:02:23.271 --> 00:02:27.146
I don't know,
people have been claiming it's faster.

45
00:02:27.146 --> 00:02:30.298
&gt;&gt; What was the charting
library you were using?

46
00:02:30.298 --> 00:02:36.933
&gt;&gt; Okay, so this one is Chrome DevTools,
memory and performance.

47
00:02:36.933 --> 00:02:40.157
This one is something I wrote with HTMX,
let's go HTMX.

48
00:02:40.157 --> 00:02:44.853
It's just as literally Charts.js and
Go and HTMX

49
00:02:44.853 --> 00:02:46.628
&gt;&gt; And if you wanna know more about HTMX.

50
00:02:46.628 --> 00:02:51.136
&gt;&gt; Yeah, if you wanna know more,
we're gonna be having an overview of HTMX.

51
00:02:51.136 --> 00:02:55.373
Yeah, we'll do gets and posts and deletes.

52
00:02:55.373 --> 00:02:59.060
&gt;&gt; Does all this apply in
browser engines as well?

53
00:02:59.060 --> 00:03:02.536
&gt;&gt; A lot of it does, but
often your problem, usually,

54
00:03:02.536 --> 00:03:08.066
your biggest problem in browser engines
isn't the fact that you're not pooling or

55
00:03:08.066 --> 00:03:11.321
pooling or you're using JSON or
not using JSON.

56
00:03:11.321 --> 00:03:15.764
It's that you're either over painting
the screen and making a bunch

57
00:03:15.764 --> 00:03:20.753
of layout changes or, B, you're using
a very heavy client-side library.

58
00:03:20.753 --> 00:03:23.955
And just like it doesn't
matter how much you do.

59
00:03:23.955 --> 00:03:26.805
They just have won the competition for
using all of your resources.

60
00:03:26.805 --> 00:03:30.561
If you've ever gone in and you try to
debug, say React or if you have RxJS or

61
00:03:30.561 --> 00:03:33.652
any of these libraries,
you'll look at the call stack, and

62
00:03:33.652 --> 00:03:36.825
the call stack is 900 calls
long to get to where you're at.

63
00:03:36.825 --> 00:03:41.138
That's just like, you're not
gonna be competing against that.

64
00:03:41.138 --> 00:03:44.033
Often, the problem isn't a you
problem in that situation,

65
00:03:44.033 --> 00:03:46.153
it's the libraries you've chosen to use.

66
00:03:46.153 --> 00:03:48.512
It's a future you problem
is I like to call that one.

67
00:03:51.036 --> 00:03:54.561
&gt;&gt; Some point, should you just
consider switching to Rust or C, or

68
00:03:54.561 --> 00:03:57.257
how do you know when
you've reached that point?

69
00:03:57.257 --> 00:04:00.266
&gt;&gt; Okay, so
that's more of a philosophical question.

70
00:04:00.266 --> 00:04:07.606
Generally, my take is that,
if performance could be a problem, use Go.

71
00:04:07.606 --> 00:04:12.529
If performance will absolutely
be a problem, use Rust.

72
00:04:12.529 --> 00:04:16.829
If you don't have skill in
statically-typed languages,

73
00:04:16.829 --> 00:04:18.122
use JavaScript.

74
00:04:18.122 --> 00:04:21.404
They take skill, they're annoying to use,
I mean, they're not fun.

75
00:04:21.404 --> 00:04:25.164
You know what, the funniest part about
TypeScript is when you quit caring,

76
00:04:25.164 --> 00:04:28.626
you just slap an any on it, and
you're just like, get out of here, and

77
00:04:28.626 --> 00:04:29.825
it's just fantastic.

78
00:04:29.825 --> 00:04:32.869
And you just don't even have
to care as any as user.

79
00:04:32.869 --> 00:04:34.678
Boom, this is a user now, all right?

80
00:04:34.678 --> 00:04:39.419
It's just like you can't do that in other
languages, that's not an option in Go.

81
00:04:42.697 --> 00:04:46.254
And so obviously, also OCaml,
people want me to mention OCaml.

82
00:04:46.254 --> 00:04:48.538
If you're a functional guy,
OCaml is really fast.

83
00:04:48.538 --> 00:04:52.341
It's like they have,
whoever is writing Reason ML,

84
00:04:52.341 --> 00:04:55.643
the creator of React wrote Reason ML,
right?

85
00:04:55.643 --> 00:04:56.638
What's the guy's name?

86
00:04:56.638 --> 00:04:58.956
I just saw it last night,
Fulk, Falk, Falk.

87
00:04:58.956 --> 00:05:03.286
Anyways, the OCamlers writing ReasonML,
they were able to serve

88
00:05:03.286 --> 00:05:07.943
the same website for Ahref,
which is a company, a pretty big company.

89
00:05:07.943 --> 00:05:12.871
Same website render 65 times faster
from their measurements on the server

90
00:05:12.871 --> 00:05:15.339
from going from JavaScript to OCaml.

91
00:05:15.339 --> 00:05:16.477
So there's other options.

92
00:05:16.477 --> 00:05:19.439
If you're a functional person,
you got Elixir, you got OCaml.

93
00:05:19.439 --> 00:05:20.455
I don't know, different.

94
00:05:20.455 --> 00:05:22.081
I'm not a functional person.

95
00:05:22.081 --> 00:05:23.179
I wrote some classes today.

96
00:05:23.179 --> 00:05:27.955
You can tell right away that it's just
not a functional overlord, all right?

97
00:05:27.955 --> 00:05:28.500
Any other questions?

98
00:05:28.500 --> 00:05:29.845
You had a question?

99
00:05:29.845 --> 00:05:36.035
&gt;&gt; Yeah, how will you rate
the optimizations we've made so far?

100
00:05:36.035 --> 00:05:39.020
Which ones are that we should care and

101
00:05:39.020 --> 00:05:43.364
that was more type pre-major
optimizations stuff?

102
00:05:43.364 --> 00:05:46.981
&gt;&gt; Well, the big one obviously
was getting off of promises.

103
00:05:46.981 --> 00:05:49.209
Finding your hot path and
getting off promises,

104
00:05:49.209 --> 00:05:51.285
that will almost
exclusively be a good call.

105
00:05:51.285 --> 00:05:53.261
Now you may not be able to do that, right?

106
00:05:53.261 --> 00:05:57.727
If the API you're using just uses
promises, if you're using fetch and

107
00:05:57.727 --> 00:06:01.310
it requires promises,
sometimes you're in a bad spot.

108
00:06:01.310 --> 00:06:04.776
And so you may not be able to do those
kind of things and so that's just real.

109
00:06:04.776 --> 00:06:07.216
That's just how it works.

110
00:06:07.216 --> 00:06:08.509
So promises off the hot.

111
00:06:08.509 --> 00:06:11.220
Hot paths are as few as possible,
always a good play.

112
00:06:11.220 --> 00:06:14.722
The next one is just try to defer as much

113
00:06:14.722 --> 00:06:19.328
of the networking to see
compared to JavaScript.

114
00:06:19.328 --> 00:06:22.349
In reality is if you're in a server,
if you're on a client, it's a completely

115
00:06:22.349 --> 00:06:25.113
different story, cuz the client is
processing a singular connection.

116
00:06:25.113 --> 00:06:27.066
A server is just processing
all the client connections.

117
00:06:27.066 --> 00:06:30.283
So that small cost of
JavaScript really adds up.

118
00:06:30.283 --> 00:06:33.849
So WS just, even though it's really
small on a singular machine,

119
00:06:33.849 --> 00:06:36.537
you could literally not
measure the difference.

120
00:06:36.537 --> 00:06:40.622
And it may actually even be faster just
by itself than doing C just cuz of

121
00:06:40.622 --> 00:06:42.708
the interop maybe, I have no idea.

122
00:06:42.708 --> 00:06:45.191
You'd have to measure it or
it's just negligent.

123
00:06:45.191 --> 00:06:49.405
It's just that once that scales,
it really falls apart.

124
00:06:49.405 --> 00:06:50.413
So those are the two big ones.

125
00:06:50.413 --> 00:06:53.235
Everything else is that only matters.

126
00:06:53.235 --> 00:06:56.210
It matters so much more not on my machine.

127
00:06:56.210 --> 00:07:00.857
All the other changes we made would likely
have shown significantly larger wins if we

128
00:07:00.857 --> 00:07:03.877
are just in a constrained memory and
CPU environment.

129
00:07:03.877 --> 00:07:05.322
It would have been glorious.

130
00:07:05.322 --> 00:07:09.751
But we're just not in one of those, so
it's really hard to surface any change.

131
00:07:09.751 --> 00:07:14.259
The goal of this course, of course, was to
make you think every time you do squirrely

132
00:07:14.259 --> 00:07:16.241
braces, there's a real cost to it.

133
00:07:16.241 --> 00:07:17.516
You should think about that.

134
00:07:17.516 --> 00:07:20.773
Spread operations, remember,
you're copying objects over.

135
00:07:20.773 --> 00:07:22.127
You should be thinking about these things.

136
00:07:22.127 --> 00:07:24.927
Do you really need to copy all the time?

137
00:07:24.927 --> 00:07:27.397
Not all software is going to be easy.

138
00:07:27.397 --> 00:07:29.829
We observed and
fix some pretty obvious stuff.

139
00:07:29.829 --> 00:07:33.163
And then we did some things
that were less obvious, right?

140
00:07:33.163 --> 00:07:36.888
Just even being able to identify that
WebSockets were a problem required us to

141
00:07:36.888 --> 00:07:38.217
use two different graphs.

142
00:07:38.217 --> 00:07:38.804
And often,

143
00:07:38.804 --> 00:07:42.753
people don't use the memory graph as
an indicator into what's right or wrong.

144
00:07:42.753 --> 00:07:45.769
And I think once you can
kinda tell once you've hit a,

145
00:07:45.769 --> 00:07:48.126
you're probably doing too much moment.

146
00:07:48.126 --> 00:07:50.322
And I wanted you to
have that feeling here,

147
00:07:50.322 --> 00:07:53.313
cuz everything we were doing
just wasn't yielding much.

148
00:07:53.313 --> 00:07:56.657
But usually at that moment, it comes
around when you start saying this.

149
00:07:56.657 --> 00:07:59.019
When you just have nothing
left that's going on,

150
00:07:59.019 --> 00:08:01.619
and you're trying to just
how can we make this better?

151
00:08:01.619 --> 00:08:03.563
Well, it's just like, at that point,

152
00:08:03.563 --> 00:08:07.347
you probably can't make things much
better unless if you change the problem.

153
00:08:07.347 --> 00:08:09.063
So good thing to think about, and

154
00:08:09.063 --> 00:08:12.017
also it's kind of cool to think
about where we came from.

155
00:08:12.017 --> 00:08:15.889
We started here, on master,
on the very on the original one,

156
00:08:15.889 --> 00:08:19.101
at 50 concurrent or
at 500 concurrent games.

157
00:08:19.101 --> 00:08:23.880
We were at 31% of the time,
we had a bad ratio, right?

158
00:08:23.880 --> 00:08:26.937
Now we're sitting at 3,000 games with 44%.

159
00:08:26.937 --> 00:08:29.664
Well, if you look at this, at 2,000,

160
00:08:29.664 --> 00:08:33.737
we're doing significantly better
than we were doing at 500.

161
00:08:33.737 --> 00:08:35.216
So that is a real win.

162
00:08:35.216 --> 00:08:37.702
We have made real wins throughout today.

163
00:08:37.702 --> 00:08:41.719
And so I just really do hope you
take home kinda the art of this all,

164
00:08:41.719 --> 00:08:45.975
which is observe and try to fix and
kinda come up with creative ideas.

165
00:08:45.975 --> 00:08:47.741
Performance and memory tabs are great.

166
00:08:47.741 --> 00:08:50.064
Node async stuff is slow for now.

167
00:08:50.064 --> 00:08:52.287
Memory is a good indication
where problems could be.

168
00:08:52.287 --> 00:08:54.497
GC time is a good indicator of issues.

169
00:08:54.497 --> 00:08:56.680
Sets are not always fast.

170
00:08:56.680 --> 00:09:00.438
Pull items, be a bit clever.

171
00:09:00.438 --> 00:09:02.784
This is more just an academic jab.

172
00:09:02.784 --> 00:09:06.554
Just because something is fast
academically, just because you

173
00:09:06.554 --> 00:09:11.105
have a Big O notation of whatever,
it doesn't mean it's practically fast.

174
00:09:11.105 --> 00:09:14.670
A really good trick to know about,
this is gonna be very deep academic stuff.

175
00:09:14.670 --> 00:09:17.546
And if you're not used to Big O,
maybe this will be a bit confusing.

176
00:09:17.546 --> 00:09:21.025
But if you're ever doing graph algorithms,

177
00:09:21.025 --> 00:09:25.334
the length of E is equivalent
to the length of V squared.

178
00:09:25.334 --> 00:09:27.539
That would be a graph algorithm edges.

179
00:09:27.539 --> 00:09:30.117
The worst case of edges is V squared.

180
00:09:30.117 --> 00:09:31.108
How do I know that?

181
00:09:31.108 --> 00:09:34.434
Think of an adjacency matrix where every
single node has a connection to every

182
00:09:34.434 --> 00:09:35.311
single other node.

183
00:09:35.311 --> 00:09:38.063
That's V by V so
that'd be V squared amount of edges.

184
00:09:38.063 --> 00:09:41.760
So that means if someone's
doing a graphing algorithm and

185
00:09:41.760 --> 00:09:45.993
the length is E log E, well,
you can change that to E log V squared.

186
00:09:45.993 --> 00:09:48.458
Which means that you can
change that to E 2 log V,

187
00:09:48.458 --> 00:09:51.120
which means that the running
time is E log V, right?

188
00:09:51.120 --> 00:09:53.493
It's just like, well,
that's kinda cheating.

189
00:09:53.493 --> 00:09:55.789
I feel like you just cheated me
because you're saying one thing but

190
00:09:55.789 --> 00:09:57.193
you're not actually doing something.

191
00:09:57.193 --> 00:10:01.886
So do you always just take run
time as this is what it is?

192
00:10:01.886 --> 00:10:06.421
Maybe take a little bit of time to
look over and to test things and

193
00:10:06.421 --> 00:10:12.680
to actually confirm the assumptions that
what you're looking at is actually better.

194
00:10:12.680 --> 00:10:14.652
All right, want more?

195
00:10:14.652 --> 00:10:17.921
I do a lot of this stuff live or
I do some amount of this stuff live.

196
00:10:17.921 --> 00:10:22.178
Mostly, we do read quite a bit
of articles these days, okay?

197
00:10:22.178 --> 00:10:24.140
I become a React and if one will, but

198
00:10:24.140 --> 00:10:28.145
we do a lot of the perf stuff exploration
from time to time on the channel.

199
00:10:28.145 --> 00:10:32.825
Do it a lot on Twitch,
yes, this is self promo.

200
00:10:32.825 --> 00:10:35.121
There you go, that's really it.

201
00:10:35.121 --> 00:10:36.241
That's the end of the course.

202
00:10:36.241 --> 00:10:38.050
I hope you guys enjoyed it.

203
00:10:38.050 --> 00:10:39.930
I had a lot of fun.

204
00:10:39.930 --> 00:10:41.497
You can clap, it's okay.

205
00:10:41.497 --> 00:10:42.959
&gt;&gt; [APPLAUSE]
&gt;&gt; Thank you.

206
00:10:42.959 --> 00:10:43.934
Thank you, thank you.

